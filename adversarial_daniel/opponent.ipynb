{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lasting-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import grid2op\n",
    "\n",
    "from grid2op.Exceptions import OpponentError\n",
    "\n",
    "# import the train function and train your agent\n",
    "from l2rpn_baselines.utils import NNParam, TrainingParam\n",
    "from l2rpn_baselines.DoubleDuelingDQN.DoubleDuelingDQNConfig import DoubleDuelingDQNConfig as cfg\n",
    "from l2rpn_baselines.DoubleDuelingDQN.DoubleDuelingDQN_NN import DoubleDuelingDQN_NN\n",
    "from l2rpn_baselines.DoubleDuelingDQN.prioritized_replay_buffer import PrioritizedReplayBuffer\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "%run base_opponent.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mechanical-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomOpponent(BaseOpponent):\n",
    "    \"\"\" This opponent will disconnect lines randomly\"\"\"\n",
    "    def __init__(self, observation_space, action_space, lines_attacked=[],\n",
    "                 attack_period=12*24, name=__name__):\n",
    "        BaseOpponent.__init__(self, action_space)\n",
    "        \n",
    "        if len(lines_attacked) == 0:\n",
    "            warnings.warn(f'The opponent is deactivated as there is no information as to which line to attack. '\n",
    "                          f'You can set the argument \"kwargs_opponent\" to the list of the line names you want '\n",
    "                          f' the opponent to attack in the \"make\" function.')\n",
    "            \n",
    "        # Store attackable lines IDs\n",
    "        self._lines_ids = []\n",
    "        for l_name in lines_attacked:\n",
    "            l_id = np.where(self.action_space.name_line == l_name)\n",
    "            if len(l_id) and len(l_id[0]):\n",
    "                self._lines_ids.append(l_id[0][0])\n",
    "            else:\n",
    "                raise OpponentError(\"Unable to find the powerline named \\\"{}\\\" on the grid. For \"\n",
    "                                    \"information, powerlines on the grid are : {}\"\n",
    "                                    \"\".format(l_name, sorted(self.action_space.name_line)))\n",
    "                \n",
    "        # Pre-build attacks actions\n",
    "        self._attacks = []\n",
    "        for l_id in self._lines_ids:\n",
    "            a = self.action_space({\n",
    "                'set_line_status': [(l_id, -1)]\n",
    "            })\n",
    "            self._attacks.append(a)\n",
    "        self._attacks = np.array(self._attacks)\n",
    "        \n",
    "        self._next_attack_time = None\n",
    "        self._attack_period = attack_period\n",
    "        \n",
    "    def reset(self, initial_budget=None):\n",
    "        self._next_attack_time = None\n",
    "        \n",
    "    def tell_attack_continues(self, observation, agent_action, env_action, budget):\n",
    "        self._next_attack_time = None\n",
    "    \n",
    "    def attack(self, observation, agent_action=None, env_action=None, budget=None,\n",
    "               previous_fails=None):\n",
    "        if observation is None:  # during creation of the environment\n",
    "            return None  # i choose not to attack in this case\n",
    "\n",
    "        # Decide the time of the next attack\n",
    "        if self._next_attack_time is None:\n",
    "            self._next_attack_time = 1 + self.space_prng.randint(self._attack_period)\n",
    "        self._next_attack_time -= 1\n",
    "        \n",
    "        # If the attack time has not come yet, do not attack\n",
    "        if self._next_attack_time > 0:\n",
    "            return None\n",
    "        \n",
    "        # Status of attackable lines\n",
    "        status = observation.line_status[self._lines_ids]\n",
    "\n",
    "        # If all attackable lines are disconnected\n",
    "        if np.all(~status):\n",
    "            return None  # i choose not to attack in this case\n",
    "\n",
    "        # Pick a line among the connected lines\n",
    "        attack = self.space_prng.choice(self._attacks[status])\n",
    "        return attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intensive-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3QN_Opponent(BaseOpponent):\n",
    "    \"\"\"\n",
    "    This opponent will disconnect lines by learning Q-values of attacks among the attackable lines `lines_attacked`.\n",
    "    When an attack becomes possible, the time of the attack will be sampled uniformly\n",
    "    in the next `attack_period` steps (see init).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, observation_space, lines_attacked=[], attack_period=12*24, name=__name__, is_training=False, learning_rate=cfg.LR):\n",
    "        BaseOpponent.__init__(self, action_space)\n",
    "\n",
    "        if len(lines_attacked) == 0:\n",
    "            warnings.warn(f'The opponent is deactivated as there is no information as to which line to attack. '\n",
    "                          f'You can set the argument \"kwargs_opponent\" to the list of the line names you want '\n",
    "                          f' the opponent to attack in the \"make\" function.')\n",
    "\n",
    "        # Store attackable lines IDs\n",
    "        self._lines_ids = []\n",
    "        for l_name in lines_attacked:\n",
    "            l_id = np.where(self.action_space.name_line == l_name)\n",
    "            if len(l_id) and len(l_id[0]):\n",
    "                self._lines_ids.append(l_id[0][0])\n",
    "            else:\n",
    "                raise OpponentError(\"Unable to find the powerline named \\\"{}\\\" on the grid. For \"\n",
    "                                    \"information, powerlines on the grid are : {}\"\n",
    "                                    \"\".format(l_name, sorted(self.action_space.name_line)))\n",
    "\n",
    "        # Pre-build attacks actions\n",
    "        self._do_nothing = self.action_space({})\n",
    "        self._attacks = []\n",
    "        for l_id in self._lines_ids:\n",
    "            a = self.action_space({\n",
    "                'set_line_status': [(l_id, -1)]\n",
    "            })\n",
    "            self._attacks.append(a)\n",
    "        self._attacks = np.array(self._attacks)\n",
    "\n",
    "        # Opponent's attack period\n",
    "        self._attack_period = attack_period   \n",
    "\n",
    "        self.obs_space = observation_space\n",
    "        \n",
    "        # Store constructor params\n",
    "        self.name = name\n",
    "        self.num_frames = cfg.N_FRAMES\n",
    "        self.is_training = is_training\n",
    "        self.batch_size = cfg.BATCH_SIZE\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Declare required vars\n",
    "        self.Qmain = None\n",
    "        self.obs = None\n",
    "        self.state = []\n",
    "        self.frames = []\n",
    "\n",
    "        # Declare training vars\n",
    "        self.per_buffer = None\n",
    "        self.done = False\n",
    "        self.frames2 = None\n",
    "        self.epoch_rewards = None\n",
    "        self.epoch_alive = None\n",
    "        self.Qtarget = None\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "        # Compute dimensions from intial spaces\n",
    "        self.observation_size = self.obs_space.size_obs()\n",
    "        self.action_size = len(self._attacks)\n",
    "\n",
    "        # Load network graph\n",
    "        self.policy_net = DoubleDuelingDQN_NN(self.action_size,\n",
    "                                         self.observation_size,\n",
    "                                         num_frames=self.num_frames,\n",
    "                                         learning_rate=self.lr,\n",
    "                                         learning_rate_decay_steps=cfg.LR_DECAY_STEPS,\n",
    "                                         learning_rate_decay_rate=cfg.LR_DECAY_RATE)\n",
    "        # Setup training vars if needed\n",
    "        if self.is_training:\n",
    "            self._init_training()\n",
    "\n",
    "    def _init_training(self):\n",
    "        self.epsilon = cfg.INITIAL_EPSILON\n",
    "        self.frames2 = []\n",
    "        self.epoch_rewards = []\n",
    "        self.epoch_alive = []\n",
    "        self.per_buffer = PrioritizedReplayBuffer(cfg.PER_CAPACITY, cfg.PER_ALPHA)\n",
    "        self.target_net = DoubleDuelingDQN_NN(self.action_size,\n",
    "                                           self.observation_size,\n",
    "                                           num_frames = self.num_frames)            \n",
    "\n",
    "\n",
    "    def tell_attack_continues(self, observation, agent_action, env_action, budget):\n",
    "        self._next_attack_time = None\n",
    "\n",
    "    def attack(self, observation, agent_action=None, env_action=None, budget=None, previous_fails=False):\n",
    "        \"\"\"\n",
    "        This method is the equivalent of \"act\" for a regular agent.\n",
    "        Opponent, in this framework can have more information than a regular agent (in particular it can\n",
    "        view time step t+1), it has access to its current budget etc.\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation: :class:`grid2op.Observation.Observation`\n",
    "            The last observation (at time t)\n",
    "        agent_action: :class:`grid2op.Action.Action`\n",
    "            The action that the agent took\n",
    "        env_action: :class:`grid2op.Action.Action`\n",
    "            The modification that the environment will take.\n",
    "        budget: ``float``\n",
    "            The current remaining budget (if an action is above this budget, it will be replaced by a do nothing.\n",
    "        previous_fails: ``bool``\n",
    "            Wheter the previous attack failed (due to budget or ambiguous action)\n",
    "        Returns\n",
    "        -------\n",
    "        attack: :class:`grid2op.Action.Action`\n",
    "            The attack performed by the opponent. In this case, a do nothing, all the time.\n",
    "        \"\"\"\n",
    "        # TODO maybe have a class \"GymOpponent\" where the observation would include the budget  and all other\n",
    "        # TODO information, and forward something to the \"act\" method.\n",
    "\n",
    "        # During creation of the environment, do not attack\n",
    "        if observation is None:\n",
    "            return None\n",
    "\n",
    "        # Register current state to stacking buffer\n",
    "        self._save_current_frame(observation)\n",
    "        # We need at least num frames to predict\n",
    "        if len(self.frames) < self.num_frames:\n",
    "            return None # Do nothing\n",
    "\n",
    "        # Decide the time of the next attack\n",
    "        if self._next_attack_time is None:\n",
    "            self._next_attack_time = 1 + self.space_prng.randint(self._attack_period)\n",
    "        self._next_attack_time -= 1\n",
    "\n",
    "        # If the attack time has not come yet, do not attack\n",
    "        if self._next_attack_time > 0:\n",
    "            return None\n",
    "\n",
    "        # If all attackable lines are disconnected, do not attack\n",
    "        status = observation.line_status[self._lines_ids]\n",
    "        if np.all(status == False):\n",
    "            return None\n",
    "\n",
    "        # Infer with the last num_frames states\n",
    "        a, _ = self.policy_net.predict_move(status, np.array(self.frames))\n",
    "        return a\n",
    "\n",
    "\n",
    "    def _reset_state(self, current_obs):\n",
    "        # Initial state\n",
    "        self.obs = current_obs\n",
    "        self.state = self.convert_obs(self.obs)\n",
    "        self.done = False\n",
    "\n",
    "    def _reset_frame_buffer(self):\n",
    "        # Reset frame buffers\n",
    "        self.frames = []\n",
    "        if self.is_training:\n",
    "            self.frames2 = []\n",
    "\n",
    "    def _save_current_frame(self, obs):\n",
    "        frame = self.convert_obs(obs)\n",
    "        self.frames.append(frame)\n",
    "        if len(self.frames) > self.num_frames:\n",
    "            self.frames.pop(0)\n",
    "\n",
    "    def _save_next_frame(self, next_obs):\n",
    "        frame = self.convert_obs(next_obs)\n",
    "        self.frames2.append(frame)\n",
    "        if len(self.frames2) > self.num_frames:\n",
    "            self.frames2.pop(0)\n",
    "\n",
    "    def _adaptive_epsilon_decay(self, step):\n",
    "        ada_div = cfg.DECAY_EPSILON / 10.0\n",
    "        step_off = step + ada_div\n",
    "        ada_eps = cfg.INITIAL_EPSILON * -math.log10((step_off + 1) / (cfg.DECAY_EPSILON + ada_div))\n",
    "        ada_eps_up_clip = min(cfg.INITIAL_EPSILON, ada_eps)\n",
    "        ada_eps_low_clip = max(cfg.FINAL_EPSILON, ada_eps_up_clip)\n",
    "        return ada_eps_low_clip\n",
    "            \n",
    "    def _save_hyperparameters(self, logpath, env, steps):\n",
    "        try:\n",
    "            # change of name in grid2op >= 1.2.3\n",
    "            r_instance = env._reward_helper.template_reward\n",
    "        except AttributeError as nm_exc_:\n",
    "            r_instance = env.reward_helper.template_reward\n",
    "        hp = {\n",
    "            \"lr\": cfg.LR,\n",
    "            \"lr_decay_steps\": cfg.LR_DECAY_STEPS,\n",
    "            \"lr_decay_rate\": cfg.LR_DECAY_RATE,\n",
    "            \"batch_size\": cfg.BATCH_SIZE,\n",
    "            \"stack_frames\": cfg.N_FRAMES,\n",
    "            \"iter\": steps,\n",
    "            \"e_start\": cfg.INITIAL_EPSILON,\n",
    "            \"e_end\": cfg.FINAL_EPSILON,\n",
    "            \"e_decay\": cfg.DECAY_EPSILON,\n",
    "            \"discount\": cfg.DISCOUNT_FACTOR,\n",
    "            \"per_alpha\": cfg.PER_ALPHA,\n",
    "            \"per_beta\": cfg.PER_BETA,\n",
    "            \"per_capacity\": cfg.PER_CAPACITY,\n",
    "            \"update_freq\": cfg.UPDATE_FREQ,\n",
    "            \"update_hard\": cfg.UPDATE_TARGET_HARD_FREQ,\n",
    "            \"update_soft\": cfg.UPDATE_TARGET_SOFT_TAU,\n",
    "            \"reward\": dict(r_instance)\n",
    "        }\n",
    "        hp_filename = \"{}-hypers.json\".format(self.name)\n",
    "        hp_path = os.path.join(logpath, hp_filename)\n",
    "        with open(hp_path, 'w') as fp:\n",
    "            json.dump(hp, fp=fp, indent=2)\n",
    "\n",
    "    ## Agent Interface\n",
    "    def convert_obs(self, observation):\n",
    "        li_vect=  []\n",
    "        for el in observation.attr_list_vect:\n",
    "            v = observation._get_array_from_attr_name(el).astype(np.float32)\n",
    "            v_fix = np.nan_to_num(v)\n",
    "            v_norm = np.linalg.norm(v_fix)\n",
    "            if v_norm > 1e6:\n",
    "                v_res = (v_fix / v_norm) * 10.0\n",
    "            else:\n",
    "                v_res = v_fix\n",
    "            li_vect.append(v_res)\n",
    "        return np.concatenate(li_vect)\n",
    "\n",
    "    ## Baseline Interface\n",
    "    def reset(self, initial_budget):\n",
    "        #self._reset_state(observation)\n",
    "        self._next_attack_time = None\n",
    "        self._reset_frame_buffer()\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.policy_net.load_network(path)\n",
    "        if self.is_training:\n",
    "            self.policy_net.update_target_hard(self.target_net.model)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.policy_net.save_network(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-attribute",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
