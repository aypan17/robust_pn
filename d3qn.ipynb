{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import grid2op\n",
    "from grid2op import make\n",
    "from grid2op.Agent import AgentWithConverter\n",
    "from grid2op.Converter import IdToAct\n",
    "\n",
    "# import the train function and train your agent\n",
    "from l2rpn_baselines.utils import NNParam, TrainingParam\n",
    "from l2rpn_baselines.DoubleDuelingDQN.DoubleDuelingDQNConfig import DoubleDuelingDQNConfig as cfg\n",
    "from l2rpn_baselines.DoubleDuelingDQN.DoubleDuelingDQN_NN import DoubleDuelingDQN_NN\n",
    "from l2rpn_baselines.DoubleDuelingDQN.prioritized_replay_buffer import PrioritizedReplayBuffer\n",
    "\n",
    "class DoubleDuelingDQN(AgentWithConverter):\n",
    "    def __init__(self,\n",
    "                 observation_space,\n",
    "                 action_space,\n",
    "                 name=__name__,\n",
    "                 is_training=False,\n",
    "                 learning_rate=cfg.LR):\n",
    "        # Call parent constructor\n",
    "        AgentWithConverter.__init__(self, action_space,\n",
    "                                    action_space_converter=IdToAct)\n",
    "        self.obs_space = observation_space\n",
    "\n",
    "        # Filter\n",
    "        #print(\"Actions filtering...\")\n",
    "        self.action_space.filter_action(self._filter_action)\n",
    "        #print(\"..Done\")\n",
    "        \n",
    "        # Store constructor params\n",
    "        self.name = name\n",
    "        self.num_frames = cfg.N_FRAMES\n",
    "        self.is_training = is_training\n",
    "        self.batch_size = cfg.BATCH_SIZE\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Declare required vars\n",
    "        self.Qmain = None\n",
    "        self.obs = None\n",
    "        self.state = []\n",
    "        self.frames = []\n",
    "\n",
    "        # Declare training vars\n",
    "        self.per_buffer = None\n",
    "        self.done = False\n",
    "        self.frames2 = None\n",
    "        self.epoch_rewards = None\n",
    "        self.epoch_alive = None\n",
    "        self.Qtarget = None\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "        # Compute dimensions from intial spaces\n",
    "        self.observation_size = self.obs_space.size_obs()\n",
    "        self.action_size = self.action_space.size()\n",
    "\n",
    "        # Load network graph\n",
    "        self.Qmain = DoubleDuelingDQN_NN(self.action_size,\n",
    "                                         self.observation_size,\n",
    "                                         num_frames=self.num_frames,\n",
    "                                         learning_rate=self.lr,\n",
    "                                         learning_rate_decay_steps=cfg.LR_DECAY_STEPS,\n",
    "                                         learning_rate_decay_rate=cfg.LR_DECAY_RATE)\n",
    "        # Setup training vars if needed\n",
    "        if self.is_training:\n",
    "            self._init_training()\n",
    "\n",
    "    def _filter_action(self, action):\n",
    "        MAX_ELEM = 2\n",
    "        act_dict = action.impact_on_objects()\n",
    "        elem = 0\n",
    "        elem += act_dict[\"force_line\"][\"reconnections\"][\"count\"]\n",
    "        elem += act_dict[\"force_line\"][\"disconnections\"][\"count\"]\n",
    "        elem += act_dict[\"switch_line\"][\"count\"]\n",
    "        elem += len(act_dict[\"topology\"][\"bus_switch\"])\n",
    "        elem += len(act_dict[\"topology\"][\"assigned_bus\"])\n",
    "        elem += len(act_dict[\"topology\"][\"disconnect_bus\"])\n",
    "        elem += len(act_dict[\"redispatch\"][\"generators\"])\n",
    "\n",
    "        if elem <= MAX_ELEM:\n",
    "            return True\n",
    "        return False\n",
    "            \n",
    "    def _init_training(self):\n",
    "        self.epsilon = cfg.INITIAL_EPSILON\n",
    "        self.frames2 = []\n",
    "        self.epoch_rewards = []\n",
    "        self.epoch_alive = []\n",
    "        self.per_buffer = PrioritizedReplayBuffer(cfg.PER_CAPACITY, cfg.PER_ALPHA)\n",
    "        self.Qtarget = DoubleDuelingDQN_NN(self.action_size,\n",
    "                                           self.observation_size,\n",
    "                                           num_frames = self.num_frames)\n",
    "\n",
    "    def _reset_state(self, current_obs):\n",
    "        # Initial state\n",
    "        self.obs = current_obs\n",
    "        self.state = self.convert_obs(self.obs)\n",
    "        self.done = False\n",
    "\n",
    "    def _reset_frame_buffer(self):\n",
    "        # Reset frame buffers\n",
    "        self.frames = []\n",
    "        if self.is_training:\n",
    "            self.frames2 = []\n",
    "\n",
    "    def _save_current_frame(self, state):\n",
    "        self.frames.append(state.copy())\n",
    "        if len(self.frames) > self.num_frames:\n",
    "            self.frames.pop(0)\n",
    "\n",
    "    def _save_next_frame(self, next_state):\n",
    "        self.frames2.append(next_state.copy())\n",
    "        if len(self.frames2) > self.num_frames:\n",
    "            self.frames2.pop(0)\n",
    "\n",
    "    def _adaptive_epsilon_decay(self, step):\n",
    "        ada_div = cfg.DECAY_EPSILON / 10.0\n",
    "        step_off = step + ada_div\n",
    "        ada_eps = cfg.INITIAL_EPSILON * -math.log10((step_off + 1) / (cfg.DECAY_EPSILON + ada_div))\n",
    "        ada_eps_up_clip = min(cfg.INITIAL_EPSILON, ada_eps)\n",
    "        ada_eps_low_clip = max(cfg.FINAL_EPSILON, ada_eps_up_clip)\n",
    "        return ada_eps_low_clip\n",
    "            \n",
    "    def _save_hyperparameters(self, logpath, env, steps):\n",
    "        try:\n",
    "            # change of name in grid2op >= 1.2.3\n",
    "            r_instance = env._reward_helper.template_reward\n",
    "        except AttributeError as nm_exc_:\n",
    "            r_instance = env.reward_helper.template_reward\n",
    "        hp = {\n",
    "            \"lr\": cfg.LR,\n",
    "            \"lr_decay_steps\": cfg.LR_DECAY_STEPS,\n",
    "            \"lr_decay_rate\": cfg.LR_DECAY_RATE,\n",
    "            \"batch_size\": cfg.BATCH_SIZE,\n",
    "            \"stack_frames\": cfg.N_FRAMES,\n",
    "            \"iter\": steps,\n",
    "            \"e_start\": cfg.INITIAL_EPSILON,\n",
    "            \"e_end\": cfg.FINAL_EPSILON,\n",
    "            \"e_decay\": cfg.DECAY_EPSILON,\n",
    "            \"discount\": cfg.DISCOUNT_FACTOR,\n",
    "            \"per_alpha\": cfg.PER_ALPHA,\n",
    "            \"per_beta\": cfg.PER_BETA,\n",
    "            \"per_capacity\": cfg.PER_CAPACITY,\n",
    "            \"update_freq\": cfg.UPDATE_FREQ,\n",
    "            \"update_hard\": cfg.UPDATE_TARGET_HARD_FREQ,\n",
    "            \"update_soft\": cfg.UPDATE_TARGET_SOFT_TAU,\n",
    "            \"reward\": dict(r_instance)\n",
    "        }\n",
    "        hp_filename = \"{}-hypers.json\".format(self.name)\n",
    "        hp_path = os.path.join(logpath, hp_filename)\n",
    "        with open(hp_path, 'w') as fp:\n",
    "            json.dump(hp, fp=fp, indent=2)\n",
    "\n",
    "    ## Agent Interface\n",
    "    def convert_obs(self, observation):\n",
    "        li_vect=  []\n",
    "        for el in observation.attr_list_vect:\n",
    "            v = observation._get_array_from_attr_name(el).astype(np.float32)\n",
    "            v_fix = np.nan_to_num(v)\n",
    "            v_norm = np.linalg.norm(v_fix)\n",
    "            if v_norm > 1e6:\n",
    "                v_res = (v_fix / v_norm) * 10.0\n",
    "            else:\n",
    "                v_res = v_fix\n",
    "            li_vect.append(v_res)\n",
    "        return np.concatenate(li_vect)\n",
    "\n",
    "    def convert_act(self, action):\n",
    "        return super().convert_act(action)\n",
    "\n",
    "    ## Baseline Interface\n",
    "    def reset(self, observation):\n",
    "        self._reset_state(observation)\n",
    "        self._reset_frame_buffer()\n",
    "\n",
    "    def my_act(self, state, reward, done=False):\n",
    "        # Register current state to stacking buffer\n",
    "        self._save_current_frame(state)\n",
    "        # We need at least num frames to predict\n",
    "        if len(self.frames) < self.num_frames:\n",
    "            return 0 # Do nothing\n",
    "        # Infer with the last num_frames states\n",
    "        a, _ = self.Qmain.predict_move(np.array(self.frames))\n",
    "        return a\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.Qmain.load_network(path)\n",
    "        if self.is_training:\n",
    "            self.Qmain.update_target_hard(self.Qtarget.model)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.Qmain.save_network(path)\n",
    "\n",
    "    ## Training Procedure\n",
    "#     def train(self, env,\n",
    "#               iterations,\n",
    "#               save_path,\n",
    "#               num_pre_training_steps=0,\n",
    "#               logdir = \"logs-train\"):\n",
    "#         # Make sure we can fill the experience buffer\n",
    "#         if num_pre_training_steps < self.batch_size * self.num_frames:\n",
    "#             num_pre_training_steps = self.batch_size * self.num_frames\n",
    "\n",
    "#         # Loop vars\n",
    "#         num_training_steps = iterations\n",
    "#         num_steps = num_pre_training_steps + num_training_steps\n",
    "#         step = 0\n",
    "#         self.epsilon = cfg.INITIAL_EPSILON\n",
    "#         alive_steps = 0\n",
    "#         total_reward = 0\n",
    "#         self.done = True\n",
    "\n",
    "#         # Create file system related vars\n",
    "#         logpath = os.path.join(logdir, self.name)\n",
    "#         os.makedirs(save_path, exist_ok=True)\n",
    "#         modelpath = os.path.join(save_path, self.name + \".h5\")\n",
    "#         self.tf_writer = tf.summary.create_file_writer(logpath, name=self.name)\n",
    "#         self._save_hyperparameters(save_path, env, num_steps)\n",
    "\n",
    "#         print(num_steps)\n",
    "#         # Training loop\n",
    "#         while step < num_steps:\n",
    "#             # Init first time or new episode\n",
    "#             if self.done:\n",
    "#                 new_obs = env.reset() # This shouldn't raise\n",
    "#                 self.reset(new_obs)\n",
    "#             if cfg.VERBOSE and step % 1000 == 0:\n",
    "#                 print(\"Step [{}] -- Random [{}]\".format(step, self.epsilon))\n",
    "\n",
    "#             # Save current observation to stacking buffer\n",
    "#             self._save_current_frame(self.state)\n",
    "\n",
    "#             # Choose an action\n",
    "#             if step <= num_pre_training_steps:\n",
    "#                 a = self.Qmain.random_move()\n",
    "#             elif np.random.rand(1) < self.epsilon:\n",
    "#                 a = self.Qmain.random_move()\n",
    "#             elif len(self.frames) < self.num_frames:\n",
    "#                 a = 0 # Do nothing\n",
    "#             else:\n",
    "#                 a, _ = self.Qmain.predict_move(np.array(self.frames))\n",
    "\n",
    "#             # Convert it to a valid action\n",
    "#             act = self.convert_act(a)\n",
    "#             # Execute action\n",
    "#             new_obs, reward, self.done, info = env.step(act)\n",
    "#             new_state = self.convert_obs(new_obs)\n",
    "#             if info[\"is_illegal\"] or info[\"is_ambiguous\"] or \\\n",
    "#                info[\"is_dispatching_illegal\"] or info[\"is_illegal_reco\"]:\n",
    "#                 if cfg.VERBOSE:\n",
    "#                     print (a, info)\n",
    "\n",
    "#             # Save new observation to stacking buffer\n",
    "#             self._save_next_frame(new_state)\n",
    "\n",
    "#             # Save to experience buffer\n",
    "#             if len(self.frames2) == self.num_frames:\n",
    "#                 self.per_buffer.add(np.array(self.frames),\n",
    "#                                     a, reward,\n",
    "#                                     np.array(self.frames2),\n",
    "#                                     self.done)\n",
    "\n",
    "#             # Perform training when we have enough experience in buffer\n",
    "#             if step >= num_pre_training_steps:\n",
    "#                 training_step = step - num_pre_training_steps\n",
    "#                 # Decay chance of random action\n",
    "#                 self.epsilon = self._adaptive_epsilon_decay(training_step)\n",
    "\n",
    "#                 # Perform training at given frequency\n",
    "#                 if step % cfg.UPDATE_FREQ == 0 and \\\n",
    "#                    len(self.per_buffer) >= self.batch_size:\n",
    "#                     # Perform training\n",
    "#                     self._batch_train(training_step, step)\n",
    "\n",
    "#                     if cfg.UPDATE_TARGET_SOFT_TAU > 0.0:\n",
    "#                         tau = cfg.UPDATE_TARGET_SOFT_TAU\n",
    "#                         # Update target network towards primary network\n",
    "#                         self.Qmain.update_target_soft(self.Qtarget.model, tau)\n",
    "\n",
    "#                 # Every UPDATE_TARGET_HARD_FREQ trainings, update target completely\n",
    "#                 if cfg.UPDATE_TARGET_HARD_FREQ > 0 and \\\n",
    "#                    step % (cfg.UPDATE_FREQ * cfg.UPDATE_TARGET_HARD_FREQ) == 0:\n",
    "#                     self.Qmain.update_target_hard(self.Qtarget.model)\n",
    "\n",
    "#             total_reward += reward\n",
    "#             if self.done:\n",
    "#                 self.epoch_rewards.append(total_reward)\n",
    "#                 self.epoch_alive.append(alive_steps)\n",
    "#                 if cfg.VERBOSE:\n",
    "#                     print(\"Survived [{}] steps\".format(alive_steps))\n",
    "#                     print(\"Total reward [{}]\".format(total_reward))\n",
    "#                 alive_steps = 0\n",
    "#                 total_reward = 0\n",
    "#             else:\n",
    "#                 alive_steps += 1\n",
    "            \n",
    "#             # Save the network every 1000 iterations\n",
    "#             if step > 0 and step % 1000 == 0:\n",
    "#                 self.save(modelpath)\n",
    "\n",
    "#             # Iterate to next loop\n",
    "#             step += 1\n",
    "#             # Make new obs the current obs\n",
    "#             self.obs = new_obs\n",
    "#             self.state = new_state\n",
    "\n",
    "#         # Save model after all steps\n",
    "#         self.save(modelpath)\n",
    "\n",
    "    def _batch_train(self, training_step, step):\n",
    "        \"\"\"Trains network to fit given parameters\"\"\"\n",
    "\n",
    "        # Sample from experience buffer\n",
    "        sample_batch = self.per_buffer.sample(self.batch_size, cfg.PER_BETA)\n",
    "        s_batch = sample_batch[0]\n",
    "        a_batch = sample_batch[1]\n",
    "        r_batch = sample_batch[2]\n",
    "        s2_batch = sample_batch[3]\n",
    "        d_batch = sample_batch[4]\n",
    "        w_batch = sample_batch[5]\n",
    "        idx_batch = sample_batch[6]\n",
    "\n",
    "        Q = np.zeros((self.batch_size, self.action_size))\n",
    "\n",
    "        # Reshape frames to 1D\n",
    "        input_size = self.observation_size * self.num_frames\n",
    "        input_t = np.reshape(s_batch, (self.batch_size, input_size))\n",
    "        input_t_1 = np.reshape(s2_batch, (self.batch_size, input_size))\n",
    "\n",
    "        # Save the graph just the first time\n",
    "        if training_step == 0:\n",
    "            tf.summary.trace_on()\n",
    "\n",
    "        # T Batch predict\n",
    "        Q = self.Qmain.model.predict(input_t, batch_size = self.batch_size)\n",
    "\n",
    "        ## Log graph once and disable graph logging\n",
    "        if training_step == 0:\n",
    "            with self.tf_writer.as_default():\n",
    "                tf.summary.trace_export(self.name + \"-graph\", step)\n",
    "\n",
    "        # T+1 batch predict\n",
    "        Q1 = self.Qmain.model.predict(input_t_1, batch_size=self.batch_size)\n",
    "        Q2 = self.Qtarget.model.predict(input_t_1, batch_size=self.batch_size)\n",
    "\n",
    "        # Compute batch Qtarget using Double DQN\n",
    "        for i in range(self.batch_size):\n",
    "            doubleQ = Q2[i, np.argmax(Q1[i])]\n",
    "            Q[i, a_batch[i]] = r_batch[i]\n",
    "            if d_batch[i] == False:\n",
    "                Q[i, a_batch[i]] += cfg.DISCOUNT_FACTOR * doubleQ\n",
    "\n",
    "        # Batch train\n",
    "        loss = self.Qmain.train_on_batch(input_t, Q, w_batch)\n",
    "\n",
    "        # Update PER buffer\n",
    "        priorities = self.Qmain.batch_sq_error\n",
    "        # Can't be zero, no upper limit\n",
    "        priorities = np.clip(priorities, a_min=1e-8, a_max=None)\n",
    "        self.per_buffer.update_priorities(idx_batch, priorities)\n",
    "\n",
    "        # Log some useful metrics every even updates\n",
    "        if step % (cfg.UPDATE_FREQ * 2) == 0:\n",
    "            with self.tf_writer.as_default():\n",
    "                mean_reward = np.mean(self.epoch_rewards)\n",
    "                mean_alive = np.mean(self.epoch_alive)\n",
    "                if len(self.epoch_rewards) >= 100:\n",
    "                    mean_reward_100 = np.mean(self.epoch_rewards[-100:])\n",
    "                    mean_alive_100 = np.mean(self.epoch_alive[-100:])\n",
    "                else:\n",
    "                    mean_reward_100 = mean_reward\n",
    "                    mean_alive_100 = mean_alive\n",
    "                tf.summary.scalar(\"mean_reward\", mean_reward, step)\n",
    "                tf.summary.scalar(\"mean_alive\", mean_alive, step)\n",
    "                tf.summary.scalar(\"mean_reward_100\", mean_reward_100, step)\n",
    "                tf.summary.scalar(\"mean_alive_100\", mean_alive_100, step)\n",
    "                tf.summary.scalar(\"loss\", loss, step)\n",
    "                tf.summary.scalar(\"lr\", self.Qmain.train_lr, step)\n",
    "            if cfg.VERBOSE:\n",
    "                print(\"loss =\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
