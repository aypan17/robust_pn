{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "robust_pn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTiGC637mm9RcCm597SDfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aypan17/robust_pn/blob/main/robust_pn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "dzn78HkfMh6f",
        "outputId": "3992fc4a-60d5-445c-d1ed-337b4a1d6113"
      },
      "source": [
        "import grid2op\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b4ad9dc683d2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install grid2op\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJV-Go7zf_HG"
      },
      "source": [
        "import torch \r\n",
        "import torch.nn as nn\r\n",
        "from grid2op.Opponent import BaseOpponent\r\n",
        "\r\n",
        "class DDQN(torch.nn.Module):\r\n",
        "    def __init__(self, d_model, num_lines):\r\n",
        "        # Embed the state (there are num_lines lines to cut)\r\n",
        "        self.emb = nn.Linear(num_lines, d_model)\r\n",
        "\r\n",
        "        # Layer to measure the value of a state\r\n",
        "        self.value_stream = nn.Sequential(\r\n",
        "            nn.Linear(d_model, d_model),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(d_model, 1)\r\n",
        "        )\r\n",
        "        # Layer to measure the advantages of an action given a state\r\n",
        "        self.advantage_stream = nn.Sequential(\r\n",
        "            nn.Linear(d_model, d_model),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(d_model, d_model)\r\n",
        "        )\r\n",
        "\r\n",
        "    # Take environment observation and return Q-values\r\n",
        "    def forward(self, obs):\r\n",
        "        # Right now I'm only looking at the capacity of each power line when\r\n",
        "        # deciding what to attack but this should be changed.\r\n",
        "        rho = obs.rho # Capacity of each power line\r\n",
        "        line_status = obs.line_status # Whether or not the line is connected\r\n",
        "\r\n",
        "        state = self.emb(torch.tensor(rho))\r\n",
        "        values = self.value_stream(state)\r\n",
        "        advantages = self.advantage_stream(state)\r\n",
        "        qvals = values + (advantages - advantages.mean())\r\n",
        "        \r\n",
        "        return qvals\r\n",
        "\r\n",
        "class RLOpp(BaseOpponent):\r\n",
        "\r\n",
        "    def __init__(self, env, agent, d_model, num_lines):\r\n",
        "        self.env = env\r\n",
        "        self.actspace = env.action_space\r\n",
        "        self.agent = agent\r\n",
        "        self.d_model = d_model \r\n",
        "        self.num_lines = num_lines\r\n",
        "        self.budget = 1000 # Random number\r\n",
        "\r\n",
        "        self.policy_net = DDQN(d_model, num_lines)\r\n",
        "        self.target_net = DDQN(d_model, num_lines)\r\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\r\n",
        "        self.target_net.eval()\r\n",
        "\r\n",
        "    def attack(self, obs, agent_act, env_act, budget, fail):\r\n",
        "        '''\r\n",
        "        Takes an observation, agent action, env action, total budget, and if\r\n",
        "        the prev attack failed or not and outputs a new attack.\r\n",
        "\r\n",
        "        Params\r\n",
        "        ------\r\n",
        "        obs: grid2op.Observation.Observation from time t\r\n",
        "        agent_act: grid2op.Action.Action from the agent\r\n",
        "        env_act: grid2op.Action.Action from the environment\r\n",
        "        budget: budget remaining of the opponent. will not attack if over budget\r\n",
        "        fail: whether or not the previous attack failed\r\n",
        "\r\n",
        "        Returns\r\n",
        "        ------\r\n",
        "        attack: grid2op.Action.Action\r\n",
        "        ''' \r\n",
        "        rho = obs.rho # Capacity of each power line\r\n",
        "        line_status = obs.line_status # Whether or not the line is connected\r\n",
        "        state = self.emb(torch.tensor(rho)) # State vector\r\n",
        "\r\n",
        "        eps_threshold = 0.05 #EPS_END + (EPS_START - EPS_END) * \\ math.exp(-1. * steps_done / EPS_DECAY)\r\n",
        "        if random.random() > eps_threshold:\r\n",
        "            # Exploit\r\n",
        "            with torch.no_grad():\r\n",
        "                policy_net.eval()\r\n",
        "                idx = torch.sort(self.policy_net(state.unsqueeze(0)), descending=True, dim=1)[1]\r\n",
        "                policy_net.train()\r\n",
        "                \r\n",
        "                # Try until we get a valid action\r\n",
        "                for line in idx:\r\n",
        "                    if line_status[line]:\r\n",
        "                        return self.actspace({\"set_line_status\":[(line, -1)]})\r\n",
        "\r\n",
        "                # This should never happen\r\n",
        "                raise ValueError('Invalid state: no possible action')\r\n",
        "\r\n",
        "        else:\r\n",
        "            ## Explore\r\n",
        "            line = random.choice(np.nonzero(line_status)[0])\r\n",
        "            return self.actspace({\"set_line_status\":[(line, -1)]})\r\n",
        "\r\n",
        "    def update_target_net(self):\r\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\r\n",
        "\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyI1vk0aaaQ5"
      },
      "source": [
        "def optimize_model():\r\n",
        "    if len(memory) < BATCH_SIZE:\r\n",
        "        return\r\n",
        "    transitions = memory.sample(BATCH_SIZE)\r\n",
        "        \r\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\r\n",
        "    # detailed explanation). This converts batch-array of Transitions\r\n",
        "    # to Transition of batch-arrays.\r\n",
        "    batch = Transition(*zip(*transitions))\r\n",
        "\r\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\r\n",
        "    # (a final state would've been the one after which simulation ended)\r\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\r\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\r\n",
        "    non_final_next_states = torch.stack([s for s in batch.next_state\r\n",
        "                                                if s is not None])\r\n",
        "        \r\n",
        "    state_batch = torch.stack(batch.state)\r\n",
        "    action_batch = torch.stack(batch.action)\r\n",
        "    reward_batch = torch.cat(batch.reward)\r\n",
        "\r\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\r\n",
        "    # columns of actions taken. These are the actions which would've been taken\r\n",
        "    # for each batch state according to policy_net\r\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\r\n",
        "\r\n",
        "    # Compute V(s_{t+1}) for all next states.\r\n",
        "    # Expected values of actions for non_final_next_states are computed based\r\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\r\n",
        "    # This is merged based on the mask, such that we'll have either the expected\r\n",
        "    # state value or 0 in case the state was final.\r\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\r\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\r\n",
        "#     print(next_state_values[non_final_mask].shape)\r\n",
        "    # Compute the expected Q values\r\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\r\n",
        "\r\n",
        "    # Compute Huber loss\r\n",
        "    loss = F.smooth_l1_loss(state_action_values.double(), expected_state_action_values.unsqueeze(1).double())\r\n",
        "    value_loss = loss.item()\r\n",
        "    # Optimize the model\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    for param in policy_net.parameters():\r\n",
        "        param.grad.data.clamp_(-1, 1)\r\n",
        "    optimizer.step()\r\n",
        "    \r\n",
        "    return value_loss\r\n",
        "\r\n",
        "class ReplayMemory(object):\r\n",
        "    def __init__(self, capacity):\r\n",
        "        self.capacity = capacity\r\n",
        "        self.memory = []\r\n",
        "        self.position = 0\r\n",
        "\r\n",
        "    def push(self, *args):\r\n",
        "        \"\"\"Saves a transition.\"\"\"\r\n",
        "        if len(self.memory) < self.capacity:\r\n",
        "            self.memory.append(None)\r\n",
        "        self.memory[self.position] = Transition(*args)\r\n",
        "        self.position = (self.position + 1) % self.capacity\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        return random.sample(self.memory, batch_size)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.memory)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y6KEpsTxqAW"
      },
      "source": [
        "BATCH_SIZE = 128\r\n",
        "GAMMA = 0.999\r\n",
        "EPS_START = 0.9\r\n",
        "EPS_END = 0.05\r\n",
        "EPS_DECAY = 200\r\n",
        "TARGET_UPDATE = 50\r\n",
        "MAX_ITERATIONS = 500\r\n",
        "\r\n",
        "# n - number of targets\r\n",
        "n = 4\r\n",
        "# m - number of weapons\r\n",
        "m = 5\r\n",
        "assert n > 1\r\n",
        "\r\n",
        "lower_val = 25\r\n",
        "upper_val = 50\r\n",
        "lower_prob = 0.6\r\n",
        "upper_prob = 0.9\r\n",
        "values = np.random.uniform(lower_val, upper_val, n)\r\n",
        "prob = np.random.uniform(lower_prob, upper_prob, (m, n))\r\n",
        "assignment = generate_initial_assignment(n, m)\r\n",
        "env = WTAEnv(assignment, values, prob, device)\r\n",
        "\r\n",
        "policy_net = DuelingDQN(n, m).to(device)\r\n",
        "target_net = DuelingDQN(n, m).to(device)\r\n",
        "target_net.load_state_dict(policy_net.state_dict())\r\n",
        "target_net.eval()\r\n",
        "\r\n",
        "optimizer = optim.Adam(policy_net.parameters(), learning_rate=0.0001)\r\n",
        "memory = ReplayMemory(10000)\r\n",
        "\r\n",
        "num_episodes = 15\r\n",
        "env.reset()\r\n",
        "init_state = env.get_state() \r\n",
        "for i_episode in range(num_episodes):\r\n",
        "    # Initialize the environment and state\r\n",
        "    env.reset()\r\n",
        "    state = init_state\r\n",
        "    for t in range(1, MAX_ITERATIONS+1):\r\n",
        "        print(f'episode {i_episode}/{num_episodes}, iteration {t}/{MAX_ITERATIONS}', end=' ')\r\n",
        "        # Select and perform an action\r\n",
        "        action = select_action(state)\r\n",
        "        observation, reward, done, _ = env.step(action)\r\n",
        "        reward = torch.tensor([reward], device=device)\r\n",
        "\r\n",
        "        if not done:\r\n",
        "            next_state = observation\r\n",
        "        else:\r\n",
        "            next_state = None\r\n",
        "\r\n",
        "        # Store the transition in memory\r\n",
        "        memory.push(state, action, next_state, reward)\r\n",
        "\r\n",
        "        # Move to the next state\r\n",
        "        state = next_state\r\n",
        "\r\n",
        "        # Perform one step of the optimization (on the target network)\r\n",
        "        loss = optimize_model()\r\n",
        "        if done:\r\n",
        "            episode_durations.append(t + 1)\r\n",
        "            plot_durations()\r\n",
        "            break\r\n",
        "        \r\n",
        "        print(f'loss: {loss}', end='\\r')\r\n",
        "    print()\r\n",
        "    # Update the target network, copying all weights and biases in DQN\r\n",
        "    if i_episode % TARGET_UPDATE == 0:\r\n",
        "        opp.update_target_net()\r\n",
        "\r\n",
        "print()\r\n",
        "print('Complete')\r\n",
        "env.render()\r\n",
        "env.close()\r\n",
        "plt.ioff()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}