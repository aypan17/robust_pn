{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting l2rpn_baselines\n",
      "  Using cached https://files.pythonhosted.org/packages/3e/6e/3bf591c6ce8edb295fe8e190f073ec0280d3f400e7dab869df2fcde7f519/l2rpn_baselines-0.5.0.tar.gz\n",
      "Requirement already satisfied: grid2op[optional]>=0.9.1.post1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from l2rpn_baselines) (1.4.0)\n",
      "Collecting tensorflow>=2.2.0 (from l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/84/75/81c0f75cf168b52e86bbc4953f09b12e46cb8c70d1b34565e3a40b3dd982/tensorflow-2.4.1-cp36-cp36m-macosx_10_11_x86_64.whl\n",
      "Collecting Keras>=2.3.1 (from l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/44/e1/dc0757b20b56c980b5553c1b5c4c32d378c7055ab7bfa92006801ad359ab/Keras-2.4.3-py2.py3-none-any.whl\n",
      "Collecting torch>=1.4.0 (from l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/b6/01/fffb29c3892d80801bc6400e07c90b8fa6cd5f3db5ce9d7ca8068e14e0b2/torch-1.7.1-cp36-none-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: statsmodels>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from l2rpn_baselines) (0.12.1)\n",
      "Collecting scikit-learn>=0.22.2.post1 (from l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/c9/35/b2a1eaf3ba1eca2d834a5ca94d23ee98b21ff6228b5e32a07703e73c58ca/scikit_learn-0.24.1-cp36-cp36m-macosx_10_13_x86_64.whl\n",
      "Collecting gym>=0.17.1 (from l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/26/f2/e7ee20bf02b2d02263becba1c5ec4203fef7cfbd57759e040e51307173f4/gym-0.18.0.tar.gz\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from l2rpn_baselines) (1.5.4)\n",
      "Requirement already satisfied: pandapower>=2.2.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.5.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.25.1)\n",
      "Requirement already satisfied: pandas>=1.0.3 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.1.5)\n",
      "Requirement already satisfied: pathlib>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.0.1)\n",
      "Requirement already satisfied: networkx>=2.4 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.5)\n",
      "Requirement already satisfied: numpy>=1.18.3 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.45.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (4.54.1)\n",
      "Collecting nbformat>=5.0.4; extra == \"optional\" (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/13/1d/59cbc5a6b627ba3b4c0ec5ccc82a9002e58b324e2620a4929b81f1f8d309/nbformat-5.1.2-py3-none-any.whl\n",
      "Collecting imageio>=2.8.0; extra == \"optional\" (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/6e/57/5d899fae74c1752f52869b613a8210a2480e1a69688e65df6cb26117d45d/imageio-2.9.0-py3-none-any.whl\n",
      "Collecting psutil>=5.7.0; extra == \"optional\" (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/30/81/37ebe0ba2840b76681072e786bae3319cade8a6861029d0ae885c274fa0b/psutil-5.8.0-cp36-cp36m-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: seaborn>=0.10.0; extra == \"optional\" in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.11.0)\n",
      "Requirement already satisfied: matplotlib>=3.2.1; extra == \"optional\" in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (3.3.3)\n",
      "Collecting jyquickhelper>=0.3.128; extra == \"optional\" (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/a0/79/88855f989694fc65ddeb61f7c984b21e82e86686f79305bb2dbe3513970d/jyquickhelper-0.4.220-py3-none-any.whl\n",
      "Collecting jupyter-client>=6.1.0; extra == \"optional\" (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/d6/30aed7ef13ff3f359e99626c1b0a32ebbc3bf9b9d5616ec46e9e245d5fa9/jupyter_client-6.1.11-py3-none-any.whl\n",
      "Collecting numba>=0.48.0; extra == \"optional\" (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/b4/3d/d0f92722cee2204aae9ecae5a2155ff9cecd389173fb282e96ddbed021d3/numba-0.52.0-cp36-cp36m-macosx_10_14_x86_64.whl\n",
      "Collecting pygifsicle>=1.0.1; extra == \"optional\" (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/09/23/7adf797d3a65f568bb96e85b1a7ad046cc9b78be627df75927b83d6d6323/pygifsicle-1.0.2.tar.gz\n",
      "Collecting plotly>=4.5.4; extra == \"optional\" (from grid2op[optional]>=0.9.1.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/1f/f6/bd3c17c8003b6641df1228e80e1acac97ed8402635e46c2571f8e1ef63af/plotly-4.14.3-py2.py3-none-any.whl\n",
      "Collecting google-pasta~=0.2 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl\n",
      "Collecting wrapt~=1.12.1 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
      "Collecting gast==0.3.3 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Collecting grpcio~=1.32.0 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/2f/787b1ae60455f42e88969e2d1cbc7ad34c9cc3d7945e11f3d6ac9b7a8325/grpcio-1.32.0-cp36-cp36m-macosx_10_9_x86_64.whl\n",
      "Collecting termcolor~=1.1.0 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "Collecting opt-einsum~=3.3.0 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl\n",
      "Collecting protobuf>=3.9.2 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/5f/ce/a550df576512c9a32118def35fb36c7d18c8334336af5c684d353a6ee627/protobuf-3.15.1-cp36-cp36m-macosx_10_9_x86_64.whl\n",
      "Collecting tensorboard~=2.4 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl\n",
      "Collecting astunparse~=1.6.3 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
      "Collecting flatbuffers~=1.12.0 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
      "Collecting six~=1.15.0 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting typing-extensions~=3.7.4 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Collecting absl-py~=0.10 (from tensorflow>=2.2.0->l2rpn_baselines)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached https://files.pythonhosted.org/packages/bc/58/0aa6fb779dc69cfc811df3398fcbeaeefbf18561b6e36b185df0782781cc/absl_py-0.11.0-py3-none-any.whl\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl\n",
      "Collecting wheel~=0.35 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing~=1.1.2 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n",
      "Collecting h5py~=2.10.0 (from tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/7f/a833846f5628d00f82ad87010f98294257535f1052e4a466888deba29f94/h5py-2.10.0-cp36-cp36m-macosx_10_6_intel.whl\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from Keras>=2.3.1->l2rpn_baselines) (3.13)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from torch>=1.4.0->l2rpn_baselines) (0.8)\n",
      "Requirement already satisfied: patsy>=0.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from statsmodels>=0.11.1->l2rpn_baselines) (0.5.1)\n",
      "Collecting joblib>=0.11 (from scikit-learn>=0.22.2.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/55/85/70c6602b078bd9e6f3da4f467047e906525c355a4dacd4f71b97a35d9897/joblib-1.0.1-py3-none-any.whl\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.22.2.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Collecting pyglet<=1.5.0,>=1.4.0 (from gym>=0.17.1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/70/ca/20aee170afe6011e295e34b27ad7d7ccd795faba581dd3c6f7cec237f561/pyglet-1.5.0-py2.py3-none-any.whl\n",
      "Collecting Pillow<=7.2.0 (from gym>=0.17.1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/04/8f/c42f534b73680f501858a8c7171705a55c7347c86419b74fa585370c8306/Pillow-7.2.0-cp36-cp36m-macosx_10_10_x86_64.whl\n",
      "Collecting cloudpickle<1.7.0,>=1.2.0 (from gym>=0.17.1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: cryptography in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pandapower>=2.2.2->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.7)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pandapower>=2.2.2->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (3.0.6)\n",
      "Requirement already satisfied: xlsxwriter in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pandapower>=2.2.2->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.3.7)\n",
      "Requirement already satisfied: xlrd in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pandapower>=2.2.2->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.0.1)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pandapower>=2.2.2->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (20.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests>=2.23.0->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests>=2.23.0->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.25.11)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests>=2.23.0->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests>=2.23.0->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pandas>=1.0.3->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pandas>=1.0.3->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2018.9)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from networkx>=2.4->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (4.3.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nbformat>=5.0.4; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.6.0)\n",
      "Requirement already satisfied: traitlets>=4.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nbformat>=5.0.4; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (4.3.3)\n",
      "Requirement already satisfied: ipython-genutils in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nbformat>=5.0.4; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nbformat>=5.0.4; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (4.6.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from matplotlib>=3.2.1; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from matplotlib>=3.2.1; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from matplotlib>=3.2.1; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.4.7)\n",
      "Requirement already satisfied: jupyter in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.0.0)\n",
      "Requirement already satisfied: ipython in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (7.11.1)\n",
      "Requirement already satisfied: notebook in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (6.0.2)\n",
      "Requirement already satisfied: tornado>=4.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jupyter-client>=6.1.0; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (6.0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jupyter-client>=6.1.0; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (18.1.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from numba>=0.48.0; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (39.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llvmlite<0.36,>=0.35.0 (from numba>=0.48.0; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/84/06/09488c0bef4776a4750c5c579b4f0f5ec5ffdfdb6cf8afe1b6aa8f48aed6/llvmlite-0.35.0-cp36-cp36m-macosx_10_9_x86_64.whl\n",
      "Collecting retrying>=1.3.3 (from plotly>=4.5.4; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/44/ef/beae4b4ef80902f22e3af073397f079c96969c69b2c7d52a57ea9ae61c9d/retrying-1.3.3.tar.gz\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/81/67/e2c34bb0628984c7ce71cce6ba6964cb29c418873847fc285f826e032e6e/google_auth_oauthlib-0.4.2-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/ac/ef/24a91ca96efa0d7802dffb83ccc7a3c677027bea19ec3c9ee80be740408e/Markdown-3.3.3-py3-none-any.whl\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/1a/c1/499e600ba0c618b451cd9c425ae1c177249940a2086316552fee7d86c954/tensorboard_plugin_wit-1.8.0-py3-none-any.whl\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/bf/6b/a364a4ad2a9d25904a19f53b9474ae9666b96e0653a6ca38492baf1bcf05/google_auth-1.27.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines) (0.15.2)\n",
      "Requirement already satisfied: future in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.1->l2rpn_baselines) (0.17.1)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from cryptography->pandapower>=2.2.2->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.12.3)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from cryptography->pandapower>=2.2.2->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.24.0)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from openpyxl->pandapower>=2.2.2->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.0.1)\n",
      "Requirement already satisfied: jdcal in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from openpyxl->pandapower>=2.2.2->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.4.1)\n",
      "Requirement already satisfied: jupyter-console in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (6.0.0)\n",
      "Requirement already satisfied: nbconvert in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (5.6.1)\n",
      "Requirement already satisfied: ipykernel in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (5.1.3)\n",
      "Requirement already satisfied: qtconsole in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (4.6.0)\n",
      "Requirement already satisfied: ipywidgets in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (7.5.1)\n",
      "Requirement already satisfied: pickleshare in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from ipython->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.7.5)\n",
      "Requirement already satisfied: pygments in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from ipython->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.5.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from ipython->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.15.2)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from ipython->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from ipython->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.0.10)\n",
      "Requirement already satisfied: backcall in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from ipython->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from ipython->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (4.7.0)\n",
      "Requirement already satisfied: prometheus-client in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from notebook->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.7.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from notebook->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.10.1)\n",
      "Requirement already satisfied: Send2Trash in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from notebook->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from notebook->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.8.3)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata; python_version < \"3.8\" (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/f3/ed/da40116a204abb5c4dd1d929346d33e0d29cedb2cedd18ea98f0385dcd92/importlib_metadata-3.4.0-py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines) (4.2.0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography->pandapower>=2.2.2->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (2.19)\n",
      "Requirement already satisfied: bleach in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nbconvert->jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (3.1.0)\n",
      "Requirement already satisfied: testpath in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nbconvert->jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.4.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nbconvert->jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nbconvert->jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.6.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nbconvert->jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nbconvert->jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from ipywidgets->jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (3.5.1)\n",
      "Requirement already satisfied: parso>=0.5.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jedi>=0.10->ipython->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.5.2)\n",
      "Requirement already satisfied: wcwidth in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.1.8)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from jinja2->notebook->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (1.1.1)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/41/ad/6a4f1a124b325618a7fb758b885b68ff7b058eec47d9220a12ab38d90b1f/zipp-3.4.0-py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->l2rpn_baselines)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
      "Requirement already satisfied: webencodings in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from bleach->nbconvert->jupyter->jyquickhelper>=0.3.128; extra == \"optional\"->grid2op[optional]>=0.9.1.post1->l2rpn_baselines) (0.5.1)\n",
      "Building wheels for collected packages: l2rpn-baselines, gym, pygifsicle, wrapt, retrying\n",
      "  Building wheel for l2rpn-baselines (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for l2rpn-baselines: filename=l2rpn_baselines-0.5.0-cp36-none-any.whl size=210126 sha256=c8de16c35678e3f74a10daffd681ca7a77492ee61f9480144289551b73a2d136\n",
      "  Stored in directory: /Users/ylee/Library/Caches/pip/wheels/95/e6/b1/1ad0409e86c803ed1429f4c1ade5a0b43f47c5c933d716be04\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.18.0-cp36-none-any.whl size=1656451 sha256=e87916c8713787367e6cf49da7fc255915a9bb931c2811adbc40633b7892d677\n",
      "  Stored in directory: /Users/ylee/Library/Caches/pip/wheels/be/85/3b/480b828a4a697b37392740a040b8989f729d952b4e441a1877\n",
      "  Building wheel for pygifsicle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pygifsicle: filename=pygifsicle-1.0.2-cp36-none-any.whl size=4517 sha256=12af4f971578a6e827021a59c0eb00a099214becb4f19a59ecd5b852c47423c1\n",
      "  Stored in directory: /Users/ylee/Library/Caches/pip/wheels/7d/9a/33/113116a2b70d0f359bdafc3ee5270bf6f73f0d8bea06907815\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-macosx_10_9_x86_64.whl size=32420 sha256=f5b792f585673b0df555ed58a797128729432ed8cd9a8c1d30feb3b45ad387e7\n",
      "  Stored in directory: /Users/ylee/Library/Caches/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
      "  Building wheel for retrying (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for retrying: filename=retrying-1.3.3-cp36-none-any.whl size=11432 sha256=50e03a115459c36c1abe157fa07de14a78dde34fb395d1bec6e8d116138251fd\n",
      "  Stored in directory: /Users/ylee/Library/Caches/pip/wheels/d7/a9/33/acc7b709e2a35caa7d4cae442f6fe6fbf2c43f80823d46460c\n",
      "Successfully built l2rpn-baselines gym pygifsicle wrapt retrying\n",
      "\u001b[31mERROR: gs-quant 0.8.229.1 has requirement pandas<1.1, but you'll have pandas 1.1.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: aws-sam-cli 0.18.0 has requirement requests==2.22.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: aws-sam-cli 0.18.0 has requirement six~=1.11.0, but you'll have six 1.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-auth 1.27.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorboard 2.4.1 has requirement setuptools>=41.0.0, but you'll have setuptools 39.0.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: six, google-pasta, wrapt, gast, grpcio, termcolor, opt-einsum, protobuf, oauthlib, requests-oauthlib, pyasn1, pyasn1-modules, google-auth, google-auth-oauthlib, zipp, typing-extensions, importlib-metadata, markdown, wheel, tensorboard-plugin-wit, absl-py, tensorboard, astunparse, flatbuffers, tensorflow-estimator, keras-preprocessing, h5py, tensorflow, Keras, torch, joblib, threadpoolctl, scikit-learn, pyglet, Pillow, cloudpickle, gym, l2rpn-baselines, nbformat, imageio, psutil, jyquickhelper, jupyter-client, llvmlite, numba, pygifsicle, retrying, plotly\n",
      "  Found existing installation: six 1.11.0\n",
      "    Uninstalling six-1.11.0:\n",
      "      Successfully uninstalled six-1.11.0\n",
      "  Found existing installation: pyasn1 0.4.5\n",
      "    Uninstalling pyasn1-0.4.5:\n",
      "      Successfully uninstalled pyasn1-0.4.5\n",
      "  Found existing installation: wheel 0.33.4\n",
      "    Uninstalling wheel-0.33.4:\n",
      "      Successfully uninstalled wheel-0.33.4\n",
      "  Found existing installation: Pillow 8.0.1\n",
      "    Uninstalling Pillow-8.0.1:\n",
      "      Successfully uninstalled Pillow-8.0.1\n",
      "  Found existing installation: nbformat 5.0.3\n",
      "    Uninstalling nbformat-5.0.3:\n",
      "      Successfully uninstalled nbformat-5.0.3\n",
      "  Found existing installation: jupyter-client 5.3.4\n",
      "    Uninstalling jupyter-client-5.3.4:\n",
      "      Successfully uninstalled jupyter-client-5.3.4\n",
      "Successfully installed Keras-2.4.3 Pillow-7.2.0 absl-py-0.11.0 astunparse-1.6.3 cloudpickle-1.6.0 flatbuffers-1.12 gast-0.3.3 google-auth-1.27.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 gym-0.18.0 h5py-2.10.0 imageio-2.9.0 importlib-metadata-3.4.0 joblib-1.0.1 jupyter-client-6.1.11 jyquickhelper-0.4.220 keras-preprocessing-1.1.2 l2rpn-baselines-0.5.0 llvmlite-0.35.0 markdown-3.3.3 nbformat-5.1.2 numba-0.52.0 oauthlib-3.1.0 opt-einsum-3.3.0 plotly-4.14.3 protobuf-3.15.1 psutil-5.8.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygifsicle-1.0.2 pyglet-1.5.0 requests-oauthlib-1.3.0 retrying-1.3.3 scikit-learn-0.24.1 six-1.15.0 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 threadpoolctl-2.1.0 torch-1.7.1 typing-extensions-3.7.4.3 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!$sys.executable -m pip install l2rpn_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import grid2op\n",
    "from grid2op import make\n",
    "\n",
    "%run reward.ipynb\n",
    "%run d3qn.ipynb\n",
    "%run evaluate.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, opponent, n_iter, save_path, save_path_opponent,\n",
    "          num_pre_training_steps, log_path, opponent_train_freq):\n",
    "    # Make sure we can fill the experience buffer\n",
    "    if num_pre_training_steps < agent.batch_size * agent.num_frames:\n",
    "        num_pre_training_steps = agent.batch_size * agent.num_frames\n",
    "        \n",
    "    # Loop vars\n",
    "    num_training_steps = n_iter\n",
    "    num_steps = num_pre_training_steps + num_training_steps\n",
    "    step = 0\n",
    "    agent.epsilon, opponent.epsilon = cfg.INITIAL_EPSILON, cfg.INITIAL_EPSILON\n",
    "    alive_steps = 0\n",
    "    total_reward, total_reward_opponent = 0, 0\n",
    "    agent.done, opponent.done = True, True\n",
    "    print(f\"Total number of steps: {num_steps}\")\n",
    "\n",
    "    # Create file system related vars\n",
    "    logpath = os.path.join(log_path, agent.name)\n",
    "    opponent_logpath = os.path.join(log_path, opponent.name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    os.makedirs(save_path_opponent, exist_ok=True)\n",
    "    modelpath = os.path.join(save_path, agent.name + \".h5\")\n",
    "    opponent_modelpath = os.path.join(save_path_opponent, opponent.name + '.h5')\n",
    "    agent.tf_writer = tf.summary.create_file_writer(logpath, name=agent.name)\n",
    "    opponent.tf_writer = tf.summary.create_file_writer(opponent_logpath, name=opponent.name)\n",
    "    agent._save_hyperparameters(save_path, env, num_steps)\n",
    "    opponent._save_hyperparameters(save_path_opponent, env, num_steps)\n",
    "    \n",
    "    while step < num_steps:\n",
    "        # Init first time or new episode\n",
    "        if agent.done or opponent.done:\n",
    "            new_obs = env.reset() # This shouldn't raise\n",
    "            agent.reset(new_obs)\n",
    "        if cfg.VERBOSE and step % 1000 == 0:\n",
    "            print(\"Step [{}] -- Random [{}]\".format(step, agent.epsilon))\n",
    "\n",
    "        # Save current observation to stacking buffer\n",
    "        agent._save_current_frame(agent.state)\n",
    "\n",
    "        # Choose an action\n",
    "        if step <= num_pre_training_steps:\n",
    "            a = agent.Qmain.random_move()\n",
    "        elif np.random.rand(1) < agent.epsilon:\n",
    "            a = agent.Qmain.random_move()\n",
    "        elif len(agent.frames) < agent.num_frames:\n",
    "            a = 0 # Do nothing\n",
    "        else:\n",
    "            a, _ = agent.Qmain.predict_move(np.array(agent.frames))\n",
    "\n",
    "        # Convert it to a valid action\n",
    "        act = agent.convert_act(a)\n",
    "        # Execute action\n",
    "        new_obs, reward, agent.done, info = env.step(act)\n",
    "        new_state = agent.convert_obs(new_obs)\n",
    "        if info[\"is_illegal\"] or info[\"is_ambiguous\"] or \\\n",
    "           info[\"is_dispatching_illegal\"] or info[\"is_illegal_reco\"]:\n",
    "            if cfg.VERBOSE:\n",
    "                print (a, info)\n",
    "\n",
    "        # Save new observation to stacking buffer\n",
    "        agent._save_next_frame(new_state)\n",
    "\n",
    "        # Save to experience buffer\n",
    "        if len(agent.frames2) == agent.num_frames:\n",
    "            agent.per_buffer.add(np.array(agent.frames),\n",
    "                                a, reward,\n",
    "                                np.array(agent.frames2),\n",
    "                                agent.done)\n",
    "\n",
    "        # Perform training when we have enough experience in buffer\n",
    "        if step >= num_pre_training_steps:\n",
    "            training_step = step - num_pre_training_steps\n",
    "            # Decay chance of random action\n",
    "            agent.epsilon = agent._adaptive_epsilon_decay(training_step)\n",
    "\n",
    "            # Perform training at given frequency\n",
    "            if step % cfg.UPDATE_FREQ == 0 and \\\n",
    "               len(agent.per_buffer) >= agent.batch_size:\n",
    "                # Perform training\n",
    "                agent._batch_train(training_step, step)\n",
    "\n",
    "                if cfg.UPDATE_TARGET_SOFT_TAU > 0.0:\n",
    "                    tau = cfg.UPDATE_TARGET_SOFT_TAU\n",
    "                    # Update target network towards primary network\n",
    "                    agent.Qmain.update_target_soft(agent.Qtarget.model, tau)\n",
    "\n",
    "            # Every UPDATE_TARGET_HARD_FREQ trainings, update target completely\n",
    "            if cfg.UPDATE_TARGET_HARD_FREQ > 0 and \\\n",
    "               step % (cfg.UPDATE_FREQ * cfg.UPDATE_TARGET_HARD_FREQ) == 0:\n",
    "                agent.Qmain.update_target_hard(agent.Qtarget.model)\n",
    "        \n",
    "        ######## Opponent #########\n",
    "        if not agent.done:\n",
    "            opponent.obs = new_obs\n",
    "            opponent.state = new_state\n",
    "\n",
    "            # Save current observation to opponent's stacking buffer\n",
    "            opponent._save_current_frame(opponent.state)\n",
    "            # Choose an action\n",
    "            if step <= num_pre_training_steps:\n",
    "                a = opponent.Qmain.random_move()\n",
    "            elif np.random.rand(1) < opponent.epsilon:\n",
    "                a = opponent.Qmain.random_move()\n",
    "            elif len(opponent.frames) < opponent.num_frames:\n",
    "                a = 0 # Do nothing\n",
    "            else:\n",
    "                a, _ = opponent.Qmain.predict_move(np.array(opponent.frames))\n",
    "\n",
    "            # Convert it to a valid action\n",
    "            act = opponent.convert_act(a)\n",
    "            # Execute action\n",
    "            new_obs, reward_opponent, opponent.done, info = env.step(act)\n",
    "            new_state = opponent.convert_obs(new_obs)\n",
    "            if info[\"is_illegal\"] or info[\"is_ambiguous\"] or \\\n",
    "               info[\"is_dispatching_illegal\"] or info[\"is_illegal_reco\"]:\n",
    "                if cfg.VERBOSE:\n",
    "                    print (a, info)\n",
    "\n",
    "            # Save new observation to stacking buffer\n",
    "            opponent._save_next_frame(new_state)\n",
    "\n",
    "            # Save to experience buffer\n",
    "            if len(opponent.frames2) == opponent.num_frames:\n",
    "                opponent.per_buffer.add(np.array(opponent.frames),\n",
    "                                    a, -reward_opponent,\n",
    "                                    np.array(opponent.frames2),\n",
    "                                    opponent.done)\n",
    "\n",
    "            # Perform training when we have enough experience in buffer\n",
    "            if step >= num_pre_training_steps and step % opponent_train_freq == 0:\n",
    "                training_step = step - num_pre_training_steps\n",
    "                # Decay chance of random action\n",
    "                opponent.epsilon = opponent._adaptive_epsilon_decay(training_step)\n",
    "\n",
    "                # Perform training at given frequency\n",
    "                if step % cfg.UPDATE_FREQ == 0 and \\\n",
    "                   len(opponent.per_buffer) >= opponent.batch_size:\n",
    "                    # Perform training\n",
    "                    opponent._batch_train(training_step, step)\n",
    "\n",
    "                    if cfg.UPDATE_TARGET_SOFT_TAU > 0.0:\n",
    "                        tau = cfg.UPDATE_TARGET_SOFT_TAU\n",
    "                        # Update target network towards primary network\n",
    "                        opponent.Qmain.update_target_soft(opponent.Qtarget.model, tau)\n",
    "\n",
    "                # Every UPDATE_TARGET_HARD_FREQ trainings, update target completely\n",
    "                if cfg.UPDATE_TARGET_HARD_FREQ > 0 and \\\n",
    "                   step % (cfg.UPDATE_FREQ * cfg.UPDATE_TARGET_HARD_FREQ) == 0:\n",
    "                    opponent.Qmain.update_target_hard(opponent.Qtarget.model)\n",
    "            total_reward_opponent += reward_opponent\n",
    "\n",
    "        total_reward += reward\n",
    "        \n",
    "        if agent.done or opponent.done:\n",
    "            agent.epoch_rewards.append(total_reward)\n",
    "            agent.epoch_alive.append(alive_steps)\n",
    "            if cfg.VERBOSE:\n",
    "                print(\"Survived [{}] steps\".format(alive_steps))\n",
    "                print(\"Total reward [{}]\".format(total_reward))\n",
    "                print(\"Totla reward opponent [{}]\".format(total_reward_opponent))\n",
    "            alive_steps = 0\n",
    "            total_reward = 0\n",
    "        else:\n",
    "            alive_steps += 1\n",
    "            \n",
    "\n",
    "        # Save the network every 1000 iterations\n",
    "        if step > 0 and step % 1000 == 0:\n",
    "            agent.save(modelpath)\n",
    "            opponent.save(modelpath_opponent)\n",
    "\n",
    "        # Iterate to next loop\n",
    "        step += 1\n",
    "        # Make new obs the current obs\n",
    "        agent.obs = new_obs\n",
    "        agent.state = new_state\n",
    "\n",
    "    # Save model after all steps\n",
    "    agent.save(modelpath)\n",
    "    opponent.save(modelpath_opponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps: 15256\n",
      "Step [0] -- Random [0.99]\n",
      "Survived [0] steps\n",
      "Total reward [0.777961015701294]\n",
      "Totla reward opponent [-1.0]\n",
      "Survived [0] steps\n",
      "Total reward [0.8037588596343994]\n",
      "Totla reward opponent [-2.0]\n",
      "Survived [1] steps\n",
      "Total reward [1.3425829410552979]\n",
      "Totla reward opponent [-2.237318754196167]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-2.237318754196167]\n",
      "Survived [1] steps\n",
      "Total reward [1.5973045825958252]\n",
      "Totla reward opponent [-2.4402196407318115]\n",
      "Survived [1] steps\n",
      "Total reward [-0.1774071455001831]\n",
      "Totla reward opponent [-1.6199557781219482]\n",
      "Survived [0] steps\n",
      "Total reward [0.7711172103881836]\n",
      "Totla reward opponent [-2.6199557781219482]\n",
      "Survived [1] steps\n",
      "Total reward [-0.21980035305023193]\n",
      "Totla reward opponent [-1.8132696151733398]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-1.8132696151733398]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-1.8132696151733398]\n",
      "Survived [1] steps\n",
      "Total reward [-0.26544618606567383]\n",
      "Totla reward opponent [-1.0767362117767334]\n",
      "Survived [1] steps\n",
      "Total reward [-0.31167006492614746]\n",
      "Totla reward opponent [-0.40442919731140137]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-0.40442919731140137]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-0.40442919731140137]\n",
      "Survived [2] steps\n",
      "Total reward [0.5813033580780029]\n",
      "Totla reward opponent [1.1692699193954468]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [1.1692699193954468]\n",
      "Survived [0] steps\n",
      "Total reward [0.7974743843078613]\n",
      "Totla reward opponent [0.16926991939544678]\n",
      "Survived [0] steps\n",
      "Total reward [0.7580206394195557]\n",
      "Totla reward opponent [-0.8307300806045532]\n",
      "Survived [1] steps\n",
      "Total reward [-0.20723533630371094]\n",
      "Totla reward opponent [-0.038422226905822754]\n",
      "Survived [0] steps\n",
      "Total reward [0.8122673034667969]\n",
      "Totla reward opponent [-1.0384222269058228]\n",
      "Survived [1] steps\n",
      "Total reward [1.3452293872833252]\n",
      "Totla reward opponent [-1.4222661256790161]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-1.4222661256790161]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-1.4222661256790161]\n",
      "Survived [0] steps\n",
      "Total reward [0.7603603601455688]\n",
      "Totla reward opponent [-2.422266125679016]\n",
      "Survived [1] steps\n",
      "Total reward [1.5360798835754395]\n",
      "Totla reward opponent [-2.666634202003479]\n",
      "Survived [0] steps\n",
      "Total reward [0.7944779396057129]\n",
      "Totla reward opponent [-3.666634202003479]\n",
      "Survived [1] steps\n",
      "Total reward [-0.2512737512588501]\n",
      "Totla reward opponent [-2.9758576154708862]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-2.9758576154708862]\n",
      "Survived [1] steps\n",
      "Total reward [-0.24074578285217285]\n",
      "Totla reward opponent [-2.2698360681533813]\n",
      "Survived [1] steps\n",
      "Total reward [1.5103552341461182]\n",
      "Totla reward opponent [-2.4778255224227905]\n",
      "Survived [1] steps\n",
      "Total reward [1.514209270477295]\n",
      "Totla reward opponent [-2.772579550743103]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-2.772579550743103]\n",
      "Survived [1] steps\n",
      "Total reward [-0.18262410163879395]\n",
      "Totla reward opponent [-1.989488959312439]\n",
      "Survived [1] steps\n",
      "Total reward [-0.16610407829284668]\n",
      "Totla reward opponent [-1.197711706161499]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-1.197711706161499]\n",
      "Survived [1] steps\n",
      "Total reward [1.5385266542434692]\n",
      "Totla reward opponent [-1.434388518333435]\n",
      "Survived [3] steps\n",
      "Total reward [2.6637362241744995]\n",
      "Totla reward opponent [-0.4359046220779419]\n",
      "Survived [2] steps\n",
      "Total reward [2.2481908798217773]\n",
      "Totla reward opponent [0.03265571594238281]\n",
      "Survived [1] steps\n",
      "Total reward [-0.3358633518218994]\n",
      "Totla reward opponent [0.7021472454071045]\n",
      "Survived [1] steps\n",
      "Total reward [-0.2829258441925049]\n",
      "Totla reward opponent [1.4147802591323853]\n",
      "Survived [1] steps\n",
      "Total reward [1.5895071029663086]\n",
      "Totla reward opponent [1.230481505393982]\n",
      "Survived [0] steps\n",
      "Total reward [0.6083054542541504]\n",
      "Totla reward opponent [0.23048150539398193]\n",
      "Survived [0] steps\n",
      "Total reward [0.8014063835144043]\n",
      "Totla reward opponent [-0.7695184946060181]\n",
      "Survived [0] steps\n",
      "Total reward [0.7668061256408691]\n",
      "Totla reward opponent [-1.769518494606018]\n",
      "Survived [2] steps\n",
      "Total reward [2.3630937337875366]\n",
      "Totla reward opponent [-1.226222276687622]\n",
      "Survived [1] steps\n",
      "Total reward [1.4870100021362305]\n",
      "Totla reward opponent [-1.5063412189483643]\n",
      "Survived [1] steps\n",
      "Total reward [-0.17100191116333008]\n",
      "Totla reward opponent [-0.7127135992050171]\n",
      "Survived [1] steps\n",
      "Total reward [-0.2059307098388672]\n",
      "Totla reward opponent [0.08006167411804199]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [0.08006167411804199]\n",
      "Survived [0] steps\n",
      "Total reward [0.8059563636779785]\n",
      "Totla reward opponent [-0.919938325881958]\n",
      "Survived [1] steps\n",
      "Total reward [-0.1873176097869873]\n",
      "Totla reward opponent [-0.10363876819610596]\n",
      "Survived [1] steps\n",
      "Total reward [-0.2357543706893921]\n",
      "Totla reward opponent [0.5961016416549683]\n",
      "Survived [1] steps\n",
      "Total reward [1.5612061023712158]\n",
      "Totla reward opponent [0.35199904441833496]\n",
      "Survived [2] steps\n",
      "Total reward [0.5668611526489258]\n",
      "Totla reward opponent [1.8860615491867065]\n",
      "Survived [0] steps\n",
      "Total reward [0.7294814586639404]\n",
      "Totla reward opponent [0.8860615491867065]\n",
      "Survived [0] steps\n",
      "Total reward [0.7736917734146118]\n",
      "Totla reward opponent [-0.11393845081329346]\n",
      "Survived [0] steps\n",
      "Total reward [0.7765772342681885]\n",
      "Totla reward opponent [-1.1139384508132935]\n",
      "Survived [1] steps\n",
      "Total reward [-0.28449392318725586]\n",
      "Totla reward opponent [-0.398335337638855]\n",
      "Survived [0] steps\n",
      "Total reward [0.7784609794616699]\n",
      "Totla reward opponent [-1.398335337638855]\n",
      "Survived [0] steps\n",
      "Total reward [0.7705028057098389]\n",
      "Totla reward opponent [-2.398335337638855]\n",
      "Survived [1] steps\n",
      "Total reward [1.5003833770751953]\n",
      "Totla reward opponent [-2.5976642370224]\n",
      "Survived [2] steps\n",
      "Total reward [0.5418229103088379]\n",
      "Totla reward opponent [-1.0720175504684448]\n",
      "Survived [1] steps\n",
      "Total reward [-0.2624548673629761]\n",
      "Totla reward opponent [-0.31138622760772705]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-0.31138622760772705]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-0.31138622760772705]\n",
      "Survived [0] steps\n",
      "Total reward [0.7343277931213379]\n",
      "Totla reward opponent [-1.311386227607727]\n",
      "Survived [1] steps\n",
      "Total reward [1.5728528499603271]\n",
      "Totla reward opponent [-1.5699301958084106]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-1.5699301958084106]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-1.5699301958084106]\n",
      "Survived [0] steps\n",
      "Total reward [0.7776856422424316]\n",
      "Totla reward opponent [-2.5699301958084106]\n",
      "Survived [1] steps\n",
      "Total reward [1.4601528644561768]\n",
      "Totla reward opponent [-2.9562500715255737]\n",
      "Survived [1] steps\n",
      "Total reward [1.4354381561279297]\n",
      "Totla reward opponent [-3.2716487646102905]\n",
      "Survived [1] steps\n",
      "Total reward [-0.31670498847961426]\n",
      "Totla reward opponent [-2.6078288555145264]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-2.6078288555145264]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-2.6078288555145264]\n",
      "Survived [0] steps\n",
      "Total reward [-1.0]\n",
      "Totla reward opponent [-2.6078288555145264]\n",
      "Survived [1] steps\n",
      "Total reward [1.3401477336883545]\n",
      "Totla reward opponent [-2.980130434036255]\n"
     ]
    }
   ],
   "source": [
    "from grid2op.Reward import *\n",
    "from grid2op.Action import *\n",
    "\n",
    "# agent params\n",
    "DEFAULT_PRE_STEPS = 256\n",
    "DEFAULT_N_FRAMES = 4\n",
    "DEFAULT_BATCH_SIZE = 32\n",
    "DEFAULT_VERBOSE = True\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# training params\n",
    "n_iter = 15000\n",
    "env_name = \"rte_case14_realistic\"\n",
    "# env = make(env_name, reward_class=IllegalBadReward)\n",
    "env = make(env_name,\n",
    "           action_class=TopologyChangeAndDispatchAction,\n",
    "           reward_class=CombinedScaledReward)\n",
    "\n",
    "# Register custom reward for training\n",
    "cr = env._reward_helper.template_reward\n",
    "#cr.addReward(\"overflow\", CloseToOverflowReward(), 1.0)\n",
    "cr.addReward(\"game\", GameplayReward(), 1.0)\n",
    "#cr.addReward(\"recolines\", LinesReconnectedReward(), 1.0)\n",
    "cr.addReward(\"l2rpn\", L2RPNReward(), 2.0/float(env.n_line))\n",
    "# Initialize custom rewards\n",
    "cr.initialize(env)\n",
    "# Set reward range to something managable\n",
    "cr.set_range(-1.0, 1.0)\n",
    "\n",
    "opponent_train_freq = 1 # how often to train opponent freq=1 means every step\n",
    "    \n",
    "agent_name = \"DDDQN\"\n",
    "save_path = \"saved_agent_DDDQN_adversarial_{}\".format(n_iter)\n",
    "log_path=\"tf_logs_DDDQN\"\n",
    "\n",
    "opponent_name = \"DDDQN_opponent_DDDQN_adversarial_{}\".format(n_iter)\n",
    "save_path_opponent = 'saved_opponent_DDDQN_adversarial_{}_{}'.format(n_iter, opponent_train_freq)\n",
    "log_path = \"tf_logs_DDDQN\"\n",
    "\n",
    "agent = DoubleDuelingDQN(env.observation_space, env.action_space, name=agent_name,\n",
    "                         is_training=True, learning_rate=learning_rate)\n",
    "opponent = DoubleDuelingDQN(env.observation_space, env.action_space, name=opponent_name,\n",
    "                            is_training=True, learning_rate=learning_rate)\n",
    "\n",
    "train(env, agent, opponent, n_iter, save_path, save_path_opponent, DEFAULT_PRE_STEPS, log_path,\n",
    "      opponent_train_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate DuelingDQN vs. other agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.MakeEnv import make\n",
    "from grid2op.Runner import Runner\n",
    "from grid2op.Reward import *\n",
    "from grid2op.Action import *\n",
    "from grid2op.Parameters import Parameters\n",
    "from l2rpn_baselines.utils.save_log_gif import save_log_gif\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episode = 10 # number of episodes to evaluate\n",
    "log_path = './logs-evals'\n",
    "nb_process = 1 # number of cores to use\n",
    "max_iter = 150 # maximum number of steps per scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded network from: ./saved_agent_DDDQN_topology-change-and-dispatch-action_combined-reward_50000/DDDQN.h5\n",
      "INFO: \"Sequential runner used.\"\n",
      "INFO: \"Env: 4.18s\n",
      "\t - apply act 0.83s\n",
      "\t - run pf: 3.05s\n",
      "\t - env update + observation: 0.29s\n",
      "Agent: 5.21s\n",
      "Total time: 9.46s\n",
      "Cumulative reward: 2076451.875000\"\n",
      "INFO: \"Env: 0.49s\n",
      "\t - apply act 0.09s\n",
      "\t - run pf: 0.36s\n",
      "\t - env update + observation: 0.03s\n",
      "Agent: 0.42s\n",
      "Total time: 0.91s\n",
      "Cumulative reward: 29856.660156\"\n",
      "INFO: \"Env: 0.77s\n",
      "\t - apply act 0.15s\n",
      "\t - run pf: 0.58s\n",
      "\t - env update + observation: 0.05s\n",
      "Agent: 0.81s\n",
      "Total time: 1.59s\n",
      "Cumulative reward: 110212.765625\"\n",
      "INFO: \"Env: 2.97s\n",
      "\t - apply act 0.61s\n",
      "\t - run pf: 2.16s\n",
      "\t - env update + observation: 0.20s\n",
      "Agent: 3.33s\n",
      "Total time: 6.34s\n",
      "Cumulative reward: 1048107.687500\"\n",
      "INFO: \"Env: 4.58s\n",
      "\t - apply act 0.93s\n",
      "\t - run pf: 3.33s\n",
      "\t - env update + observation: 0.32s\n",
      "Agent: 5.44s\n",
      "Total time: 10.09s\n",
      "Cumulative reward: 2644980.250000\"\n",
      "INFO: \"Env: 4.99s\n",
      "\t - apply act 1.02s\n",
      "\t - run pf: 3.63s\n",
      "\t - env update + observation: 0.34s\n",
      "Agent: 5.97s\n",
      "Total time: 11.05s\n",
      "Cumulative reward: 2318963.500000\"\n",
      "INFO: \"Env: 1.11s\n",
      "\t - apply act 0.23s\n",
      "\t - run pf: 0.80s\n",
      "\t - env update + observation: 0.08s\n",
      "Agent: 1.20s\n",
      "Total time: 2.33s\n",
      "Cumulative reward: 245475.343750\"\n",
      "INFO: \"Env: 4.30s\n",
      "\t - apply act 0.87s\n",
      "\t - run pf: 3.15s\n",
      "\t - env update + observation: 0.29s\n",
      "Agent: 5.02s\n",
      "Total time: 9.38s\n",
      "Cumulative reward: 1804978.250000\"\n",
      "INFO: \"Env: 4.78s\n",
      "\t - apply act 0.96s\n",
      "\t - run pf: 3.48s\n",
      "\t - env update + observation: 0.34s\n",
      "Agent: 5.53s\n",
      "Total time: 10.38s\n",
      "Cumulative reward: 2047603.375000\"\n",
      "INFO: \"Env: 0.69s\n",
      "\t - apply act 0.14s\n",
      "\t - run pf: 0.50s\n",
      "\t - env update + observation: 0.04s\n",
      "Agent: 0.70s\n",
      "Total time: 1.41s\n",
      "Cumulative reward: 101081.218750\"\n",
      "Evaluation summary:\n",
      "chronics at: 000\ttotal reward: 2076451.875000\ttime steps: 126/150\n",
      "chronics at: 001\ttotal reward: 29856.660156\ttime steps: 13/150\n",
      "chronics at: 002\ttotal reward: 110212.765625\ttime steps: 22/150\n",
      "chronics at: 003\ttotal reward: 1048107.687500\ttime steps: 94/150\n",
      "chronics at: 004\ttotal reward: 2644980.250000\ttime steps: 150/150\n",
      "chronics at: 005\ttotal reward: 2318963.500000\ttime steps: 137/150\n",
      "chronics at: 006\ttotal reward: 245475.343750\ttime steps: 32/150\n",
      "chronics at: 007\ttotal reward: 1804978.250000\ttime steps: 114/150\n",
      "chronics at: 008\ttotal reward: 2047603.375000\ttime steps: 126/150\n",
      "chronics at: 009\ttotal reward: 101081.218750\ttime steps: 21/150\n"
     ]
    }
   ],
   "source": [
    "env_name = \"rte_case14_realistic\"\n",
    "agent = DoubleDuelingDQN(env.observation_space, env.action_space, name='D3QN', is_training=False)\n",
    "agent.load('./saved_agent_DDDQN_topology-change-and-dispatch-action_combined-reward_50000/DDDQN.h5')\n",
    "\n",
    "_ = evaluate(env_name, agent, TopologyChangeAndDispatchAction, log_path, nb_episode=nb_episode, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/grid2op/MakeEnv/Make.py:269: UserWarning:\n",
      "\n",
      "You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: \"Sequential runner used.\"\n",
      "INFO: \"Env: 3.58s\n",
      "\t - apply act 0.51s\n",
      "\t - run pf: 2.79s\n",
      "\t - env update + observation: 0.27s\n",
      "Agent: 0.01s\n",
      "Total time: 3.64s\n",
      "Cumulative reward: 32312.628906\"\n",
      "INFO: \"Env: 3.50s\n",
      "\t - apply act 0.51s\n",
      "\t - run pf: 2.72s\n",
      "\t - env update + observation: 0.27s\n",
      "Agent: 0.01s\n",
      "Total time: 3.56s\n",
      "Cumulative reward: 30661.560547\"\n",
      "INFO: \"Env: 3.74s\n",
      "\t - apply act 0.54s\n",
      "\t - run pf: 2.91s\n",
      "\t - env update + observation: 0.29s\n",
      "Agent: 0.01s\n",
      "Total time: 3.81s\n",
      "Cumulative reward: 32312.628906\"\n",
      "INFO: \"Env: 3.71s\n",
      "\t - apply act 0.53s\n",
      "\t - run pf: 2.90s\n",
      "\t - env update + observation: 0.28s\n",
      "Agent: 0.01s\n",
      "Total time: 3.78s\n",
      "Cumulative reward: 30661.560547\"\n",
      "INFO: \"Env: 3.60s\n",
      "\t - apply act 0.52s\n",
      "\t - run pf: 2.81s\n",
      "\t - env update + observation: 0.27s\n",
      "Agent: 0.01s\n",
      "Total time: 3.67s\n",
      "Cumulative reward: 32312.628906\"\n",
      "INFO: \"Env: 3.93s\n",
      "\t - apply act 0.57s\n",
      "\t - run pf: 3.06s\n",
      "\t - env update + observation: 0.30s\n",
      "Agent: 0.01s\n",
      "Total time: 4.00s\n",
      "Cumulative reward: 30661.560547\"\n",
      "INFO: \"Env: 3.58s\n",
      "\t - apply act 0.52s\n",
      "\t - run pf: 2.78s\n",
      "\t - env update + observation: 0.27s\n",
      "Agent: 0.01s\n",
      "Total time: 3.64s\n",
      "Cumulative reward: 32312.628906\"\n",
      "INFO: \"Env: 3.53s\n",
      "\t - apply act 0.52s\n",
      "\t - run pf: 2.75s\n",
      "\t - env update + observation: 0.27s\n",
      "Agent: 0.01s\n",
      "Total time: 3.60s\n",
      "Cumulative reward: 30661.560547\"\n",
      "INFO: \"Env: 3.55s\n",
      "\t - apply act 0.52s\n",
      "\t - run pf: 2.76s\n",
      "\t - env update + observation: 0.27s\n",
      "Agent: 0.01s\n",
      "Total time: 3.62s\n",
      "Cumulative reward: 32312.628906\"\n",
      "INFO: \"Env: 3.68s\n",
      "\t - apply act 0.53s\n",
      "\t - run pf: 2.87s\n",
      "\t - env update + observation: 0.29s\n",
      "Agent: 0.01s\n",
      "Total time: 3.75s\n",
      "Cumulative reward: 30661.560547\"\n",
      "Evaluation summary:\n",
      "chronics at: 000\ttotal reward: 32312.628906\ttime steps: 150/150\n",
      "chronics at: 001\ttotal reward: 30661.560547\ttime steps: 150/150\n",
      "chronics at: 000\ttotal reward: 32312.628906\ttime steps: 150/150\n",
      "chronics at: 001\ttotal reward: 30661.560547\ttime steps: 150/150\n",
      "chronics at: 000\ttotal reward: 32312.628906\ttime steps: 150/150\n",
      "chronics at: 001\ttotal reward: 30661.560547\ttime steps: 150/150\n",
      "chronics at: 000\ttotal reward: 32312.628906\ttime steps: 150/150\n",
      "chronics at: 001\ttotal reward: 30661.560547\ttime steps: 150/150\n",
      "chronics at: 000\ttotal reward: 32312.628906\ttime steps: 150/150\n",
      "chronics at: 001\ttotal reward: 30661.560547\ttime steps: 150/150\n"
     ]
    }
   ],
   "source": [
    "log_path = './log-eval-nothing'\n",
    "env_name = \"rte_case14_realistic\"\n",
    "_ = evaluate_nothing_agent(env_name, TopologyChangeAndDispatchAction, log_path, nb_episode=nb_episode,\n",
    "                           max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent_exec_times.npz',\n",
       " 'disc_lines_cascading_failure.npz',\n",
       " 'actions.npz',\n",
       " 'observations.npz',\n",
       " 'rewards.npz',\n",
       " 'other_rewards.json',\n",
       " 'opponent_attack.npz',\n",
       " 'episode_meta.json',\n",
       " '000.gif',\n",
       " '_parameters.json',\n",
       " 'env_modifications.npz',\n",
       " 'episode_times.json']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./logs-evals/000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](./logs-evals/001/001.gif \"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
