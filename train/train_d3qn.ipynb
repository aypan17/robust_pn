{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "binary-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "import grid2op\n",
    "from d3qn.adversary import D3QN_Opponent\n",
    "from grid2op.Agent import DoNothingAgent\n",
    "from grid2op.Action import TopologyChangeAndDispatchAction\n",
    "from grid2op.Reward import CombinedScaledReward, L2RPNSandBoxScore, L2RPNReward, GameplayReward\n",
    "from l2rpn_baselines.DoubleDuelingDQN.DoubleDuelingDQNConfig import DoubleDuelingDQNConfig as cfg\n",
    "\n",
    "from kaist_agent.Kaist import Kaist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "genuine-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIMESTEP = 7 * 288\n",
    "\n",
    "def train_adversary(env, agent, opponent, num_pre_training_steps, n_iter, save_path, log_path):\n",
    "    # Make sure we can fill the experience buffer\n",
    "    if num_pre_training_steps < opponent.batch_size * opponent.num_frames:\n",
    "        num_pre_training_steps = opponent.batch_size * opponent.num_frames\n",
    "        \n",
    "    # Loop vars\n",
    "    num_training_steps = n_iter\n",
    "    num_steps = num_pre_training_steps + num_training_steps\n",
    "    step = 0\n",
    "    alive_steps = 0\n",
    "    total_reward = 0\n",
    "    done = True\n",
    "    print(f\"Total number of steps: {num_steps}\")\n",
    "\n",
    "    # Create file system related vars\n",
    "    logpath = os.path.join(log_path, opponent.name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    modelpath = os.path.join(save_path, opponent.name + \".h5\")\n",
    "    opponent.tf_writer = tf.summary.create_file_writer(logpath, name=opponent.name)\n",
    "    opponent._save_hyperparameters(save_path, env, num_steps)\n",
    "    \n",
    "    while step < num_steps:\n",
    "        # Init first time or new episode\n",
    "        if done:\n",
    "            new_obs = env.reset() # This shouldn't raise\n",
    "            agent.reset(new_obs)\n",
    "            opponent.reset(new_obs)\n",
    "        if cfg.VERBOSE and step % 1000 == 0:\n",
    "            print(\"Step [{}] -- Random [{}]\".format(step, opponent.epsilon))\n",
    "\n",
    "        # Save current observation to stacking buffer\n",
    "        opponent._save_current_frame(opponent.state)\n",
    "\n",
    "        # Execute attack if allowed\n",
    "        if step <= num_pre_training_steps:\n",
    "            opponent.remaining_time = 0\n",
    "            attack, a = opponent._do_nothing, 0\n",
    "        else:\n",
    "            attack, a = opponent.attack(new_obs)\n",
    "\n",
    "        if a != 0:\n",
    "            print(f'ATTACK step {step}: disconnected {a}')\n",
    "            attack_obs, opp_reward, done, info = env.step(attack)\n",
    "            if info[\"is_illegal\"] or info[\"is_ambiguous\"] or \\\n",
    "               info[\"is_dispatching_illegal\"] or info[\"is_illegal_reco\"]:\n",
    "                if cfg.VERBOSE:\n",
    "                    print(attack, info)\n",
    "            new_obs = attack_obs\n",
    "            opponent.tell_attack_continues(None, None, None, None)\n",
    "\n",
    "        while opponent.remaining_time >= 0:\n",
    "            new_obs.time_before_cooldown_line[opponent.attack_line] = opponent.remaining_time\n",
    "            response = agent.act(new_obs, None, None)\n",
    "            new_obs, reward, done, info = env.step(response)\n",
    "            opponent.remaining_time -= 1\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Save new observation to stacking buffer\n",
    "        new_state = opponent.convert_obs(new_obs)\n",
    "        opponent._save_next_frame(new_state)\n",
    "\n",
    "        # Save to experience buffer\n",
    "        if len(opponent.frames2) == opponent.num_frames:\n",
    "            opponent.per_buffer.add(np.array(opponent.frames),\n",
    "                                a, -1 * reward,\n",
    "                                np.array(opponent.frames2),\n",
    "                                opponent.done)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        # Perform training when we have enough experience in buffer\n",
    "        if step >= num_pre_training_steps:\n",
    "            training_step = step - num_pre_training_steps\n",
    "            # Decay chance of random action\n",
    "            opponent.epsilon = opponent._adaptive_epsilon_decay(training_step)\n",
    "\n",
    "            # Perform training at given frequency\n",
    "            if step % cfg.UPDATE_FREQ == 0 and \\\n",
    "               len(opponent.per_buffer) >= opponent.batch_size:\n",
    "                # Perform training\n",
    "                opponent._batch_train(training_step, step)\n",
    "\n",
    "                if cfg.UPDATE_TARGET_SOFT_TAU > 0.0:\n",
    "                    tau = cfg.UPDATE_TARGET_SOFT_TAU\n",
    "                    # Update target network towards primary network\n",
    "                    opponent.policy_net.update_target_soft(opponent.target_net.model, tau)\n",
    "\n",
    "            # Every UPDATE_TARGET_HARD_FREQ trainings, update target completely\n",
    "            if cfg.UPDATE_TARGET_HARD_FREQ > 0 and \\\n",
    "               step % (cfg.UPDATE_FREQ * cfg.UPDATE_TARGET_HARD_FREQ) == 0:\n",
    "                opponent.policy_net.update_target_hard(opponent.target_net.model)\n",
    "        \n",
    "        if done:\n",
    "            opponent.epoch_rewards.append(-1 * total_reward)\n",
    "            opponent.epoch_alive.append(alive_steps)\n",
    "            if cfg.VERBOSE and step > num_pre_training_steps:\n",
    "                print(\"step {}: Survived [{}] steps\".format(step, alive_steps))\n",
    "                print(\"Total reward of agent [{}]\".format(total_reward))\n",
    "            alive_steps = 0\n",
    "            total_reward = 0         \n",
    "        else:\n",
    "            alive_steps += 1\n",
    "            \n",
    "        ######## After Each Step #######\n",
    "        if step > 0 and step % 2000 == 0: # save network every 5000 iters\n",
    "            opponent.save(modelpath)\n",
    "        step += 1\n",
    "        # Make new obs the current obs\n",
    "        opponent.obs = new_obs\n",
    "        opponent.state = new_state\n",
    "\n",
    "    # Save model after all steps\n",
    "    opponent.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "criminal-century",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'head_number': 8, 'n_history': 12, 'state_dim': 128, 'dropout': 0.0, 'sim_trial': 15, 'threshold': 0.35, 'max_low_len': 19, 'danger': 0.9, 'mask': 3, 'mask_hi': 19, 'use_order': True, 'device': 'cpu'}\n",
      "O: 72 S: 128 A: 108 (19)\n",
      "['2_3_0' '2_4_1' '0_4_2' '1_3_3' '1_4_4' '4_6_5' '4_7_6' '6_7_7' '7_8_8'\n",
      " '7_9_9' '8_9_10' '10_11_11' '1_10_12' '11_12_13' '12_13_14' '13_14_15'\n",
      " '13_15_16' '14_16_17' '9_16_18' '9_16_19' '12_16_20' '15_16_21'\n",
      " '16_17_22' '16_18_23' '18_19_24' '19_20_25' '20_21_26' '16_21_27'\n",
      " '16_21_28' '21_22_29' '21_23_30' '22_23_31' '23_24_32' '17_24_33'\n",
      " '23_25_34' '18_25_35' '21_26_36' '23_26_37' '23_26_38' '22_26_39'\n",
      " '26_27_40' '26_28_41' '27_28_42' '27_29_43' '28_29_44' '30_31_45'\n",
      " '5_32_46' '31_32_47' '16_33_48' '16_33_49' '29_33_50' '29_34_51'\n",
      " '33_34_52' '14_35_53' '16_35_54' '4_5_55' '26_30_56' '28_31_57'\n",
      " '32_33_58']\n",
      "Total number of steps: 10256\n",
      "Step [0] -- Random [0.99]\n",
      "ATTACK step 257: disconnected 17\n",
      "ATTACK step 258: disconnected 17\n",
      "ATTACK step 259: disconnected 17\n",
      "ATTACK step 260: disconnected 17\n",
      "ATTACK step 261: disconnected 17\n",
      "ATTACK step 262: disconnected 17\n",
      "ATTACK step 263: disconnected 17\n",
      "ATTACK step 264: disconnected 17\n",
      "ATTACK step 265: disconnected 17\n",
      "ATTACK step 266: disconnected 17\n",
      "ATTACK step 267: disconnected 17\n",
      "ATTACK step 268: disconnected 17\n",
      "ATTACK step 269: disconnected 17\n",
      "ATTACK step 270: disconnected 17\n",
      "ATTACK step 271: disconnected 17\n",
      "ATTACK step 272: disconnected 17\n",
      "ATTACK step 273: disconnected 17\n",
      "ATTACK step 274: disconnected 17\n",
      "ATTACK step 275: disconnected 17\n",
      "ATTACK step 276: disconnected 17\n",
      "ATTACK step 277: disconnected 17\n",
      "ATTACK step 278: disconnected 17\n",
      "ATTACK step 279: disconnected 17\n",
      "ATTACK step 280: disconnected 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 93657.44\n",
      "ATTACK step 309: disconnected 44\n",
      "step 309: Survived [309] steps\n",
      "Total reward of agent [269.82234835624695]\n",
      "ATTACK step 313: disconnected 44\n",
      "step 313: Survived [3] steps\n",
      "Total reward of agent [1.5816912651062012]\n",
      "ATTACK step 317: disconnected 40\n",
      "ATTACK step 318: disconnected 47\n",
      "ATTACK step 319: disconnected 40\n",
      "ATTACK step 320: disconnected 47\n",
      "ATTACK step 321: disconnected 40\n",
      "ATTACK step 322: disconnected 47\n",
      "ATTACK step 323: disconnected 40\n",
      "ATTACK step 324: disconnected 44\n",
      "step 324: Survived [10] steps\n",
      "Total reward of agent [7.66756796836853]\n",
      "ATTACK step 328: disconnected 44\n",
      "step 328: Survived [3] steps\n",
      "Total reward of agent [1.5476787090301514]\n",
      "ATTACK step 332: disconnected 47\n",
      "ATTACK step 333: disconnected 47\n",
      "ATTACK step 334: disconnected 47\n",
      "ATTACK step 335: disconnected 47\n",
      "ATTACK step 336: disconnected 47\n",
      "loss = 230619.36\n",
      "ATTACK step 337: disconnected 47\n",
      "ATTACK step 338: disconnected 47\n",
      "ATTACK step 339: disconnected 47\n",
      "ATTACK step 341: disconnected 47\n",
      "ATTACK step 342: disconnected 47\n",
      "ATTACK step 343: disconnected 47\n",
      "ATTACK step 344: disconnected 47\n",
      "ATTACK step 346: disconnected 47\n",
      "ATTACK step 355: disconnected 47\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - force disconnection of 1 powerlines ([58])\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('Powerline with ids [58] have been modified illegally (cooldown)',)], 'rewards': {}}\n",
      "ATTACK step 356: disconnected 47\n",
      "ATTACK step 357: disconnected 47\n",
      "ATTACK step 358: disconnected 47\n",
      "ATTACK step 359: disconnected 47\n",
      "ATTACK step 360: disconnected 47\n",
      "ATTACK step 361: disconnected 47\n",
      "ATTACK step 362: disconnected 47\n",
      "ATTACK step 363: disconnected 47\n",
      "ATTACK step 364: disconnected 47\n",
      "loss = 1013.2887\n",
      "ATTACK step 393: disconnected 17\n",
      "ATTACK step 394: disconnected 17\n",
      "ATTACK step 395: disconnected 17\n",
      "ATTACK step 396: disconnected 17\n",
      "ATTACK step 397: disconnected 17\n",
      "ATTACK step 398: disconnected 17\n",
      "ATTACK step 399: disconnected 17\n",
      "ATTACK step 400: disconnected 17\n",
      "ATTACK step 401: disconnected 17\n",
      "ATTACK step 402: disconnected 17\n",
      "ATTACK step 403: disconnected 17\n",
      "ATTACK step 404: disconnected 17\n",
      "ATTACK step 405: disconnected 17\n",
      "ATTACK step 409: disconnected 17\n",
      "ATTACK step 410: disconnected 17\n",
      "ATTACK step 411: disconnected 17\n",
      "ATTACK step 412: disconnected 17\n",
      "ATTACK step 418: disconnected 17\n",
      "ATTACK step 419: disconnected 17\n",
      "ATTACK step 420: disconnected 17\n",
      "loss = 26.073662\n",
      "loss = 31.252525\n",
      "loss = 7.2780886\n",
      "ATTACK step 589: disconnected 47\n",
      "ATTACK step 590: disconnected 47\n",
      "ATTACK step 591: disconnected 47\n",
      "ATTACK step 592: disconnected 47\n",
      "ATTACK step 593: disconnected 47\n",
      "ATTACK step 594: disconnected 47\n",
      "ATTACK step 595: disconnected 47\n",
      "ATTACK step 596: disconnected 47\n",
      "ATTACK step 597: disconnected 47\n",
      "ATTACK step 598: disconnected 47\n",
      "ATTACK step 599: disconnected 47\n",
      "ATTACK step 600: disconnected 47\n",
      "ATTACK step 601: disconnected 47\n",
      "ATTACK step 602: disconnected 47\n",
      "ATTACK step 603: disconnected 47\n",
      "ATTACK step 604: disconnected 47\n",
      "ATTACK step 605: disconnected 47\n",
      "ATTACK step 606: disconnected 47\n",
      "ATTACK step 607: disconnected 47\n",
      "ATTACK step 608: disconnected 47\n",
      "ATTACK step 609: disconnected 47\n",
      "ATTACK step 610: disconnected 47\n",
      "ATTACK step 611: disconnected 47\n",
      "ATTACK step 612: disconnected 47\n",
      "ATTACK step 613: disconnected 47\n",
      "ATTACK step 614: disconnected 47\n",
      "step 614: Survived [285] steps\n",
      "Total reward of agent [237.6733022928238]\n",
      "loss = 3.4827921\n",
      "loss = 73.418076\n",
      "loss = 11.059504\n",
      "loss = 46.55068\n",
      "loss = 19.787508\n",
      "loss = 2.0124145\n",
      "loss = 0.92453545\n",
      "ATTACK step 954: disconnected 45\n",
      "ATTACK step 955: disconnected 45\n",
      "step 955: Survived [340] steps\n",
      "Total reward of agent [290.9261364340782]\n",
      "ATTACK step 981: disconnected 45\n",
      "ATTACK step 982: disconnected 45\n",
      "ATTACK step 983: disconnected 45\n",
      "ATTACK step 984: disconnected 45\n",
      "step 984: Survived [28] steps\n",
      "Total reward of agent [23.41795063018799]\n",
      "ATTACK step 988: disconnected 45\n",
      "ATTACK step 989: disconnected 45\n",
      "step 989: Survived [4] steps\n",
      "Total reward of agent [2.471732974052429]\n",
      "Step [1000] -- Random [0.9847456352762105]\n",
      "loss = 477.0086\n",
      "ATTACK step 1009: disconnected 45\n",
      "ATTACK step 1010: disconnected 45\n",
      "ATTACK step 1011: disconnected 45\n",
      "ATTACK step 1012: disconnected 45\n",
      "ATTACK step 1013: disconnected 45\n",
      "ATTACK step 1014: disconnected 45\n",
      "ATTACK step 1015: disconnected 45\n",
      "ATTACK step 1016: disconnected 45\n",
      "ATTACK step 1017: disconnected 45\n",
      "ATTACK step 1018: disconnected 45\n",
      "step 1018: Survived [28] steps\n",
      "Total reward of agent [23.67272174358368]\n",
      "ATTACK step 1022: disconnected 45\n",
      "ATTACK step 1023: disconnected 45\n",
      "step 1023: Survived [4] steps\n",
      "Total reward of agent [2.461829423904419]\n",
      "ATTACK step 1037: disconnected 45\n",
      "ATTACK step 1038: disconnected 45\n",
      "step 1038: Survived [14] steps\n",
      "Total reward of agent [11.128011107444763]\n",
      "ATTACK step 1042: disconnected 45\n",
      "ATTACK step 1043: disconnected 45\n",
      "ATTACK step 1044: disconnected 45\n",
      "ATTACK step 1045: disconnected 45\n",
      "ATTACK step 1046: disconnected 45\n",
      "ATTACK step 1047: disconnected 45\n",
      "ATTACK step 1048: disconnected 45\n",
      "ATTACK step 1049: disconnected 45\n",
      "ATTACK step 1050: disconnected 45\n",
      "ATTACK step 1051: disconnected 45\n",
      "ATTACK step 1052: disconnected 45\n",
      "step 1052: Survived [13] steps\n",
      "Total reward of agent [10.474151253700256]\n",
      "ATTACK step 1056: disconnected 45\n",
      "ATTACK step 1057: disconnected 45\n",
      "step 1057: Survived [4] steps\n",
      "Total reward of agent [2.4669406414031982]\n",
      "ATTACK step 1061: disconnected 45\n",
      "loss = 122.34932\n",
      "ATTACK step 1065: disconnected 45\n",
      "ATTACK step 1066: disconnected 45\n",
      "step 1066: Survived [8] steps\n",
      "Total reward of agent [5.205627679824829]\n",
      "ATTACK step 1070: disconnected 45\n",
      "ATTACK step 1071: disconnected 45\n",
      "step 1071: Survived [4] steps\n",
      "Total reward of agent [2.4806647300720215]\n",
      "ATTACK step 1075: disconnected 45\n",
      "ATTACK step 1076: disconnected 45\n",
      "step 1076: Survived [4] steps\n",
      "Total reward of agent [2.4656429290771484]\n",
      "ATTACK step 1080: disconnected 45\n",
      "ATTACK step 1081: disconnected 45\n",
      "step 1081: Survived [4] steps\n",
      "Total reward of agent [2.434613347053528]\n",
      "ATTACK step 1085: disconnected 45\n",
      "ATTACK step 1086: disconnected 45\n",
      "step 1086: Survived [4] steps\n",
      "Total reward of agent [2.4457218647003174]\n",
      "ATTACK step 1090: disconnected 45\n",
      "ATTACK step 1091: disconnected 45\n",
      "step 1091: Survived [4] steps\n",
      "Total reward of agent [2.4282445907592773]\n",
      "ATTACK step 1095: disconnected 45\n",
      "ATTACK step 1096: disconnected 45\n",
      "step 1096: Survived [4] steps\n",
      "Total reward of agent [2.4483349323272705]\n",
      "ATTACK step 1100: disconnected 45\n",
      "ATTACK step 1101: disconnected 45\n",
      "ATTACK step 1102: disconnected 45\n",
      "step 1102: Survived [5] steps\n",
      "Total reward of agent [2.6299009323120117]\n",
      "ATTACK step 1106: disconnected 45\n",
      "ATTACK step 1107: disconnected 45\n",
      "step 1107: Survived [4] steps\n",
      "Total reward of agent [2.4440956115722656]\n",
      "ATTACK step 1111: disconnected 45\n",
      "ATTACK step 1112: disconnected 45\n",
      "step 1112: Survived [4] steps\n",
      "Total reward of agent [2.4299395084381104]\n",
      "ATTACK step 1116: disconnected 45\n",
      "ATTACK step 1117: disconnected 45\n",
      "step 1117: Survived [4] steps\n",
      "Total reward of agent [2.4844634532928467]\n",
      "loss = 639.0562\n",
      "ATTACK step 1121: disconnected 45\n",
      "ATTACK step 1122: disconnected 45\n",
      "step 1122: Survived [4] steps\n",
      "Total reward of agent [2.4508697986602783]\n",
      "ATTACK step 1126: disconnected 45\n",
      "ATTACK step 1127: disconnected 45\n",
      "step 1127: Survived [4] steps\n",
      "Total reward of agent [2.477508306503296]\n",
      "ATTACK step 1131: disconnected 45\n",
      "ATTACK step 1132: disconnected 45\n",
      "step 1132: Survived [4] steps\n",
      "Total reward of agent [2.4531586170196533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK step 1136: disconnected 45\n",
      "ATTACK step 1137: disconnected 45\n",
      "step 1137: Survived [4] steps\n",
      "Total reward of agent [2.450249195098877]\n",
      "ATTACK step 1141: disconnected 45\n",
      "ATTACK step 1142: disconnected 45\n",
      "step 1142: Survived [4] steps\n",
      "Total reward of agent [2.504326343536377]\n",
      "ATTACK step 1146: disconnected 45\n",
      "ATTACK step 1147: disconnected 45\n",
      "step 1147: Survived [4] steps\n",
      "Total reward of agent [2.446071982383728]\n",
      "ATTACK step 1151: disconnected 45\n",
      "ATTACK step 1152: disconnected 45\n",
      "step 1152: Survived [4] steps\n",
      "Total reward of agent [2.484434723854065]\n",
      "ATTACK step 1156: disconnected 45\n",
      "ATTACK step 1157: disconnected 45\n",
      "step 1157: Survived [4] steps\n",
      "Total reward of agent [2.5183258056640625]\n",
      "ATTACK step 1161: disconnected 45\n",
      "ATTACK step 1162: disconnected 45\n",
      "step 1162: Survived [4] steps\n",
      "Total reward of agent [2.4634045362472534]\n",
      "ATTACK step 1166: disconnected 45\n",
      "ATTACK step 1167: disconnected 45\n",
      "step 1167: Survived [4] steps\n",
      "Total reward of agent [2.484995722770691]\n",
      "ATTACK step 1171: disconnected 45\n",
      "ATTACK step 1172: disconnected 45\n",
      "step 1172: Survived [4] steps\n",
      "Total reward of agent [2.446330428123474]\n",
      "ATTACK step 1176: disconnected 45\n",
      "loss = 342.63614\n",
      "ATTACK step 1177: disconnected 45\n",
      "step 1177: Survived [4] steps\n",
      "Total reward of agent [2.4177403450012207]\n",
      "ATTACK step 1181: disconnected 45\n",
      "ATTACK step 1182: disconnected 45\n",
      "step 1182: Survived [4] steps\n",
      "Total reward of agent [2.479994535446167]\n",
      "ATTACK step 1186: disconnected 45\n",
      "ATTACK step 1187: disconnected 45\n",
      "step 1187: Survived [4] steps\n",
      "Total reward of agent [2.4342522621154785]\n",
      "ATTACK step 1191: disconnected 45\n",
      "ATTACK step 1192: disconnected 45\n",
      "step 1192: Survived [4] steps\n",
      "Total reward of agent [2.491966485977173]\n",
      "ATTACK step 1196: disconnected 45\n",
      "ATTACK step 1197: disconnected 45\n",
      "step 1197: Survived [4] steps\n",
      "Total reward of agent [2.457953691482544]\n",
      "ATTACK step 1201: disconnected 45\n",
      "ATTACK step 1202: disconnected 45\n",
      "step 1202: Survived [4] steps\n",
      "Total reward of agent [2.4763731956481934]\n",
      "ATTACK step 1206: disconnected 45\n",
      "ATTACK step 1207: disconnected 45\n",
      "step 1207: Survived [4] steps\n",
      "Total reward of agent [2.4143707752227783]\n",
      "ATTACK step 1211: disconnected 45\n",
      "ATTACK step 1212: disconnected 45\n",
      "step 1212: Survived [4] steps\n",
      "Total reward of agent [2.4883203506469727]\n",
      "ATTACK step 1216: disconnected 45\n",
      "ATTACK step 1217: disconnected 45\n",
      "step 1217: Survived [4] steps\n",
      "Total reward of agent [2.4150381088256836]\n",
      "ATTACK step 1221: disconnected 45\n",
      "ATTACK step 1222: disconnected 45\n",
      "step 1222: Survived [4] steps\n",
      "Total reward of agent [2.474015712738037]\n",
      "ATTACK step 1226: disconnected 45\n",
      "ATTACK step 1227: disconnected 45\n",
      "step 1227: Survived [4] steps\n",
      "Total reward of agent [2.4582780599594116]\n",
      "ATTACK step 1231: disconnected 45\n",
      "ATTACK step 1232: disconnected 45\n",
      "loss = 468.29138\n",
      "step 1232: Survived [4] steps\n",
      "Total reward of agent [2.423962950706482]\n",
      "ATTACK step 1236: disconnected 45\n",
      "ATTACK step 1237: disconnected 45\n",
      "step 1237: Survived [4] steps\n",
      "Total reward of agent [2.4776443243026733]\n",
      "ATTACK step 1241: disconnected 45\n",
      "ATTACK step 1242: disconnected 45\n",
      "step 1242: Survived [4] steps\n",
      "Total reward of agent [2.4637500047683716]\n",
      "ATTACK step 1246: disconnected 45\n",
      "ATTACK step 1247: disconnected 45\n",
      "step 1247: Survived [4] steps\n",
      "Total reward of agent [2.406258702278137]\n",
      "ATTACK step 1251: disconnected 45\n",
      "ATTACK step 1252: disconnected 45\n",
      "step 1252: Survived [4] steps\n",
      "Total reward of agent [2.4683583974838257]\n",
      "ATTACK step 1256: disconnected 45\n",
      "ATTACK step 1257: disconnected 45\n",
      "step 1257: Survived [4] steps\n",
      "Total reward of agent [2.464594841003418]\n",
      "ATTACK step 1261: disconnected 45\n",
      "ATTACK step 1262: disconnected 45\n",
      "step 1262: Survived [4] steps\n",
      "Total reward of agent [2.4391558170318604]\n",
      "ATTACK step 1266: disconnected 45\n",
      "ATTACK step 1267: disconnected 45\n",
      "step 1267: Survived [4] steps\n",
      "Total reward of agent [2.4752326011657715]\n",
      "ATTACK step 1271: disconnected 45\n",
      "ATTACK step 1272: disconnected 45\n",
      "step 1272: Survived [4] steps\n",
      "Total reward of agent [2.464636445045471]\n",
      "ATTACK step 1276: disconnected 45\n",
      "ATTACK step 1277: disconnected 45\n",
      "step 1277: Survived [4] steps\n",
      "Total reward of agent [2.411315083503723]\n",
      "ATTACK step 1281: disconnected 45\n",
      "ATTACK step 1282: disconnected 45\n",
      "step 1282: Survived [4] steps\n",
      "Total reward of agent [2.4621347188949585]\n",
      "ATTACK step 1286: disconnected 45\n",
      "ATTACK step 1287: disconnected 45\n",
      "step 1287: Survived [4] steps\n",
      "Total reward of agent [2.4650663137435913]\n",
      "loss = 609.8728\n",
      "ATTACK step 1291: disconnected 45\n",
      "ATTACK step 1292: disconnected 45\n",
      "step 1292: Survived [4] steps\n",
      "Total reward of agent [2.477579116821289]\n",
      "ATTACK step 1296: disconnected 45\n",
      "ATTACK step 1297: disconnected 45\n",
      "step 1297: Survived [4] steps\n",
      "Total reward of agent [2.4042574167251587]\n",
      "ATTACK step 1301: disconnected 45\n",
      "ATTACK step 1302: disconnected 45\n",
      "step 1302: Survived [4] steps\n",
      "Total reward of agent [2.402450680732727]\n",
      "ATTACK step 1306: disconnected 45\n",
      "ATTACK step 1307: disconnected 45\n",
      "step 1307: Survived [4] steps\n",
      "Total reward of agent [2.4793643951416016]\n",
      "ATTACK step 1311: disconnected 45\n",
      "ATTACK step 1312: disconnected 45\n",
      "ATTACK step 1313: disconnected 45\n",
      "ATTACK step 1314: disconnected 45\n",
      "ATTACK step 1315: disconnected 45\n",
      "ATTACK step 1316: disconnected 45\n",
      "ATTACK step 1317: disconnected 45\n",
      "ATTACK step 1318: disconnected 45\n",
      "step 1318: Survived [10] steps\n",
      "Total reward of agent [7.801560997962952]\n",
      "ATTACK step 1322: disconnected 45\n",
      "ATTACK step 1323: disconnected 45\n",
      "step 1323: Survived [4] steps\n",
      "Total reward of agent [2.4486799240112305]\n",
      "ATTACK step 1327: disconnected 45\n",
      "ATTACK step 1328: disconnected 45\n",
      "step 1328: Survived [4] steps\n",
      "Total reward of agent [2.4841349124908447]\n",
      "ATTACK step 1332: disconnected 45\n",
      "ATTACK step 1333: disconnected 45\n",
      "step 1333: Survived [4] steps\n",
      "Total reward of agent [2.4546215534210205]\n",
      "ATTACK step 1337: disconnected 45\n",
      "ATTACK step 1338: disconnected 45\n",
      "step 1338: Survived [4] steps\n",
      "Total reward of agent [2.451470971107483]\n",
      "ATTACK step 1342: disconnected 45\n",
      "ATTACK step 1343: disconnected 45\n",
      "step 1343: Survived [4] steps\n",
      "Total reward of agent [2.4020572900772095]\n",
      "loss = 800.0969\n",
      "ATTACK step 1347: disconnected 45\n",
      "ATTACK step 1348: disconnected 45\n",
      "step 1348: Survived [4] steps\n",
      "Total reward of agent [2.4385993480682373]\n",
      "ATTACK step 1352: disconnected 45\n",
      "ATTACK step 1353: disconnected 45\n",
      "step 1353: Survived [4] steps\n",
      "Total reward of agent [2.4813663959503174]\n",
      "ATTACK step 1357: disconnected 45\n",
      "ATTACK step 1358: disconnected 45\n",
      "step 1358: Survived [4] steps\n",
      "Total reward of agent [2.4748724699020386]\n",
      "ATTACK step 1362: disconnected 45\n",
      "ATTACK step 1363: disconnected 45\n",
      "step 1363: Survived [4] steps\n",
      "Total reward of agent [2.4873872995376587]\n",
      "ATTACK step 1367: disconnected 45\n",
      "ATTACK step 1368: disconnected 45\n",
      "ATTACK step 1369: disconnected 45\n",
      "step 1369: Survived [5] steps\n",
      "Total reward of agent [2.6098745465278625]\n",
      "ATTACK step 1373: disconnected 45\n",
      "ATTACK step 1374: disconnected 45\n",
      "step 1374: Survived [4] steps\n",
      "Total reward of agent [2.455836296081543]\n",
      "ATTACK step 1378: disconnected 45\n",
      "ATTACK step 1379: disconnected 45\n",
      "step 1379: Survived [4] steps\n",
      "Total reward of agent [2.4952971935272217]\n",
      "ATTACK step 1383: disconnected 45\n",
      "ATTACK step 1384: disconnected 45\n",
      "step 1384: Survived [4] steps\n",
      "Total reward of agent [2.4294705390930176]\n",
      "ATTACK step 1388: disconnected 45\n",
      "ATTACK step 1389: disconnected 45\n",
      "step 1389: Survived [4] steps\n",
      "Total reward of agent [2.412908911705017]\n",
      "ATTACK step 1393: disconnected 45\n",
      "ATTACK step 1394: disconnected 45\n",
      "step 1394: Survived [4] steps\n",
      "Total reward of agent [2.453245520591736]\n",
      "ATTACK step 1398: disconnected 45\n",
      "ATTACK step 1399: disconnected 45\n",
      "step 1399: Survived [4] steps\n",
      "Total reward of agent [2.467233419418335]\n",
      "loss = 1291.1545\n",
      "ATTACK step 1403: disconnected 45\n",
      "ATTACK step 1404: disconnected 45\n",
      "step 1404: Survived [4] steps\n",
      "Total reward of agent [2.4281057119369507]\n",
      "ATTACK step 1408: disconnected 45\n",
      "ATTACK step 1409: disconnected 45\n",
      "step 1409: Survived [4] steps\n",
      "Total reward of agent [2.4492162466049194]\n",
      "ATTACK step 1413: disconnected 45\n",
      "ATTACK step 1414: disconnected 45\n",
      "step 1414: Survived [4] steps\n",
      "Total reward of agent [2.473235607147217]\n",
      "ATTACK step 1418: disconnected 45\n",
      "ATTACK step 1419: disconnected 45\n",
      "step 1419: Survived [4] steps\n",
      "Total reward of agent [2.4645121097564697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK step 1423: disconnected 45\n",
      "ATTACK step 1424: disconnected 45\n",
      "step 1424: Survived [4] steps\n",
      "Total reward of agent [1.5717024207115173]\n",
      "ATTACK step 1428: disconnected 45\n",
      "ATTACK step 1429: disconnected 45\n",
      "step 1429: Survived [4] steps\n",
      "Total reward of agent [2.4289162158966064]\n",
      "ATTACK step 1433: disconnected 45\n",
      "ATTACK step 1434: disconnected 45\n",
      "step 1434: Survived [4] steps\n",
      "Total reward of agent [2.432889938354492]\n",
      "ATTACK step 1438: disconnected 45\n",
      "ATTACK step 1439: disconnected 45\n",
      "step 1439: Survived [4] steps\n",
      "Total reward of agent [2.4178311824798584]\n",
      "ATTACK step 1443: disconnected 45\n",
      "ATTACK step 1444: disconnected 45\n",
      "step 1444: Survived [4] steps\n",
      "Total reward of agent [2.4381682872772217]\n",
      "ATTACK step 1448: disconnected 45\n",
      "ATTACK step 1449: disconnected 45\n",
      "step 1449: Survived [4] steps\n",
      "Total reward of agent [2.459296941757202]\n",
      "ATTACK step 1453: disconnected 45\n",
      "ATTACK step 1454: disconnected 45\n",
      "step 1454: Survived [4] steps\n",
      "Total reward of agent [2.4718363285064697]\n",
      "loss = 684.86475\n",
      "ATTACK step 1458: disconnected 45\n",
      "ATTACK step 1459: disconnected 45\n",
      "step 1459: Survived [4] steps\n",
      "Total reward of agent [2.4349864721298218]\n",
      "ATTACK step 1463: disconnected 45\n",
      "ATTACK step 1464: disconnected 45\n",
      "step 1464: Survived [4] steps\n",
      "Total reward of agent [2.4995967149734497]\n",
      "ATTACK step 1468: disconnected 45\n",
      "ATTACK step 1469: disconnected 45\n",
      "step 1469: Survived [4] steps\n",
      "Total reward of agent [2.4303237199783325]\n",
      "ATTACK step 1473: disconnected 45\n",
      "ATTACK step 1474: disconnected 45\n",
      "step 1474: Survived [4] steps\n",
      "Total reward of agent [2.421190023422241]\n",
      "ATTACK step 1478: disconnected 45\n",
      "ATTACK step 1479: disconnected 45\n",
      "step 1479: Survived [4] steps\n",
      "Total reward of agent [2.4903745651245117]\n",
      "ATTACK step 1483: disconnected 45\n",
      "ATTACK step 1484: disconnected 45\n",
      "step 1484: Survived [4] steps\n",
      "Total reward of agent [2.4255871772766113]\n",
      "ATTACK step 1488: disconnected 45\n",
      "ATTACK step 1489: disconnected 45\n",
      "step 1489: Survived [4] steps\n",
      "Total reward of agent [2.4919960498809814]\n",
      "ATTACK step 1493: disconnected 45\n",
      "ATTACK step 1494: disconnected 45\n",
      "step 1494: Survived [4] steps\n",
      "Total reward of agent [2.448858380317688]\n",
      "ATTACK step 1498: disconnected 45\n",
      "ATTACK step 1499: disconnected 45\n",
      "step 1499: Survived [4] steps\n",
      "Total reward of agent [2.5022881031036377]\n",
      "ATTACK step 1503: disconnected 45\n",
      "ATTACK step 1504: disconnected 45\n",
      "step 1504: Survived [4] steps\n",
      "Total reward of agent [2.4578157663345337]\n",
      "ATTACK step 1508: disconnected 45\n",
      "ATTACK step 1509: disconnected 45\n",
      "step 1509: Survived [4] steps\n",
      "Total reward of agent [2.460706353187561]\n",
      "loss = 954.53253\n",
      "ATTACK step 1513: disconnected 45\n",
      "ATTACK step 1514: disconnected 45\n",
      "step 1514: Survived [4] steps\n",
      "Total reward of agent [2.455909490585327]\n",
      "ATTACK step 1518: disconnected 45\n",
      "ATTACK step 1519: disconnected 45\n",
      "step 1519: Survived [4] steps\n",
      "Total reward of agent [2.44463849067688]\n",
      "ATTACK step 1523: disconnected 45\n",
      "ATTACK step 1524: disconnected 45\n",
      "step 1524: Survived [4] steps\n",
      "Total reward of agent [2.4605106115341187]\n",
      "ATTACK step 1528: disconnected 45\n",
      "ATTACK step 1529: disconnected 45\n",
      "step 1529: Survived [4] steps\n",
      "Total reward of agent [2.417442560195923]\n",
      "ATTACK step 1533: disconnected 45\n",
      "ATTACK step 1534: disconnected 45\n",
      "step 1534: Survived [4] steps\n",
      "Total reward of agent [2.451636791229248]\n",
      "ATTACK step 1538: disconnected 45\n",
      "ATTACK step 1539: disconnected 45\n",
      "step 1539: Survived [4] steps\n",
      "Total reward of agent [2.4381957054138184]\n",
      "ATTACK step 1543: disconnected 45\n",
      "ATTACK step 1544: disconnected 45\n",
      "step 1544: Survived [4] steps\n",
      "Total reward of agent [2.462907314300537]\n",
      "ATTACK step 1548: disconnected 45\n",
      "ATTACK step 1549: disconnected 45\n",
      "step 1549: Survived [4] steps\n",
      "Total reward of agent [2.5010087490081787]\n",
      "ATTACK step 1553: disconnected 45\n",
      "ATTACK step 1554: disconnected 45\n",
      "ATTACK step 1555: disconnected 45\n",
      "step 1555: Survived [5] steps\n",
      "Total reward of agent [2.645904839038849]\n",
      "ATTACK step 1559: disconnected 45\n",
      "ATTACK step 1560: disconnected 45\n",
      "step 1560: Survived [4] steps\n",
      "Total reward of agent [2.4617955684661865]\n",
      "ATTACK step 1564: disconnected 45\n",
      "ATTACK step 1565: disconnected 45\n",
      "step 1565: Survived [4] steps\n",
      "Total reward of agent [2.4082465171813965]\n",
      "loss = 1960.5654\n",
      "ATTACK step 1569: disconnected 45\n",
      "ATTACK step 1570: disconnected 45\n",
      "step 1570: Survived [4] steps\n",
      "Total reward of agent [2.3832186460494995]\n",
      "ATTACK step 1574: disconnected 45\n",
      "ATTACK step 1575: disconnected 45\n",
      "step 1575: Survived [4] steps\n",
      "Total reward of agent [2.4726141691207886]\n",
      "ATTACK step 1579: disconnected 45\n",
      "ATTACK step 1580: disconnected 45\n",
      "step 1580: Survived [4] steps\n",
      "Total reward of agent [2.4904783964157104]\n",
      "ATTACK step 1584: disconnected 45\n",
      "ATTACK step 1585: disconnected 45\n",
      "step 1585: Survived [4] steps\n",
      "Total reward of agent [2.4080220460891724]\n",
      "ATTACK step 1589: disconnected 45\n",
      "ATTACK step 1590: disconnected 45\n",
      "step 1590: Survived [4] steps\n",
      "Total reward of agent [2.4864269495010376]\n",
      "ATTACK step 1594: disconnected 45\n",
      "ATTACK step 1595: disconnected 45\n",
      "step 1595: Survived [4] steps\n",
      "Total reward of agent [2.43845272064209]\n",
      "ATTACK step 1599: disconnected 45\n",
      "ATTACK step 1600: disconnected 45\n",
      "ATTACK step 1601: disconnected 45\n",
      "ATTACK step 1602: disconnected 45\n",
      "ATTACK step 1603: disconnected 45\n",
      "ATTACK step 1604: disconnected 45\n",
      "ATTACK step 1605: disconnected 45\n",
      "ATTACK step 1606: disconnected 45\n",
      "ATTACK step 1607: disconnected 45\n",
      "step 1607: Survived [11] steps\n",
      "Total reward of agent [8.777841567993164]\n",
      "ATTACK step 1611: disconnected 45\n",
      "ATTACK step 1612: disconnected 45\n",
      "step 1612: Survived [4] steps\n",
      "Total reward of agent [2.4607744216918945]\n",
      "ATTACK step 1616: disconnected 45\n",
      "ATTACK step 1617: disconnected 45\n",
      "step 1617: Survived [4] steps\n",
      "Total reward of agent [2.4619208574295044]\n",
      "ATTACK step 1621: disconnected 45\n",
      "ATTACK step 1622: disconnected 45\n",
      "step 1622: Survived [4] steps\n",
      "Total reward of agent [2.4157811403274536]\n",
      "loss = 939.44763\n",
      "ATTACK step 1626: disconnected 45\n",
      "ATTACK step 1627: disconnected 45\n",
      "step 1627: Survived [4] steps\n",
      "Total reward of agent [2.4050216674804688]\n",
      "ATTACK step 1631: disconnected 45\n",
      "ATTACK step 1632: disconnected 45\n",
      "step 1632: Survived [4] steps\n",
      "Total reward of agent [2.4981476068496704]\n",
      "ATTACK step 1636: disconnected 45\n",
      "ATTACK step 1637: disconnected 45\n",
      "step 1637: Survived [4] steps\n",
      "Total reward of agent [2.4895689487457275]\n",
      "ATTACK step 1641: disconnected 45\n",
      "ATTACK step 1642: disconnected 45\n",
      "step 1642: Survived [4] steps\n",
      "Total reward of agent [2.45126736164093]\n",
      "ATTACK step 1646: disconnected 45\n",
      "ATTACK step 1647: disconnected 45\n",
      "ATTACK step 1648: disconnected 45\n",
      "step 1648: Survived [5] steps\n",
      "Total reward of agent [2.603270411491394]\n",
      "ATTACK step 1652: disconnected 45\n",
      "ATTACK step 1653: disconnected 45\n",
      "step 1653: Survived [4] steps\n",
      "Total reward of agent [2.436441659927368]\n",
      "ATTACK step 1657: disconnected 45\n",
      "ATTACK step 1658: disconnected 45\n",
      "step 1658: Survived [4] steps\n",
      "Total reward of agent [2.457730293273926]\n",
      "ATTACK step 1662: disconnected 45\n",
      "ATTACK step 1663: disconnected 45\n",
      "step 1663: Survived [4] steps\n",
      "Total reward of agent [2.4317342042922974]\n",
      "ATTACK step 1667: disconnected 45\n",
      "ATTACK step 1668: disconnected 45\n",
      "step 1668: Survived [4] steps\n",
      "Total reward of agent [2.454909920692444]\n",
      "ATTACK step 1672: disconnected 45\n",
      "ATTACK step 1673: disconnected 45\n",
      "ATTACK step 1674: disconnected 45\n",
      "ATTACK step 1675: disconnected 45\n",
      "ATTACK step 1676: disconnected 45\n",
      "ATTACK step 1677: disconnected 45\n",
      "ATTACK step 1678: disconnected 45\n",
      "ATTACK step 1679: disconnected 45\n",
      "step 1679: Survived [10] steps\n",
      "Total reward of agent [7.801716089248657]\n",
      "loss = 1138.7903\n",
      "ATTACK step 1683: disconnected 45\n",
      "ATTACK step 1684: disconnected 45\n",
      "step 1684: Survived [4] steps\n",
      "Total reward of agent [2.4309232234954834]\n",
      "ATTACK step 1688: disconnected 45\n",
      "ATTACK step 1689: disconnected 45\n",
      "step 1689: Survived [4] steps\n",
      "Total reward of agent [2.4589744806289673]\n",
      "ATTACK step 1693: disconnected 45\n",
      "ATTACK step 1694: disconnected 45\n",
      "step 1694: Survived [4] steps\n",
      "Total reward of agent [2.429835319519043]\n",
      "ATTACK step 1698: disconnected 45\n",
      "ATTACK step 1699: disconnected 45\n",
      "step 1699: Survived [4] steps\n",
      "Total reward of agent [2.4155662059783936]\n",
      "ATTACK step 1703: disconnected 45\n",
      "ATTACK step 1704: disconnected 45\n",
      "step 1704: Survived [4] steps\n",
      "Total reward of agent [2.4593156576156616]\n",
      "ATTACK step 1708: disconnected 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK step 1709: disconnected 45\n",
      "step 1709: Survived [4] steps\n",
      "Total reward of agent [2.412395715713501]\n",
      "ATTACK step 1713: disconnected 45\n",
      "ATTACK step 1714: disconnected 45\n",
      "step 1714: Survived [4] steps\n",
      "Total reward of agent [2.4504783153533936]\n",
      "ATTACK step 1718: disconnected 45\n",
      "ATTACK step 1719: disconnected 45\n",
      "step 1719: Survived [4] steps\n",
      "Total reward of agent [2.4935693740844727]\n",
      "ATTACK step 1723: disconnected 45\n",
      "ATTACK step 1724: disconnected 45\n",
      "step 1724: Survived [4] steps\n",
      "Total reward of agent [2.463309645652771]\n",
      "ATTACK step 1728: disconnected 45\n",
      "ATTACK step 1729: disconnected 45\n",
      "step 1729: Survived [4] steps\n",
      "Total reward of agent [2.4466822147369385]\n",
      "ATTACK step 1733: disconnected 45\n",
      "ATTACK step 1734: disconnected 45\n",
      "step 1734: Survived [4] steps\n",
      "Total reward of agent [2.397442579269409]\n",
      "loss = 1249.6381\n",
      "ATTACK step 1738: disconnected 45\n",
      "ATTACK step 1739: disconnected 45\n",
      "step 1739: Survived [4] steps\n",
      "Total reward of agent [2.4387524127960205]\n",
      "ATTACK step 1743: disconnected 45\n",
      "ATTACK step 1744: disconnected 45\n",
      "ATTACK step 1745: disconnected 45\n",
      "step 1745: Survived [5] steps\n",
      "Total reward of agent [2.596815288066864]\n",
      "ATTACK step 1749: disconnected 45\n",
      "ATTACK step 1750: disconnected 45\n",
      "step 1750: Survived [4] steps\n",
      "Total reward of agent [2.513198733329773]\n",
      "ATTACK step 1754: disconnected 45\n",
      "ATTACK step 1755: disconnected 45\n",
      "step 1755: Survived [4] steps\n",
      "Total reward of agent [2.429186224937439]\n",
      "ATTACK step 1759: disconnected 45\n",
      "ATTACK step 1760: disconnected 45\n",
      "step 1760: Survived [4] steps\n",
      "Total reward of agent [2.485974669456482]\n",
      "ATTACK step 1764: disconnected 45\n",
      "ATTACK step 1765: disconnected 45\n",
      "step 1765: Survived [4] steps\n",
      "Total reward of agent [2.447747230529785]\n",
      "ATTACK step 1769: disconnected 45\n",
      "ATTACK step 1770: disconnected 45\n",
      "step 1770: Survived [4] steps\n",
      "Total reward of agent [2.470193862915039]\n",
      "ATTACK step 1774: disconnected 45\n",
      "ATTACK step 1775: disconnected 45\n",
      "step 1775: Survived [4] steps\n",
      "Total reward of agent [2.3769941329956055]\n",
      "ATTACK step 1779: disconnected 45\n",
      "ATTACK step 1780: disconnected 45\n",
      "step 1780: Survived [4] steps\n",
      "Total reward of agent [2.4210433959960938]\n",
      "ATTACK step 1784: disconnected 45\n",
      "ATTACK step 1785: disconnected 45\n",
      "step 1785: Survived [4] steps\n",
      "Total reward of agent [2.4161462783813477]\n",
      "ATTACK step 1789: disconnected 45\n",
      "ATTACK step 1790: disconnected 45\n",
      "step 1790: Survived [4] steps\n",
      "Total reward of agent [2.4396917819976807]\n",
      "loss = 1275.7864\n",
      "ATTACK step 1794: disconnected 45\n",
      "ATTACK step 1795: disconnected 45\n",
      "step 1795: Survived [4] steps\n",
      "Total reward of agent [2.446642518043518]\n",
      "ATTACK step 1799: disconnected 45\n",
      "ATTACK step 1800: disconnected 45\n",
      "step 1800: Survived [4] steps\n",
      "Total reward of agent [2.445169687271118]\n",
      "ATTACK step 1804: disconnected 45\n",
      "ATTACK step 1805: disconnected 45\n",
      "step 1805: Survived [4] steps\n",
      "Total reward of agent [2.5097321271896362]\n",
      "ATTACK step 1809: disconnected 45\n",
      "ATTACK step 1810: disconnected 45\n",
      "step 1810: Survived [4] steps\n",
      "Total reward of agent [2.4750442504882812]\n",
      "ATTACK step 1814: disconnected 45\n",
      "ATTACK step 1815: disconnected 45\n",
      "step 1815: Survived [4] steps\n",
      "Total reward of agent [2.3986332416534424]\n",
      "ATTACK step 1819: disconnected 45\n",
      "ATTACK step 1820: disconnected 45\n",
      "step 1820: Survived [4] steps\n",
      "Total reward of agent [2.4556171894073486]\n",
      "ATTACK step 1824: disconnected 45\n",
      "ATTACK step 1825: disconnected 45\n",
      "step 1825: Survived [4] steps\n",
      "Total reward of agent [2.463721752166748]\n",
      "ATTACK step 1829: disconnected 45\n",
      "ATTACK step 1830: disconnected 45\n",
      "step 1830: Survived [4] steps\n",
      "Total reward of agent [2.475752353668213]\n",
      "ATTACK step 1834: disconnected 45\n",
      "ATTACK step 1835: disconnected 45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2fe13f5ac283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mlog_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf_logs_D3QN\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtrain_adversary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pre_training_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-7e1ce8df9205>\u001b[0m in \u001b[0;36mtrain_adversary\u001b[0;34m(env, agent, opponent, num_pre_training_steps, n_iter, save_path, log_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mattack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_nothing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mattack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/caltech/2021_winter/CS165/robust_pn/train/d3qn/adversary.py\u001b[0m in \u001b[0;36mattack\u001b[0;34m(self, observation, agent_action, env_action, budget, previous_fails)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# TODO: use random move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremaining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_duration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction2line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/caltech/2021_winter/CS165/robust_pn/train/d3qn/neuralnet.py\u001b[0m in \u001b[0;36mpredict_move\u001b[0;34m(self, status, data)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mq_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mq_valid_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq_valid_actions\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1835\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \"\"\"\n\u001b[0;32m-> 1837\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m   def interleave(self,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4283\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4284\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 4285\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   4286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4287\u001b[0m       raise TypeError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Copy the recursive list, tuple and map structure, but not base objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     func_args_before = nest.pack_sequence_as(func_args, flat_func_args,\n\u001b[0;32m--> 930\u001b[0;31m                                              expand_composites=True)\n\u001b[0m\u001b[1;32m    931\u001b[0m     func_kwargs_before = nest.pack_sequence_as(\n\u001b[1;32m    932\u001b[0m         func_kwargs, flat_func_kwargs, expand_composites=True)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m   \"\"\"\n\u001b[0;32m--> 579\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    540\u001b[0m           \u001b[0;34m\"flat_sequence had %d elements.  Structure: %s, flat_sequence: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m           (len(flat_structure), len(flat_sequence), structure, flat_sequence))\n\u001b[0;32m--> 542\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msequence_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_sequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   \u001b[0;32melif\u001b[0m \u001b[0m_is_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0minstance_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_name = 'l2rpn_wcci_2020'\n",
    "env = grid2op.make(env_name,\n",
    "#            action_class=TopologyChangeAndDispatchAction,\n",
    "           reward_class=CombinedScaledReward)\n",
    "\n",
    "# Agent \n",
    "agent_name = \"kaist\"\n",
    "data_dir = os.path.join('kaist_agent/data')\n",
    "with open(os.path.join(data_dir, 'param.json'), 'r', encoding='utf-8') as f:\n",
    "    param = json.load(f)\n",
    "print(param)\n",
    "state_mean = torch.load(os.path.join(data_dir, 'mean.pt'), map_location=param['device']).cpu()\n",
    "state_std = torch.load(os.path.join(data_dir, 'std.pt'), map_location=param['device']).cpu()\n",
    "state_std = state_std.masked_fill(state_std<1e-5, 1.)\n",
    "state_mean[0, sum(env.observation_space.shape[:20]):] = 0\n",
    "state_std[0, sum(env.observation_space.shape[:20]):] = 1\n",
    "agent = Kaist(env, state_mean, state_std, name=agent_name, **param)\n",
    "agent.sim_trial = 0\n",
    "agent.load_model(data_dir)\n",
    "\n",
    "# Opponent \n",
    "opponent_name = \"D3QN_kaist\"\n",
    "num_pre_training_steps = 256\n",
    "learning_rate = 1e-4\n",
    "initial_epsilon = 0.99\n",
    "final_epsilon = 0.01\n",
    "decay_epsilon = 10000\n",
    "attack_period = 20\n",
    "lines = ['0_4_2', '10_11_11', '11_12_13', '12_13_14', '12_16_20', \n",
    "            '13_14_15', '13_15_16', '14_16_17', '14_35_53', '15_16_21', \n",
    "            '16_17_22', '16_18_23', '16_21_27', '16_21_28', '16_33_48', \n",
    "            '16_33_49', '16_35_54', '17_24_33', '18_19_24', '18_25_35', \n",
    "            '19_20_25', '1_10_12', '1_3_3', '1_4_4', '20_21_26', \n",
    "            '21_22_29', '21_23_30', '21_26_36', '22_23_31', '22_26_39', \n",
    "            '23_24_32', '23_25_34', '23_26_37', '23_26_38', '26_27_40', \n",
    "            '26_28_41', '26_30_56', '27_28_42', '27_29_43', '28_29_44', \n",
    "            '28_31_57', '29_33_50', '29_34_51', '2_3_0', '2_4_1', \n",
    "            '30_31_45', '31_32_47', '32_33_58', '33_34_52', '4_5_55', \n",
    "            '4_6_5', '4_7_6', '5_32_46', '6_7_7', '7_8_8', \n",
    "            '7_9_9', '8_9_10', '9_16_18', '9_16_19']\n",
    "\n",
    "opponent = D3QN_Opponent(env.action_space, env.observation_space, lines_attacked=lines, attack_period=attack_period,\n",
    "            name=opponent_name, is_training=True, learning_rate=learning_rate,\n",
    "            initial_epsilon=initial_epsilon, final_epsilon=final_epsilon, decay_epsilon=decay_epsilon)\n",
    "\n",
    "# Training\n",
    "n_iter = 10000\n",
    "# Register custom reward for training\n",
    "cr = env._reward_helper.template_reward\n",
    "#cr.addReward(\"overflow\", CloseToOverflowReward(), 1.0)\n",
    "cr.addReward(\"game\", GameplayReward(), 1.0)\n",
    "#cr.addReward(\"recolines\", LinesReconnectedReward(), 1.0)\n",
    "cr.addReward(\"l2rpn\", L2RPNReward(), 2.0/float(env.n_line))\n",
    "# Initialize custom rewards\n",
    "cr.initialize(env)\n",
    "# Set reward range to something managable\n",
    "cr.set_range(-1.0, 1.0)\n",
    "\n",
    "save_path = \"kaist_agent_D3QN_opponent_{}_{}\".format(attack_period, n_iter)\n",
    "log_path=\"tf_logs_D3QN\"\n",
    "\n",
    "train_adversary(env, agent, opponent, num_pre_training_steps, n_iter, save_path, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "executive-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op import make\n",
    "from grid2op.Runner import Runner\n",
    "from grid2op.Reward import L2RPNSandBoxScore, L2RPNReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "allied-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episode = 10 # number of episodes to evaluate\n",
    "log_path = './logs-evals'\n",
    "nb_process = 1 # number of cores to use\n",
    "max_iter = 150 # maximum number of steps per scenario\n",
    "verbose = True\n",
    "save_gif = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sophisticated-grill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O: 72 S: 128 A: 108 (19)\n",
      "Evaluation summary:\n",
      "chronics at: Scenario_april_000\ttotal reward: 49896.312500\ttime steps: 150/150\n",
      "chronics at: Scenario_april_001\ttotal reward: 53077.667969\ttime steps: 150/150\n",
      "chronics at: Scenario_april_002\ttotal reward: 43475.250000\ttime steps: 150/150\n",
      "chronics at: Scenario_april_003\ttotal reward: 50524.660156\ttime steps: 150/150\n",
      "chronics at: Scenario_april_004\ttotal reward: 55378.964844\ttime steps: 150/150\n",
      "chronics at: Scenario_april_005\ttotal reward: 44330.468750\ttime steps: 150/150\n",
      "chronics at: Scenario_april_006\ttotal reward: 50290.003906\ttime steps: 150/150\n",
      "chronics at: Scenario_april_007\ttotal reward: 62669.691406\ttime steps: 150/150\n",
      "chronics at: Scenario_april_008\ttotal reward: 53732.957031\ttime steps: 150/150\n",
      "chronics at: Scenario_april_009\ttotal reward: 49542.824219\ttime steps: 150/150\n"
     ]
    }
   ],
   "source": [
    "env_name = 'l2rpn_wcci_2020'\n",
    "env = make(env_name, reward_class=L2RPNSandBoxScore,\n",
    "           other_rewards={\n",
    "               \"reward\": L2RPNReward\n",
    "           })\n",
    "\n",
    "agent_name = \"kaist\"\n",
    "data_dir = os.path.join('kaist_agent/data')\n",
    "with open(os.path.join(data_dir, 'param.json'), 'r', encoding='utf-8') as f:\n",
    "    param = json.load(f)\n",
    "\n",
    "state_mean = torch.load(os.path.join(data_dir, 'mean.pt'), map_location=param['device']).cpu()\n",
    "state_std = torch.load(os.path.join(data_dir, 'std.pt'), map_location=param['device']).cpu()\n",
    "state_std = state_std.masked_fill(state_std<1e-5, 1.)\n",
    "state_mean[0, sum(env.observation_space.shape[:20]):] = 0\n",
    "state_std[0, sum(env.observation_space.shape[:20]):] = 1\n",
    "agent = Kaist(env, state_mean, state_std, name=agent_name, **param)\n",
    "agent.sim_trial = 0\n",
    "agent.load_model(data_dir)\n",
    "    \n",
    "runner_params = env.get_params_for_runner()\n",
    "runner_params[\"verbose\"] = False\n",
    "runner = Runner(**runner_params, agentClass=None, agentInstance=agent)\n",
    "    \n",
    "res = runner.run(path_save=log_path, nb_episode=nb_episode, nb_process=nb_process, max_iter=150)\n",
    "if verbose:\n",
    "    print(\"Evaluation summary:\")\n",
    "    for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "        msg_tmp = \"chronics at: {}\".format(chron_name)\n",
    "        msg_tmp += \"\\ttotal reward: {:.6f}\".format(cum_reward)\n",
    "        msg_tmp += \"\\ttime steps: {:.0f}/{:.0f}\".format(nb_time_step,\n",
    "                                                        max_ts)\n",
    "        print(msg_tmp)\n",
    "\n",
    "if save_gif:\n",
    "    save_log_gif(log_path, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-alpha",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
