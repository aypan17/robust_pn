{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "middle-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "import grid2op\n",
    "from d3qn.adversary import D3QN_Opponent\n",
    "from grid2op.Agent import DoNothingAgent\n",
    "from grid2op.Action import TopologyChangeAndDispatchAction\n",
    "from grid2op.Reward import CombinedScaledReward, L2RPNSandBoxScore, L2RPNReward, GameplayReward\n",
    "from l2rpn_baselines.DoubleDuelingDQN.DoubleDuelingDQNConfig import DoubleDuelingDQNConfig as cfg\n",
    "\n",
    "from kaist_agent.Kaist import Kaist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "backed-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIMESTEP = 7 * 288\n",
    "\n",
    "def train_adversary(env, agent, opponent, num_pre_training_steps, n_iter, save_path, log_path):\n",
    "    # Make sure we can fill the experience buffer\n",
    "    if num_pre_training_steps < opponent.batch_size * opponent.num_frames:\n",
    "        num_pre_training_steps = opponent.batch_size * opponent.num_frames\n",
    "        \n",
    "    # Loop vars\n",
    "    num_training_steps = n_iter\n",
    "    num_steps = num_pre_training_steps + num_training_steps\n",
    "    step = 0\n",
    "    alive_steps = 0\n",
    "    total_reward = 0\n",
    "    done = True\n",
    "    print(f\"Total number of steps: {num_steps}\")\n",
    "\n",
    "    # Create file system related vars\n",
    "    logpath = os.path.join(log_path, opponent.name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    modelpath = os.path.join(save_path, opponent.name + \".h5\")\n",
    "    opponent.tf_writer = tf.summary.create_file_writer(logpath, name=opponent.name)\n",
    "    opponent._save_hyperparameters(save_path, env, num_steps)\n",
    "    \n",
    "    while step < num_steps:\n",
    "        # Init first time or new episode\n",
    "        if done:\n",
    "            new_obs = env.reset() # This shouldn't raise\n",
    "            agent.reset(new_obs)\n",
    "            opponent.reset(new_obs)\n",
    "            done = False\n",
    "        if cfg.VERBOSE and step % 1000 == 0:\n",
    "            print(\"Step [{}] -- Random [{}]\".format(step, opponent.epsilon))\n",
    "\n",
    "        # Save current observation to stacking buffer\n",
    "        opponent._save_current_frame(opponent.state)\n",
    "\n",
    "        # Execute attack if allowed\n",
    "        if step <= num_pre_training_steps:\n",
    "            opponent.remaining_time = 0\n",
    "            attack, a = opponent._do_nothing, 0\n",
    "        else:\n",
    "            attack, a = opponent.attack(new_obs)\n",
    "\n",
    "        if a != 0:\n",
    "#             print(f'ATTACK step {step}: disconnected {a}')\n",
    "            attack_obs, opp_reward, done, info = env.step(attack)\n",
    "            if info[\"is_illegal\"] or info[\"is_ambiguous\"] or \\\n",
    "               info[\"is_dispatching_illegal\"] or info[\"is_illegal_reco\"]:\n",
    "                if cfg.VERBOSE:\n",
    "                    print(attack, info)\n",
    "            new_obs = attack_obs\n",
    "            opponent.tell_attack_continues(None, None, None, None)\n",
    "\n",
    "        while opponent.remaining_time >= 0 and not done:\n",
    "            new_obs.time_before_cooldown_line[opponent.attack_line] = opponent.remaining_time\n",
    "            response = agent.act(new_obs, None, None)\n",
    "            new_obs, reward, done, info = env.step(response)\n",
    "            opponent.remaining_time -= 1\n",
    "            total_reward += reward\n",
    "            alive_steps += 1\n",
    "        \n",
    "        # Save new observation to stacking buffer\n",
    "        new_state = opponent.convert_obs(new_obs)\n",
    "        opponent._save_next_frame(new_state)\n",
    "\n",
    "        # Save to experience buffer\n",
    "        if len(opponent.frames2) == opponent.num_frames:\n",
    "            opponent.per_buffer.add(np.array(opponent.frames),\n",
    "                                a, -1 * reward,\n",
    "                                np.array(opponent.frames2),\n",
    "                                opponent.done)\n",
    "\n",
    "\n",
    "        # Perform training when we have enough experience in buffer\n",
    "        if step >= num_pre_training_steps:\n",
    "            training_step = step - num_pre_training_steps\n",
    "            # Decay chance of random action\n",
    "            opponent.epsilon = opponent._adaptive_epsilon_decay(training_step)\n",
    "\n",
    "            # Perform training at given frequency\n",
    "            if step % cfg.UPDATE_FREQ == 0 and \\\n",
    "               len(opponent.per_buffer) >= opponent.batch_size:\n",
    "                # Perform training\n",
    "                opponent._batch_train(training_step, step)\n",
    "\n",
    "                if cfg.UPDATE_TARGET_SOFT_TAU > 0.0:\n",
    "                    tau = cfg.UPDATE_TARGET_SOFT_TAU\n",
    "                    # Update target network towards primary network\n",
    "                    opponent.policy_net.update_target_soft(opponent.target_net.model, tau)\n",
    "\n",
    "            # Every UPDATE_TARGET_HARD_FREQ trainings, update target completely\n",
    "            if cfg.UPDATE_TARGET_HARD_FREQ > 0 and \\\n",
    "               step % (cfg.UPDATE_FREQ * cfg.UPDATE_TARGET_HARD_FREQ) == 0:\n",
    "                opponent.policy_net.update_target_hard(opponent.target_net.model)\n",
    "        \n",
    "        if done:\n",
    "            opponent.epoch_rewards.append(-1 * total_reward)\n",
    "            opponent.epoch_alive.append(alive_steps)\n",
    "            if cfg.VERBOSE and step > num_pre_training_steps:\n",
    "                print(\"step {}: Agent survived [{}] steps with reward {}\".format(step, alive_steps, total_reward))\n",
    "            alive_steps = 0\n",
    "            total_reward = 0         \n",
    "        else:\n",
    "            alive_steps += 1\n",
    "            \n",
    "        ######## After Each Step #######\n",
    "        if step > 0 and step % 2000 == 0: # save network every 5000 iters\n",
    "            opponent.save(modelpath)\n",
    "        step += 1\n",
    "        # Make new obs the current obs\n",
    "        opponent.obs = new_obs\n",
    "        opponent.state = new_state\n",
    "\n",
    "    # Save model after all steps\n",
    "    opponent.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "numerical-marina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'head_number': 8, 'n_history': 12, 'state_dim': 128, 'dropout': 0.0, 'sim_trial': 15, 'threshold': 0.35, 'max_low_len': 19, 'danger': 0.9, 'mask': 3, 'mask_hi': 19, 'use_order': True, 'device': 'cpu'}\n",
      "O: 72 S: 128 A: 108 (19)\n",
      "['2_3_0' '2_4_1' '0_4_2' '1_3_3' '1_4_4' '4_6_5' '4_7_6' '6_7_7' '7_8_8'\n",
      " '7_9_9' '8_9_10' '10_11_11' '1_10_12' '11_12_13' '12_13_14' '13_14_15'\n",
      " '13_15_16' '14_16_17' '9_16_18' '9_16_19' '12_16_20' '15_16_21'\n",
      " '16_17_22' '16_18_23' '18_19_24' '19_20_25' '20_21_26' '16_21_27'\n",
      " '16_21_28' '21_22_29' '21_23_30' '22_23_31' '23_24_32' '17_24_33'\n",
      " '23_25_34' '18_25_35' '21_26_36' '23_26_37' '23_26_38' '22_26_39'\n",
      " '26_27_40' '26_28_41' '27_28_42' '27_29_43' '28_29_44' '30_31_45'\n",
      " '5_32_46' '31_32_47' '16_33_48' '16_33_49' '29_33_50' '29_34_51'\n",
      " '33_34_52' '14_35_53' '16_35_54' '4_5_55' '26_30_56' '28_31_57'\n",
      " '32_33_58']\n"
     ]
    }
   ],
   "source": [
    "env_name = 'l2rpn_wcci_2020'\n",
    "env = grid2op.make(env_name, reward_class=CombinedScaledReward)\n",
    "\n",
    "# Agent \n",
    "agent_name = \"kaist\"\n",
    "data_dir = os.path.join('kaist_agent/data')\n",
    "with open(os.path.join(data_dir, 'param.json'), 'r', encoding='utf-8') as f:\n",
    "    param = json.load(f)\n",
    "print(param)\n",
    "state_mean = torch.load(os.path.join(data_dir, 'mean.pt'), map_location=param['device']).cpu()\n",
    "state_std = torch.load(os.path.join(data_dir, 'std.pt'), map_location=param['device']).cpu()\n",
    "state_std = state_std.masked_fill(state_std<1e-5, 1.)\n",
    "state_mean[0, sum(env.observation_space.shape[:20]):] = 0\n",
    "state_std[0, sum(env.observation_space.shape[:20]):] = 1\n",
    "agent = Kaist(env, state_mean, state_std, name=agent_name, **param)\n",
    "agent.sim_trial = 0\n",
    "agent.load_model(data_dir)\n",
    "\n",
    "# Opponent \n",
    "opponent_name = \"D3QN_kaist\"\n",
    "num_pre_training_steps = 256\n",
    "learning_rate = 5e-5\n",
    "initial_epsilon = 0.99\n",
    "final_epsilon = 0.01\n",
    "decay_epsilon = 5000\n",
    "attack_period = 20\n",
    "lines = ['0_4_2', '10_11_11', '11_12_13', '12_13_14', '12_16_20', \n",
    "            '13_14_15', '13_15_16', '14_16_17', '14_35_53', '15_16_21', \n",
    "            '16_17_22', '16_18_23', '16_21_27', '16_21_28', '16_33_48', \n",
    "            '16_33_49', '16_35_54', '17_24_33', '18_19_24', '18_25_35', \n",
    "            '19_20_25', '1_10_12', '1_3_3', '1_4_4', '20_21_26', \n",
    "            '21_22_29', '21_23_30', '21_26_36', '22_23_31', '22_26_39', \n",
    "            '23_24_32', '23_25_34', '23_26_37', '23_26_38', '26_27_40', \n",
    "            '26_28_41', '26_30_56', '27_28_42', '27_29_43', '28_29_44', \n",
    "            '28_31_57', '29_33_50', '29_34_51', '2_3_0', '2_4_1', \n",
    "            '30_31_45', '31_32_47', '32_33_58', '33_34_52', '4_5_55', \n",
    "            '4_6_5', '4_7_6', '5_32_46', '6_7_7', '7_8_8', \n",
    "            '7_9_9', '8_9_10', '9_16_18', '9_16_19']\n",
    "\n",
    "opponent = D3QN_Opponent(env.action_space, env.observation_space, lines_attacked=lines, attack_period=attack_period,\n",
    "            name=opponent_name, is_training=True, learning_rate=learning_rate,\n",
    "            initial_epsilon=initial_epsilon, final_epsilon=final_epsilon, decay_epsilon=decay_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "regional-punch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps: 5256\n",
      "Step [0] -- Random [0.99]\n",
      "step 266: Agent survived [622] steps with reward 305.9651598930359\n",
      "step 272: Agent survived [41] steps with reward 25.828663289546967\n",
      "step 278: Agent survived [44] steps with reward 27.575585186481476\n",
      "loss = 109489.51\n",
      "step 287: Agent survived [68] steps with reward 43.43529826402664\n",
      "step 302: Agent survived [149] steps with reward 91.74529415369034\n",
      "step 313: Agent survived [95] steps with reward 67.10616558790207\n",
      "step 318: Agent survived [23] steps with reward 16.47074830532074\n",
      "step 329: Agent survived [104] steps with reward 74.5729193687439\n",
      "loss = 43257.875\n",
      "step 337: Agent survived [59] steps with reward 42.027507066726685\n",
      "step 347: Agent survived [83] steps with reward 55.785299479961395\n",
      "step 360: Agent survived [125] steps with reward 85.89805418252945\n",
      "step 370: Agent survived [83] steps with reward 61.21436274051666\n",
      "step 380: Agent survived [86] steps with reward 55.96154886484146\n",
      "step 386: Agent survived [41] steps with reward 28.245803833007812\n",
      "loss = 11733.678\n",
      "step 394: Agent survived [62] steps with reward 45.14445877075195\n",
      "step 403: Agent survived [74] steps with reward 52.5469845533371\n",
      "step 412: Agent survived [74] steps with reward 41.00634723901749\n",
      "step 420: Agent survived [62] steps with reward 44.550995111465454\n",
      "step 428: Agent survived [59] steps with reward 43.235574662685394\n",
      "step 434: Agent survived [41] steps with reward 26.92705923318863\n",
      "loss = 46997.523\n",
      "step 449: Agent survived [146] steps with reward 102.70681875944138\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - force disconnection of 1 powerlines ([35])\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('Powerline with ids [35] have been modified illegally (cooldown)',)], 'rewards': {}}\n",
      "step 479: Agent survived [323] steps with reward 234.47297769784927\n",
      "step 491: Agent survived [113] steps with reward 57.86589103937149\n",
      "step 498: Agent survived [50] steps with reward 34.60339617729187\n",
      "step 502: Agent survived [17] steps with reward 7.187881827354431\n",
      "loss = 41038.406\n",
      "step 509: Agent survived [47] steps with reward 30.54018658399582\n",
      "step 513: Agent survived [11] steps with reward 6.847732663154602\n",
      "step 521: Agent survived [62] steps with reward 48.26431608200073\n",
      "step 537: Agent survived [161] steps with reward 112.22941070795059\n",
      "step 549: Agent survived [110] steps with reward 65.88421040773392\n",
      "loss = 35242.836\n",
      "step 570: Agent survived [215] steps with reward 151.34096157550812\n",
      "step 582: Agent survived [113] steps with reward 82.10194957256317\n",
      "step 589: Agent survived [50] steps with reward 31.066933512687683\n",
      "step 601: Agent survived [113] steps with reward 83.7419986128807\n",
      "step 609: Agent survived [59] steps with reward 42.77828747034073\n",
      "step 613: Agent survived [14] steps with reward 7.275022625923157\n",
      "loss = 37879.625\n",
      "step 623: Agent survived [83] steps with reward 64.58472061157227\n",
      "step 631: Agent survived [62] steps with reward 43.041896760463715\n",
      "step 640: Agent survived [77] steps with reward 53.67257022857666\n",
      "step 652: Agent survived [110] steps with reward 81.65240973234177\n",
      "step 660: Agent survived [62] steps with reward 44.29464936256409\n",
      "loss = 43277.273\n",
      "step 672: Agent survived [110] steps with reward 80.95446252822876\n",
      "step 688: Agent survived [155] steps with reward 107.35998910665512\n",
      "step 700: Agent survived [119] steps with reward 83.5286568403244\n",
      "step 706: Agent survived [29] steps with reward 20.588762640953064\n",
      "step 712: Agent survived [41] steps with reward 26.936502993106842\n",
      "loss = 25895.457\n",
      "step 733: Agent survived [224] steps with reward 172.35824632644653\n",
      "step 740: Agent survived [44] steps with reward 30.294071197509766\n",
      "step 757: Agent survived [173] steps with reward 119.59729623794556\n",
      "step 767: Agent survived [83] steps with reward 50.78822332620621\n",
      "step 778: Agent survived [95] steps with reward 67.51984864473343\n",
      "loss = 16519.654\n",
      "step 814: Agent survived [401] steps with reward 278.6634377837181\n",
      "step 825: Agent survived [95] steps with reward 67.59727656841278\n",
      "step 835: Agent survived [92] steps with reward 64.67961770296097\n",
      "loss = 19405.818\n",
      "step 849: Agent survived [128] steps with reward 92.23667484521866\n",
      "step 864: Agent survived [149] steps with reward 102.21176379919052\n",
      "step 868: Agent survived [11] steps with reward 6.904260993003845\n",
      "step 878: Agent survived [91] steps with reward 65.62923526763916\n",
      "step 891: Agent survived [120] steps with reward 86.18574839830399\n",
      "loss = 14548.018\n",
      "step 899: Agent survived [62] steps with reward 42.424538373947144\n",
      "step 904: Agent survived [23] steps with reward 13.964320123195648\n",
      "step 919: Agent survived [149] steps with reward 108.85882312059402\n",
      "step 926: Agent survived [56] steps with reward 28.950388252735138\n",
      "step 942: Agent survived [155] steps with reward 108.08688545227051\n",
      "step 946: Agent survived [11] steps with reward 3.5315358638763428\n",
      "loss = 14300.388\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - force disconnection of 1 powerlines ([51])\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('Powerline with ids [51] have been modified illegally (cooldown)',)], 'rewards': {}}\n",
      "step 966: Agent survived [206] steps with reward 150.04617697000504\n",
      "step 971: Agent survived [26] steps with reward 17.182875514030457\n",
      "step 992: Agent survived [218] steps with reward 155.58394920825958\n",
      "Step [1000] -- Random [0.9847456352762105]\n",
      "step 1004: Agent survived [107] steps with reward 74.24162179231644\n",
      "loss = 9723.439\n",
      "step 1010: Agent survived [41] steps with reward 29.431814551353455\n",
      "step 1027: Agent survived [167] steps with reward 121.88173568248749\n",
      "step 1033: Agent survived [41] steps with reward 29.236880660057068\n",
      "step 1037: Agent survived [11] steps with reward 6.9598716497421265\n",
      "step 1043: Agent survived [38] steps with reward 28.28322148323059\n",
      "step 1050: Agent survived [50] steps with reward 37.647619009017944\n",
      "step 1054: Agent survived [17] steps with reward 10.118483185768127\n",
      "step 1059: Agent survived [26] steps with reward 16.8818701505661\n",
      "step 1063: Agent survived [14] steps with reward 7.563934803009033\n",
      "loss = 14279.141\n",
      "step 1070: Agent survived [50] steps with reward 33.424164950847626\n",
      "step 1083: Agent survived [122] steps with reward 90.40978688001633\n",
      "step 1095: Agent survived [110] steps with reward 79.16705340147018\n",
      "step 1099: Agent survived [17] steps with reward 7.227943062782288\n",
      "step 1108: Agent survived [68] steps with reward 48.49304324388504\n",
      "step 1113: Agent survived [31] steps with reward 18.45053917169571\n",
      "loss = 11445.465\n",
      "step 1120: Agent survived [48] steps with reward 32.16114228963852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1124: Agent survived [11] steps with reward 6.774989366531372\n",
      "step 1146: Agent survived [236] steps with reward 166.80284082889557\n",
      "step 1153: Agent survived [47] steps with reward 32.25837832689285\n",
      "step 1160: Agent survived [47] steps with reward 32.12467420101166\n",
      "step 1168: Agent survived [62] steps with reward 45.271926164627075\n",
      "loss = 5933.5986\n",
      "step 1180: Agent survived [116] steps with reward 85.06474888324738\n",
      "step 1186: Agent survived [32] steps with reward 21.31094402074814\n",
      "step 1191: Agent survived [29] steps with reward 18.872675716876984\n",
      "step 1209: Agent survived [182] steps with reward 131.97413992881775\n",
      "step 1213: Agent survived [14] steps with reward 6.118500232696533\n",
      "step 1218: Agent survived [26] steps with reward 14.799134612083435\n",
      "step 1228: Agent survived [86] steps with reward 62.307415187358856\n",
      "loss = 13107.638\n",
      "step 1234: Agent survived [38] steps with reward 26.613938808441162\n",
      "step 1242: Agent survived [62] steps with reward 43.31673884391785\n",
      "step 1247: Agent survived [26] steps with reward 17.19504141807556\n",
      "step 1260: Agent survived [124] steps with reward 79.71375304460526\n",
      "step 1266: Agent survived [36] steps with reward 23.353050708770752\n",
      "step 1277: Agent survived [103] steps with reward 71.59186720848083\n",
      "step 1282: Agent survived [21] steps with reward 11.998898327350616\n",
      "loss = 12908.333\n",
      "step 1295: Agent survived [122] steps with reward 87.53712236881256\n",
      "step 1301: Agent survived [35] steps with reward 25.88975965976715\n",
      "step 1311: Agent survived [86] steps with reward 67.27119946479797\n",
      "step 1320: Agent survived [77] steps with reward 56.04633712768555\n",
      "step 1326: Agent survived [38] steps with reward 27.095285892486572\n",
      "step 1338: Agent survived [110] steps with reward 82.60555642843246\n",
      "loss = 9119.077\n",
      "step 1346: Agent survived [62] steps with reward 44.28404366970062\n",
      "step 1356: Agent survived [83] steps with reward 60.550114154815674\n",
      "step 1361: Agent survived [26] steps with reward 18.72303545475006\n",
      "step 1372: Agent survived [107] steps with reward 70.97152400016785\n",
      "step 1376: Agent survived [14] steps with reward 3.7549514770507812\n",
      "step 1387: Agent survived [89] steps with reward 61.728049993515015\n",
      "step 1393: Agent survived [41] steps with reward 27.747518181800842\n",
      "loss = 5239.1475\n",
      "step 1400: Agent survived [47] steps with reward 33.929284155368805\n",
      "step 1404: Agent survived [17] steps with reward 10.533937573432922\n",
      "step 1409: Agent survived [26] steps with reward 15.645585179328918\n",
      "step 1415: Agent survived [38] steps with reward 26.72361135482788\n",
      "step 1421: Agent survived [35] steps with reward 25.488688707351685\n",
      "step 1428: Agent survived [53] steps with reward 33.40859651565552\n",
      "step 1434: Agent survived [41] steps with reward 19.945742189884186\n",
      "step 1442: Agent survived [56] steps with reward 41.135718166828156\n",
      "step 1447: Agent survived [29] steps with reward 19.585719347000122\n",
      "step 1451: Agent survived [14] steps with reward 7.451594233512878\n",
      "loss = 9649.706\n",
      "step 1469: Agent survived [182] steps with reward 115.38668519258499\n",
      "step 1480: Agent survived [98] steps with reward 68.30189388990402\n",
      "step 1494: Agent survived [131] steps with reward 97.53949302434921\n",
      "step 1498: Agent survived [14] steps with reward 9.475519895553589\n",
      "step 1502: Agent survived [17] steps with reward 10.278171420097351\n",
      "step 1506: Agent survived [14] steps with reward 7.298766613006592\n",
      "step 1510: Agent survived [14] steps with reward 6.080560505390167\n",
      "loss = 6551.962\n",
      "step 1516: Agent survived [35] steps with reward 25.89007794857025\n",
      "step 1521: Agent survived [32] steps with reward 19.686701953411102\n",
      "step 1527: Agent survived [35] steps with reward 24.090046644210815\n",
      "step 1533: Agent survived [35] steps with reward 23.161566197872162\n",
      "step 1538: Agent survived [29] steps with reward 19.72697949409485\n",
      "step 1547: Agent survived [74] steps with reward 50.353224754333496\n",
      "step 1555: Agent survived [62] steps with reward 44.34650760889053\n",
      "step 1564: Agent survived [74] steps with reward 52.40061664581299\n",
      "loss = 4127.3545\n",
      "step 1568: Agent survived [11] steps with reward 6.739620685577393\n",
      "step 1592: Agent survived [254] steps with reward 191.89744287729263\n",
      "step 1607: Agent survived [146] steps with reward 111.86954987049103\n",
      "step 1622: Agent survived [149] steps with reward 110.55638146400452\n",
      "loss = 3623.8662\n",
      "step 1629: Agent survived [50] steps with reward 34.52164548635483\n",
      "step 1635: Agent survived [38] steps with reward 23.751008808612823\n",
      "step 1640: Agent survived [29] steps with reward 17.490038216114044\n",
      "step 1653: Agent survived [116] steps with reward 78.77261137962341\n",
      "step 1668: Agent survived [149] steps with reward 111.0209299325943\n",
      "step 1679: Agent survived [95] steps with reward 66.89144688844681\n",
      "loss = 1818.7461\n",
      "step 1686: Agent survived [53] steps with reward 33.87219059467316\n",
      "step 1709: Agent survived [242] steps with reward 170.9363654255867\n",
      "step 1713: Agent survived [20] steps with reward 7.826320290565491\n",
      "step 1731: Agent survived [176] steps with reward 130.80948513746262\n",
      "loss = 39690.066\n",
      "step 1745: Agent survived [131] steps with reward 94.98264616727829\n",
      "step 1762: Agent survived [170] steps with reward 119.84808379411697\n",
      "step 1773: Agent survived [98] steps with reward 72.9489421248436\n",
      "step 1778: Agent survived [29] steps with reward 19.6247718334198\n",
      "step 1784: Agent survived [38] steps with reward 22.37811553478241\n",
      "step 1788: Agent survived [14] steps with reward 6.1367886662483215\n",
      "loss = 37079.664\n",
      "step 1796: Agent survived [62] steps with reward 44.038737297058105\n",
      "step 1817: Agent survived [215] steps with reward 156.60102194547653\n",
      "step 1824: Agent survived [50] steps with reward 35.750321328639984\n",
      "step 1837: Agent survived [122] steps with reward 90.75058698654175\n",
      "step 1844: Agent survived [53] steps with reward 37.900918424129486\n",
      "loss = 32384.953\n",
      "step 1867: Agent survived [242] steps with reward 165.20900851488113\n",
      "step 1888: Agent survived [218] steps with reward 158.51086443662643\n",
      "step 1892: Agent survived [14] steps with reward 7.431169867515564\n",
      "step 1902: Agent survived [86] steps with reward 62.34135717153549\n",
      "loss = 29681.96\n",
      "step 1907: Agent survived [26] steps with reward 15.632879078388214\n",
      "step 1917: Agent survived [83] steps with reward 57.25819593667984\n",
      "step 1928: Agent survived [101] steps with reward 76.41211169958115\n",
      "step 1935: Agent survived [53] steps with reward 34.30862331390381\n",
      "step 1944: Agent survived [71] steps with reward 52.18814414739609\n",
      "step 1957: Agent survived [122] steps with reward 88.76280814409256\n",
      "loss = 46379.516\n",
      "step 1961: Agent survived [17] steps with reward 7.222787618637085\n",
      "step 1979: Agent survived [179] steps with reward 130.26673656702042\n",
      "step 1984: Agent survived [26] steps with reward 17.50717043876648\n",
      "step 1994: Agent survived [90] steps with reward 56.746872782707214\n",
      "Step [2000] -- Random [0.9295309194857401]\n",
      "Successfully saved model at: kaist_agent_D3QN_opponent_20_5000/D3QN_kaist.h5\n",
      "step 2001: Agent survived [46] steps with reward 32.78844738006592\n",
      "loss = 37448.0\n",
      "step 2018: Agent survived [167] steps with reward 129.13282996416092\n",
      "step 2026: Agent survived [65] steps with reward 43.53596347570419\n",
      "step 2034: Agent survived [62] steps with reward 43.26749724149704\n",
      "step 2044: Agent survived [86] steps with reward 63.05784660577774\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - force disconnection of 1 powerlines ([55])\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('Powerline with ids [55] have been modified illegally (cooldown)',)], 'rewards': {}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2057: Agent survived [119] steps with reward 92.7317344546318\n",
      "step 2068: Agent survived [101] steps with reward 73.76946115493774\n",
      "loss = 34156.945\n",
      "step 2074: Agent survived [35] steps with reward 25.109918475151062\n",
      "step 2083: Agent survived [74] steps with reward 57.56542372703552\n",
      "step 2094: Agent survived [101] steps with reward 72.20702522993088\n",
      "step 2099: Agent survived [23] steps with reward 13.899448573589325\n",
      "step 2108: Agent survived [77] steps with reward 52.176638305187225\n",
      "step 2116: Agent survived [62] steps with reward 42.98801392316818\n",
      "loss = 18487.361\n",
      "step 2130: Agent survived [134] steps with reward 100.60043799877167\n",
      "step 2141: Agent survived [98] steps with reward 72.81356489658356\n",
      "step 2149: Agent survived [59] steps with reward 44.62063026428223\n",
      "step 2171: Agent survived [233] steps with reward 169.5452790260315\n",
      "loss = 22827.75\n",
      "step 2186: Agent survived [146] steps with reward 109.46653825044632\n",
      "step 2190: Agent survived [14] steps with reward 7.380891799926758\n",
      "step 2202: Agent survived [110] steps with reward 83.5438334941864\n",
      "step 2209: Agent survived [50] steps with reward 29.294392228126526\n",
      "step 2215: Agent survived [38] steps with reward 23.596668541431427\n",
      "step 2220: Agent survived [26] steps with reward 16.960026502609253\n",
      "step 2228: Agent survived [62] steps with reward 41.246857047080994\n",
      "step 2234: Agent survived [38] steps with reward 24.44894826412201\n",
      "step 2238: Agent survived [14] steps with reward 5.89190798997879\n",
      "loss = 24314.594\n",
      "step 2248: Agent survived [86] steps with reward 59.126352310180664\n",
      "step 2260: Agent survived [110] steps with reward 80.8767329454422\n",
      "step 2275: Agent survived [146] steps with reward 107.43385577201843\n",
      "step 2284: Agent survived [71] steps with reward 51.502451062202454\n",
      "loss = 16387.17\n",
      "step 2300: Agent survived [163] steps with reward 121.76326245069504\n",
      "step 2312: Agent survived [105] steps with reward 74.63053470849991\n",
      "step 2316: Agent survived [17] steps with reward 10.025768041610718\n",
      "step 2323: Agent survived [50] steps with reward 34.41671133041382\n",
      "step 2338: Agent survived [146] steps with reward 96.26863098144531\n",
      "step 2343: Agent survived [26] steps with reward 17.019391655921936\n",
      "loss = 18573.32\n",
      "step 2355: Agent survived [116] steps with reward 79.47664213180542\n",
      "step 2365: Agent survived [77] steps with reward 53.97994548082352\n",
      "step 2370: Agent survived [29] steps with reward 13.199835240840912\n",
      "step 2377: Agent survived [50] steps with reward 35.94884932041168\n",
      "step 2389: Agent survived [107] steps with reward 73.13040393590927\n",
      "step 2400: Agent survived [107] steps with reward 72.53529173135757\n",
      "step 2404: Agent survived [8] steps with reward 2.3474522829055786\n",
      "loss = 24680.684\n",
      "step 2435: Agent survived [338] steps with reward 255.82627552747726\n",
      "step 2439: Agent survived [14] steps with reward 6.045503795146942\n",
      "step 2448: Agent survived [74] steps with reward 53.90550243854523\n",
      "step 2454: Agent survived [43] steps with reward 27.814166605472565\n",
      "loss = 22511.057\n",
      "step 2467: Agent survived [123] steps with reward 91.30497854948044\n",
      "step 2476: Agent survived [68] steps with reward 42.19465446472168\n",
      "step 2480: Agent survived [17] steps with reward 7.272972047328949\n",
      "step 2489: Agent survived [71] steps with reward 41.820237815380096\n",
      "step 2496: Agent survived [50] steps with reward 30.050414204597473\n",
      "step 2507: Agent survived [95] steps with reward 69.08040338754654\n",
      "step 2514: Agent survived [50] steps with reward 35.687888383865356\n",
      "loss = 15129.639\n",
      "step 2522: Agent survived [62] steps with reward 47.59674692153931\n",
      "step 2540: Agent survived [185] steps with reward 141.67582523822784\n",
      "step 2553: Agent survived [122] steps with reward 88.87997907400131\n",
      "step 2568: Agent survived [146] steps with reward 84.81399893760681\n",
      "step 2573: Agent survived [26] steps with reward 16.9800386428833\n",
      "loss = 15297.726\n",
      "step 2589: Agent survived [158] steps with reward 110.60137385129929\n",
      "step 2594: Agent survived [23] steps with reward 13.954190790653229\n",
      "step 2601: Agent survived [53] steps with reward 30.599342942237854\n",
      "step 2613: Agent survived [107] steps with reward 74.72058844566345\n",
      "step 2619: Agent survived [41] steps with reward 26.80112135410309\n",
      "step 2624: Agent survived [26] steps with reward 15.710406720638275\n",
      "loss = 15093.11\n",
      "step 2633: Agent survived [77] steps with reward 53.04171499609947\n",
      "step 2643: Agent survived [86] steps with reward 59.72624754905701\n",
      "step 2648: Agent survived [23] steps with reward 14.342868089675903\n",
      "step 2654: Agent survived [38] steps with reward 23.93833953142166\n",
      "step 2665: Agent survived [98] steps with reward 67.65388160943985\n",
      "step 2670: Agent survived [26] steps with reward 15.866497278213501\n",
      "step 2676: Agent survived [38] steps with reward 21.870947778224945\n",
      "loss = 12776.436\n",
      "step 2693: Agent survived [167] steps with reward 111.24791592359543\n",
      "step 2704: Agent survived [101] steps with reward 70.38318282365799\n",
      "step 2717: Agent survived [119] steps with reward 86.3780483007431\n",
      "step 2728: Agent survived [98] steps with reward 71.61503434181213\n",
      "step 2733: Agent survived [32] steps with reward 20.100266337394714\n",
      "loss = 14395.801\n",
      "step 2747: Agent survived [128] steps with reward 91.26541233062744\n",
      "step 2751: Agent survived [17] steps with reward 8.578193962574005\n",
      "step 2755: Agent survived [14] steps with reward 7.400900840759277\n",
      "step 2768: Agent survived [122] steps with reward 74.10965937376022\n",
      "step 2774: Agent survived [38] steps with reward 24.66509521007538\n",
      "step 2785: Agent survived [95] steps with reward 68.11448359489441\n",
      "step 2791: Agent survived [41] steps with reward 26.95745301246643\n",
      "loss = 11761.989\n",
      "step 2801: Agent survived [83] steps with reward 57.17345458269119\n",
      "step 2808: Agent survived [50] steps with reward 36.038132548332214\n",
      "step 2814: Agent survived [38] steps with reward 21.407546639442444\n",
      "step 2819: Agent survived [26] steps with reward 18.930907726287842\n",
      "step 2832: Agent survived [125] steps with reward 86.40097695589066\n",
      "step 2839: Agent survived [50] steps with reward 34.04357975721359\n",
      "step 2846: Agent survived [50] steps with reward 34.91037219762802\n",
      "loss = 10784.02\n",
      "step 2858: Agent survived [115] steps with reward 82.92284065485\n",
      "step 2885: Agent survived [288] steps with reward 214.83726060390472\n",
      "step 2894: Agent survived [71] steps with reward 47.53329420089722\n",
      "step 2901: Agent survived [50] steps with reward 34.32128745317459\n",
      "loss = 9387.32\n",
      "step 2917: Agent survived [161] steps with reward 105.89375901222229\n",
      "step 2921: Agent survived [11] steps with reward 4.95306396484375\n",
      "step 2927: Agent survived [35] steps with reward 23.735598981380463\n",
      "step 2932: Agent survived [29] steps with reward 19.56701898574829\n",
      "step 2941: Agent survived [80] steps with reward 34.3483070731163\n",
      "step 2953: Agent survived [101] steps with reward 75.8047970533371\n",
      "step 2957: Agent survived [20] steps with reward 10.178965389728546\n",
      "step 2965: Agent survived [62] steps with reward 41.26968342065811\n",
      "loss = 7866.527\n",
      "step 2971: Agent survived [38] steps with reward 24.80917501449585\n",
      "step 2977: Agent survived [35] steps with reward 22.10376888513565\n",
      "step 2987: Agent survived [83] steps with reward 60.81741404533386\n",
      "step 2991: Agent survived [14] steps with reward 9.484434485435486\n",
      "step 2995: Agent survived [17] steps with reward 8.479191601276398\n",
      "Step [3000] -- Random [0.8806067401103539]\n",
      "step 3004: Agent survived [71] steps with reward 53.34710592031479\n",
      "step 3012: Agent survived [65] steps with reward 46.460503935813904\n",
      "step 3021: Agent survived [74] steps with reward 53.665331184864044\n",
      "loss = 7142.1807\n",
      "step 3028: Agent survived [53] steps with reward 35.69065606594086\n",
      "step 3045: Agent survived [167] steps with reward 124.9425899386406\n",
      "step 3052: Agent survived [47] steps with reward 35.2537385225296\n",
      "step 3066: Agent survived [134] steps with reward 97.95352071523666\n",
      "step 3076: Agent survived [92] steps with reward 65.05847263336182\n",
      "loss = 9896.167\n",
      "step 3093: Agent survived [167] steps with reward 126.46293640136719\n",
      "step 3098: Agent survived [26] steps with reward 17.17472219467163\n",
      "step 3114: Agent survived [161] steps with reward 118.2488603591919\n",
      "step 3118: Agent survived [11] steps with reward 3.356401562690735\n",
      "step 3123: Agent survived [26] steps with reward 16.996925354003906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3128: Agent survived [26] steps with reward 16.000807523727417\n",
      "loss = 9350.234\n",
      "step 3138: Agent survived [86] steps with reward 61.45883822441101\n",
      "step 3142: Agent survived [14] steps with reward 5.959876656532288\n",
      "step 3151: Agent survived [74] steps with reward 55.20153284072876\n",
      "step 3162: Agent survived [98] steps with reward 70.09688293933868\n",
      "step 3170: Agent survived [62] steps with reward 39.8563197851181\n",
      "step 3179: Agent survived [71] steps with reward 51.99316471815109\n",
      "step 3189: Agent survived [89] steps with reward 64.47385013103485\n",
      "loss = 9232.734\n",
      "step 3198: Agent survived [74] steps with reward 52.73938947916031\n",
      "step 3218: Agent survived [203] steps with reward 146.93529379367828\n",
      "step 3224: Agent survived [41] steps with reward 29.38334596157074\n",
      "step 3229: Agent survived [23] steps with reward 16.131144285202026\n",
      "step 3237: Agent survived [65] steps with reward 45.44643718004227\n",
      "step 3247: Agent survived [86] steps with reward 63.619648575782776\n",
      "loss = 8446.254\n",
      "step 3252: Agent survived [29] steps with reward 17.360542953014374\n",
      "step 3263: Agent survived [95] steps with reward 67.23326247930527\n",
      "step 3269: Agent survived [38] steps with reward 21.286286652088165\n",
      "step 3279: Agent survived [86] steps with reward 57.13676542043686\n",
      "step 3287: Agent survived [62] steps with reward 35.9716482758522\n",
      "step 3293: Agent survived [38] steps with reward 26.275957703590393\n",
      "loss = 5591.411\n",
      "step 3310: Agent survived [167] steps with reward 126.0257283449173\n",
      "step 3325: Agent survived [152] steps with reward 107.25552266836166\n",
      "step 3340: Agent survived [146] steps with reward 104.7429558634758\n",
      "step 3350: Agent survived [80] steps with reward 56.34545075893402\n",
      "step 3357: Agent survived [50] steps with reward 37.299099802970886\n",
      "loss = 9580.924\n",
      "step 3364: Agent survived [53] steps with reward 34.132660269737244\n",
      "step 3374: Agent survived [83] steps with reward 58.64717262983322\n",
      "step 3386: Agent survived [116] steps with reward 75.9696359038353\n",
      "step 3390: Agent survived [11] steps with reward 4.666937708854675\n",
      "step 3396: Agent survived [35] steps with reward 25.163081169128418\n",
      "step 3401: Agent survived [32] steps with reward 16.85431659221649\n",
      "step 3408: Agent survived [44] steps with reward 31.57639253139496\n",
      "step 3412: Agent survived [17] steps with reward 10.130158543586731\n",
      "loss = 7721.6875\n",
      "step 3419: Agent survived [50] steps with reward 34.03382521867752\n",
      "step 3431: Agent survived [114] steps with reward 82.81211173534393\n",
      "step 3446: Agent survived [145] steps with reward 104.46487158536911\n",
      "step 3453: Agent survived [44] steps with reward 30.52857905626297\n",
      "step 3467: Agent survived [137] steps with reward 99.72320854663849\n",
      "loss = 2599.3142\n",
      "step 3475: Agent survived [65] steps with reward 41.42118978500366\n",
      "step 3484: Agent survived [71] steps with reward 50.92125600576401\n",
      "step 3496: Agent survived [114] steps with reward 81.2282327413559\n",
      "step 3507: Agent survived [91] steps with reward 69.45684766769409\n",
      "loss = 1342.9836\n",
      "step 3529: Agent survived [233] steps with reward 173.73100239038467\n",
      "step 3546: Agent survived [170] steps with reward 126.6582687497139\n",
      "step 3567: Agent survived [215] steps with reward 146.12540519237518\n",
      "step 3572: Agent survived [32] steps with reward 19.667113840579987\n",
      "step 3578: Agent survived [35] steps with reward 22.695919036865234\n",
      "loss = 1296.1427\n",
      "step 3593: Agent survived [146] steps with reward 99.13265836238861\n",
      "step 3597: Agent survived [14] steps with reward 5.945461630821228\n",
      "step 3604: Agent survived [50] steps with reward 30.73430860042572\n",
      "step 3625: Agent survived [215] steps with reward 154.83927708864212\n",
      "step 3632: Agent survived [53] steps with reward 30.813827335834503\n",
      "loss = 731.2099\n",
      "step 3640: Agent survived [67] steps with reward 43.429242968559265\n",
      "step 3648: Agent survived [57] steps with reward 40.74509572982788\n",
      "step 3655: Agent survived [50] steps with reward 35.34613311290741\n",
      "step 3665: Agent survived [89] steps with reward 58.39053171873093\n",
      "step 3682: Agent survived [166] steps with reward 104.11102402210236\n",
      "step 3694: Agent survived [117] steps with reward 85.80197364091873\n",
      "loss = 1206.7905\n",
      "step 3702: Agent survived [56] steps with reward 38.47507494688034\n",
      "step 3713: Agent survived [95] steps with reward 71.16241681575775\n",
      "step 3725: Agent survived [110] steps with reward 79.62080407142639\n",
      "step 3734: Agent survived [77] steps with reward 53.1999626159668\n",
      "step 3739: Agent survived [29] steps with reward 15.83290958404541\n",
      "step 3749: Agent survived [83] steps with reward 58.621306359767914\n",
      "loss = 872.568\n",
      "step 3758: Agent survived [71] steps with reward 47.10966032743454\n",
      "step 3768: Agent survived [86] steps with reward 59.93155550956726\n",
      "step 3780: Agent survived [113] steps with reward 79.6472761631012\n",
      "step 3789: Agent survived [74] steps with reward 54.318823635578156\n",
      "step 3793: Agent survived [14] steps with reward 6.321749269962311\n",
      "step 3802: Agent survived [74] steps with reward 53.32505863904953\n",
      "loss = 275.6272\n",
      "step 3820: Agent survived [182] steps with reward 126.06169104576111\n",
      "step 3841: Agent survived [215] steps with reward 135.76325142383575\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - force disconnection of 1 powerlines ([15])\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('Powerline with ids [15] have been modified illegally (cooldown)',)], 'rewards': {}}\n",
      "step 3850: Agent survived [77] steps with reward 57.73903226852417\n",
      "step 3855: Agent survived [23] steps with reward 16.438223361968994\n",
      "loss = 875.4918\n",
      "step 3865: Agent survived [89] steps with reward 61.24960815906525\n",
      "step 3869: Agent survived [11] steps with reward 6.840632200241089\n",
      "step 3879: Agent survived [92] steps with reward 61.936734825372696\n",
      "step 3894: Agent survived [140] steps with reward 100.3155597448349\n",
      "step 3903: Agent survived [77] steps with reward 54.942554235458374\n",
      "step 3907: Agent survived [17] steps with reward 7.177410960197449\n",
      "step 3915: Agent survived [56] steps with reward 42.51924967765808\n",
      "loss = 549.1204\n",
      "step 3927: Agent survived [120] steps with reward 86.49041736125946\n",
      "step 3937: Agent survived [79] steps with reward 56.61060988903046\n",
      "step 3942: Agent survived [26] steps with reward 17.054311275482178\n",
      "step 3953: Agent survived [101] steps with reward 73.12551480531693\n",
      "step 3966: Agent survived [119] steps with reward 83.40025323629379\n",
      "loss = 5584.5767\n",
      "step 3976: Agent survived [86] steps with reward 62.394133031368256\n",
      "step 3983: Agent survived [50] steps with reward 35.82884240150452\n",
      "step 3988: Agent survived [23] steps with reward 16.369126558303833\n",
      "step 3998: Agent survived [86] steps with reward 63.18862211704254\n",
      "Step [4000] -- Random [0.8366852317220175]\n",
      "Successfully saved model at: kaist_agent_D3QN_opponent_20_5000/D3QN_kaist.h5\n",
      "step 4008: Agent survived [89] steps with reward 60.837977170944214\n",
      "step 4015: Agent survived [50] steps with reward 32.75110858678818\n",
      "step 4021: Agent survived [35] steps with reward 25.572091460227966\n",
      "loss = 7786.461\n",
      "step 4034: Agent survived [122] steps with reward 77.79895162582397\n",
      "step 4040: Agent survived [38] steps with reward 28.62530767917633\n",
      "step 4047: Agent survived [53] steps with reward 30.083100855350494\n",
      "step 4051: Agent survived [14] steps with reward 6.0221744775772095\n",
      "step 4058: Agent survived [50] steps with reward 30.581171989440918\n",
      "step 4072: Agent survived [134] steps with reward 100.57192742824554\n",
      "step 4078: Agent survived [41] steps with reward 26.99584311246872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4085: Agent survived [47] steps with reward 32.88774049282074\n",
      "loss = 8290.16\n",
      "step 4096: Agent survived [98] steps with reward 71.51432198286057\n",
      "step 4108: Agent survived [113] steps with reward 82.7127189040184\n",
      "step 4122: Agent survived [131] steps with reward 96.03501015901566\n",
      "step 4130: Agent survived [59] steps with reward 43.0027619600296\n",
      "step 4137: Agent survived [59] steps with reward 39.96928060054779\n",
      "loss = 5910.961\n",
      "step 4145: Agent survived [56] steps with reward 39.191299736499786\n",
      "step 4150: Agent survived [26] steps with reward 17.006691694259644\n",
      "step 4165: Agent survived [143] steps with reward 101.86745148897171\n",
      "step 4172: Agent survived [53] steps with reward 36.90305668115616\n",
      "step 4179: Agent survived [50] steps with reward 31.474450409412384\n",
      "step 4185: Agent survived [38] steps with reward 24.02320009469986\n",
      "step 4189: Agent survived [14] steps with reward 7.4701114892959595\n",
      "loss = 2857.7742\n",
      "step 4204: Agent survived [153] steps with reward 104.26078885793686\n",
      "step 4220: Agent survived [154] steps with reward 98.35236066579819\n",
      "step 4227: Agent survived [47] steps with reward 32.76834899187088\n",
      "step 4246: Agent survived [191] steps with reward 137.90169382095337\n",
      "step 4252: Agent survived [41] steps with reward 29.344349145889282\n",
      "loss = 2384.047\n",
      "step 4258: Agent survived [35] steps with reward 25.46537947654724\n",
      "step 4270: Agent survived [116] steps with reward 83.2801479101181\n",
      "step 4280: Agent survived [83] steps with reward 56.46928596496582\n",
      "step 4288: Agent survived [59] steps with reward 44.218553602695465\n",
      "step 4299: Agent survived [98] steps with reward 72.22217589616776\n",
      "step 4310: Agent survived [98] steps with reward 70.34882712364197\n",
      "loss = 2187.208\n",
      "step 4321: Agent survived [101] steps with reward 69.48127865791321\n",
      "step 4325: Agent survived [14] steps with reward 6.042677521705627\n",
      "step 4331: Agent survived [38] steps with reward 26.620768666267395\n",
      "step 4344: Agent survived [122] steps with reward 83.68495100736618\n",
      "step 4349: Agent survived [26] steps with reward 16.996460676193237\n",
      "step 4358: Agent survived [74] steps with reward 50.940854609012604\n",
      "loss = 1030.2118\n",
      "step 4374: Agent survived [155] steps with reward 114.74527883529663\n",
      "step 4383: Agent survived [77] steps with reward 53.03799843788147\n",
      "step 4398: Agent survived [146] steps with reward 108.22777664661407\n",
      "step 4409: Agent survived [98] steps with reward 66.9150550365448\n",
      "step 4422: Agent survived [125] steps with reward 86.540964782238\n",
      "loss = 1684.1926\n",
      "step 4439: Agent survived [167] steps with reward 120.67873311042786\n",
      "step 4446: Agent survived [50] steps with reward 34.68107599020004\n",
      "step 4454: Agent survived [62] steps with reward 44.492109060287476\n",
      "step 4463: Agent survived [74] steps with reward 49.40804982185364\n",
      "step 4470: Agent survived [50] steps with reward 33.58429342508316\n",
      "step 4474: Agent survived [14] steps with reward 7.301244139671326\n",
      "step 4479: Agent survived [26] steps with reward 17.474955081939697\n",
      "loss = 1266.1886\n",
      "step 4485: Agent survived [41] steps with reward 26.503855764865875\n",
      "step 4490: Agent survived [23] steps with reward 13.679924428462982\n",
      "step 4502: Agent survived [113] steps with reward 82.59215778112411\n",
      "step 4515: Agent survived [116] steps with reward 80.84858065843582\n",
      "step 4521: Agent survived [41] steps with reward 29.255698800086975\n",
      "step 4532: Agent survived [101] steps with reward 73.65552723407745\n",
      "loss = 1297.5752\n",
      "step 4544: Agent survived [107] steps with reward 77.09785348176956\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - force disconnection of 1 powerlines ([5])\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('Powerline with ids [5] have been modified illegally (cooldown)',)], 'rewards': {}}\n",
      "step 4563: Agent survived [191] steps with reward 140.3569012284279\n",
      "step 4575: Agent survived [110] steps with reward 77.67122662067413\n",
      "step 4587: Agent survived [113] steps with reward 81.12541282176971\n",
      "loss = 4525.2188\n",
      "step 4596: Agent survived [77] steps with reward 52.99667751789093\n",
      "step 4601: Agent survived [23] steps with reward 14.39387309551239\n",
      "step 4614: Agent survived [125] steps with reward 87.66305178403854\n",
      "step 4626: Agent survived [107] steps with reward 74.3364719748497\n",
      "step 4639: Agent survived [119] steps with reward 87.06672888994217\n",
      "loss = 3365.5913\n",
      "step 4652: Agent survived [125] steps with reward 93.35620993375778\n",
      "step 4676: Agent survived [251] steps with reward 182.17180544137955\n",
      "step 4687: Agent survived [101] steps with reward 69.61690205335617\n",
      "step 4697: Agent survived [86] steps with reward 55.44418406486511\n",
      "loss = 105.98872\n",
      "step 4710: Agent survived [119] steps with reward 83.16312140226364\n",
      "step 4720: Agent survived [86] steps with reward 64.33467209339142\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - force disconnection of 1 powerlines ([44])\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('Powerline with ids [44] have been modified illegally (cooldown)',)], 'rewards': {}}\n",
      "step 4731: Agent survived [101] steps with reward 75.63083338737488\n",
      "step 4741: Agent survived [83] steps with reward 62.37739872932434\n",
      "step 4747: Agent survived [41] steps with reward 23.93382441997528\n",
      "step 4753: Agent survived [41] steps with reward 26.772508144378662\n",
      "step 4759: Agent survived [38] steps with reward 23.999842047691345\n",
      "loss = 130.63734\n",
      "step 4763: Agent survived [14] steps with reward 4.955362856388092\n",
      "step 4767: Agent survived [14] steps with reward 5.081483483314514\n",
      "step 4789: Agent survived [232] steps with reward 170.18105882406235\n",
      "step 4795: Agent survived [33] steps with reward 20.957291901111603\n",
      "step 4804: Agent survived [77] steps with reward 53.87107300758362\n",
      "step 4810: Agent survived [32] steps with reward 23.33380675315857\n",
      "step 4815: Agent survived [32] steps with reward 19.79059261083603\n",
      "loss = 141.07509\n",
      "step 4820: Agent survived [23] steps with reward 13.355943262577057\n",
      "step 4827: Agent survived [53] steps with reward 34.86894649267197\n",
      "step 4831: Agent survived [8] steps with reward 3.9154735803604126\n",
      "step 4835: Agent survived [17] steps with reward 9.217573881149292\n",
      "step 4842: Agent survived [47] steps with reward 31.06769073009491\n",
      "step 4850: Agent survived [65] steps with reward 35.33667266368866\n",
      "step 4858: Agent survived [59] steps with reward 40.147853314876556\n",
      "step 4865: Agent survived [53] steps with reward 28.766342222690582\n",
      "step 4870: Agent survived [26] steps with reward 7.152042090892792\n",
      "loss = 84.021545\n",
      "step 4876: Agent survived [41] steps with reward 24.628164291381836\n",
      "step 4882: Agent survived [32] steps with reward 21.23356854915619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4887: Agent survived [29] steps with reward 17.47747230529785\n",
      "step 4891: Agent survived [15] steps with reward 6.047279417514801\n",
      "step 4896: Agent survived [22] steps with reward 14.496838927268982\n",
      "step 4900: Agent survived [14] steps with reward 8.51778507232666\n",
      "step 4906: Agent survived [38] steps with reward 21.525287330150604\n",
      "step 4911: Agent survived [26] steps with reward 15.73215264081955\n",
      "step 4917: Agent survived [38] steps with reward 25.544349551200867\n",
      "step 4922: Agent survived [26] steps with reward 17.757930517196655\n",
      "loss = 57.7219\n",
      "step 4929: Agent survived [59] steps with reward 32.85921984910965\n",
      "step 4935: Agent survived [32] steps with reward 16.76658171415329\n",
      "step 4945: Agent survived [92] steps with reward 53.086640655994415\n",
      "step 4951: Agent survived [29] steps with reward 16.098089039325714\n",
      "step 4955: Agent survived [14] steps with reward 8.458327770233154\n",
      "step 4960: Agent survived [29] steps with reward 13.203466534614563\n",
      "step 4965: Agent survived [23] steps with reward 14.88700544834137\n",
      "step 4969: Agent survived [20] steps with reward 8.040532529354095\n",
      "step 4973: Agent survived [8] steps with reward 3.9370633363723755\n",
      "step 4980: Agent survived [53] steps with reward 30.364118218421936\n",
      "loss = 709.48\n",
      "step 4986: Agent survived [38] steps with reward 23.07751351594925\n",
      "step 4990: Agent survived [11] steps with reward 6.264913201332092\n",
      "step 4994: Agent survived [14] steps with reward 8.920560479164124\n",
      "Step [5000] -- Random [0.7968375574149078]\n",
      "step 5000: Agent survived [41] steps with reward 25.601308405399323\n",
      "step 5006: Agent survived [35] steps with reward 23.347368001937866\n",
      "step 5010: Agent survived [14] steps with reward 8.438040852546692\n",
      "step 5016: Agent survived [38] steps with reward 25.604827284812927\n",
      "step 5021: Agent survived [29] steps with reward 16.205595433712006\n",
      "step 5027: Agent survived [38] steps with reward 24.655441522598267\n",
      "step 5034: Agent survived [47] steps with reward 31.029626429080963\n",
      "step 5038: Agent survived [20] steps with reward 9.21424025297165\n",
      "loss = 2944.3757\n",
      "step 5043: Agent survived [20] steps with reward 12.716613531112671\n",
      "step 5047: Agent survived [14] steps with reward 8.69041883945465\n",
      "step 5051: Agent survived [14] steps with reward 8.788731575012207\n",
      "step 5059: Agent survived [62] steps with reward 34.59693098068237\n",
      "step 5063: Agent survived [14] steps with reward 8.696630835533142\n",
      "step 5067: Agent survived [14] steps with reward 8.585043549537659\n",
      "step 5075: Agent survived [65] steps with reward 38.35675722360611\n",
      "step 5079: Agent survived [11] steps with reward 6.327313184738159\n",
      "step 5083: Agent survived [14] steps with reward 8.531613230705261\n",
      "step 5088: Agent survived [26] steps with reward 14.1495903134346\n",
      "step 5093: Agent survived [32] steps with reward 16.67992502450943\n",
      "loss = 1669.4476\n",
      "step 5097: Agent survived [11] steps with reward 4.328013181686401\n",
      "step 5101: Agent survived [14] steps with reward 6.6745429039001465\n",
      "step 5105: Agent survived [11] steps with reward 5.531398355960846\n",
      "step 5110: Agent survived [26] steps with reward 17.099693298339844\n",
      "step 5118: Agent survived [62] steps with reward 43.43906843662262\n",
      "step 5122: Agent survived [17] steps with reward 7.987229347229004\n",
      "step 5127: Agent survived [29] steps with reward 9.60509729385376\n",
      "step 5131: Agent survived [18] steps with reward 2.552660882472992\n",
      "step 5135: Agent survived [7] steps with reward 1.3526263236999512\n",
      "step 5144: Agent survived [71] steps with reward 46.48294407129288\n",
      "step 5151: Agent survived [53] steps with reward 33.68289113044739\n",
      "loss = 2333.1992\n",
      "step 5158: Agent survived [53] steps with reward 23.767646968364716\n",
      "step 5165: Agent survived [44] steps with reward 28.386527597904205\n",
      "step 5169: Agent survived [14] steps with reward 8.85578989982605\n",
      "step 5174: Agent survived [26] steps with reward 17.789079070091248\n",
      "step 5182: Agent survived [68] steps with reward 29.54078733921051\n",
      "step 5189: Agent survived [47] steps with reward 27.408111333847046\n",
      "step 5196: Agent survived [47] steps with reward 27.717520594596863\n",
      "step 5200: Agent survived [14] steps with reward 8.804014086723328\n",
      "loss = 2797.0154\n",
      "step 5208: Agent survived [62] steps with reward 38.698385417461395\n",
      "step 5212: Agent survived [17] steps with reward 7.520124137401581\n",
      "step 5223: Agent survived [98] steps with reward 60.70488905906677\n",
      "step 5227: Agent survived [17] steps with reward 6.85920923948288\n",
      "step 5231: Agent survived [8] steps with reward 3.9549036026000977\n",
      "step 5236: Agent survived [26] steps with reward 17.543147206306458\n",
      "step 5240: Agent survived [14] steps with reward 8.770702481269836\n",
      "step 5245: Agent survived [26] steps with reward 15.887120425701141\n",
      "step 5249: Agent survived [20] steps with reward 9.103106319904327\n",
      "step 5255: Agent survived [32] steps with reward 21.45929753780365\n",
      "Successfully saved model at: kaist_agent_D3QN_opponent_20_5000/D3QN_kaist.h5\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "n_iter = 5000\n",
    "# Register custom reward for training\n",
    "cr = env._reward_helper.template_reward\n",
    "#cr.addReward(\"overflow\", CloseToOverflowReward(), 1.0)\n",
    "cr.addReward(\"game\", GameplayReward(), 1.0)\n",
    "#cr.addReward(\"recolines\", LinesReconnectedReward(), 1.0)\n",
    "cr.addReward(\"l2rpn\", L2RPNReward(), 2.0/float(env.n_line))\n",
    "# Initialize custom rewards\n",
    "cr.initialize(env)\n",
    "# Set reward range to something managable\n",
    "cr.set_range(-1.0, 1.0)\n",
    "\n",
    "save_path = \"kaist_agent_D3QN_opponent_{}_{}\".format(attack_period, n_iter)\n",
    "log_path=\"tf_logs_D3QN\"\n",
    "\n",
    "train_adversary(env, agent, opponent, num_pre_training_steps, n_iter, save_path, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nutritional-denial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.insert(np.array([True, True, True]), 0, False, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op import make\n",
    "from grid2op.Runner import Runner\n",
    "from grid2op.Reward import L2RPNSandBoxScore, L2RPNReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episode = 10 # number of episodes to evaluate\n",
    "log_path = './logs-evals'\n",
    "nb_process = 1 # number of cores to use\n",
    "max_iter = 150 # maximum number of steps per scenario\n",
    "verbose = True\n",
    "save_gif = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'l2rpn_wcci_2020'\n",
    "env = make(env_name, reward_class=L2RPNSandBoxScore,\n",
    "           other_rewards={\n",
    "               \"reward\": L2RPNReward\n",
    "           })\n",
    "\n",
    "agent_name = \"kaist\"\n",
    "data_dir = os.path.join('kaist_agent/data')\n",
    "with open(os.path.join(data_dir, 'param.json'), 'r', encoding='utf-8') as f:\n",
    "    param = json.load(f)\n",
    "\n",
    "state_mean = torch.load(os.path.join(data_dir, 'mean.pt'), map_location=param['device']).cpu()\n",
    "state_std = torch.load(os.path.join(data_dir, 'std.pt'), map_location=param['device']).cpu()\n",
    "state_std = state_std.masked_fill(state_std<1e-5, 1.)\n",
    "state_mean[0, sum(env.observation_space.shape[:20]):] = 0\n",
    "state_std[0, sum(env.observation_space.shape[:20]):] = 1\n",
    "agent = Kaist(env, state_mean, state_std, name=agent_name, **param)\n",
    "agent.sim_trial = 0\n",
    "agent.load_model(data_dir)\n",
    "    \n",
    "runner_params = env.get_params_for_runner()\n",
    "runner_params[\"verbose\"] = False\n",
    "runner = Runner(**runner_params, agentClass=None, agentInstance=agent)\n",
    "    \n",
    "res = runner.run(path_save=log_path, nb_episode=nb_episode, nb_process=nb_process, max_iter=150)\n",
    "if verbose:\n",
    "    print(\"Evaluation summary:\")\n",
    "    for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "        msg_tmp = \"chronics at: {}\".format(chron_name)\n",
    "        msg_tmp += \"\\ttotal reward: {:.6f}\".format(cum_reward)\n",
    "        msg_tmp += \"\\ttime steps: {:.0f}/{:.0f}\".format(nb_time_step,\n",
    "                                                        max_ts)\n",
    "        print(msg_tmp)\n",
    "\n",
    "if save_gif:\n",
    "    save_log_gif(log_path, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-criticism",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
