{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "corresponding-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid2op\n",
    "import lightsim2grid\n",
    "import warnings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from lightsim2grid.LightSimBackend import LightSimBackend\n",
    "import numpy as np\n",
    "from agent import Track2PowerNetAgent\n",
    "\n",
    "from d3qn.adversary import D3QN_Opponent\n",
    "from l2rpn_baselines.DoubleDuelingDQN.DoubleDuelingDQNConfig import DoubleDuelingDQNConfig as cfg\n",
    "\n",
    "MAX_TIMESTEP = 7 * 288\n",
    "\n",
    "LINES = ['0_1_0', '0_2_1', '10_11_2', '69_70_3', '23_71_4', '70_71_5',\n",
    "       '70_72_6', '69_73_7', '69_74_8', '68_74_9', '73_74_10', '75_76_11',\n",
    "       '68_76_12', '1_11_13', '74_76_14', '76_77_15', '77_78_16',\n",
    "       '76_79_17', '76_79_18', '78_79_19', '76_81_20', '81_82_21',\n",
    "       '82_83_22', '82_84_23', '2_11_24', '83_84_25', '84_85_26',\n",
    "       '84_87_27', '84_88_28', '87_88_29', '88_89_30', '88_89_31',\n",
    "       '89_90_32', '88_91_33', '88_91_34', '6_11_35', '90_91_36',\n",
    "       '91_92_37', '91_93_38', '92_93_39', '93_94_40', '79_95_41',\n",
    "       '81_95_42', '93_95_43', '79_96_44', '79_97_45', '10_12_46',\n",
    "       '79_98_47', '91_99_48', '93_99_49', '94_95_50', '95_96_51',\n",
    "       '97_99_52', '98_99_53', '99_100_54', '91_101_55', '100_101_56',\n",
    "       '11_13_57', '99_102_58', '99_103_59', '102_103_60', '102_104_61',\n",
    "       '99_105_62', '103_104_63', '104_105_64', '104_106_65',\n",
    "       '104_107_66', '105_106_67', '12_14_68', '107_108_69', '102_109_70',\n",
    "       '108_109_71', '109_110_72', '109_111_73', '16_112_74', '31_112_75',\n",
    "       '31_113_76', '26_114_77', '113_114_78', '13_14_79', '11_116_80',\n",
    "       '74_117_81', '75_117_82', '11_15_83', '14_16_84', '3_4_85',\n",
    "       '15_16_86', '16_17_87', '17_18_88', '18_19_89', '14_18_90',\n",
    "       '19_20_91', '20_21_92', '21_22_93', '22_23_94', '22_24_95',\n",
    "       '2_4_96', '24_26_97', '26_27_98', '27_28_99', '7_29_100',\n",
    "       '25_29_101', '16_30_102', '28_30_103', '22_31_104', '30_31_105',\n",
    "       '26_31_106', '4_5_107', '14_32_108', '18_33_109', '34_35_110',\n",
    "       '34_36_111', '32_36_112', '33_35_113', '33_36_114', '36_38_115',\n",
    "       '36_39_116', '29_37_117', '5_6_118', '38_39_119', '39_40_120',\n",
    "       '39_41_121', '40_41_122', '42_43_123', '33_42_124', '43_44_125',\n",
    "       '44_45_126', '45_46_127', '45_47_128', '7_8_129', '46_48_130',\n",
    "       '41_48_131', '41_48_132', '44_48_133', '47_48_134', '48_49_135',\n",
    "       '48_50_136', '50_51_137', '51_52_138', '52_53_139', '8_9_140',\n",
    "       '48_53_141', '48_53_142', '53_54_143', '53_55_144', '54_55_145',\n",
    "       '55_56_146', '49_56_147', '55_57_148', '50_57_149', '53_58_150',\n",
    "       '3_10_151', '55_58_152', '55_58_153', '54_58_154', '58_59_155',\n",
    "       '58_60_156', '59_60_157', '59_61_158', '60_61_159', '62_63_160',\n",
    "       '37_64_161', '4_10_162', '63_64_163', '48_65_164', '48_65_165',\n",
    "       '61_65_166', '61_66_167', '65_66_168', '46_68_169', '48_68_170',\n",
    "       '68_69_171', '23_69_172', '7_4_173', '25_24_174', '80_79_175',\n",
    "       '86_85_176', '115_67_177', '29_16_178', '37_36_179', '62_58_180',\n",
    "       '63_60_181', '64_65_182', '64_67_183', '67_68_184', '80_67_185']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "potential-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversary(env, agent, opponent, num_pre_training_steps, n_iter, save_path, log_path):\n",
    "    # Make sure we can fill the experience buffer\n",
    "    if num_pre_training_steps < opponent.batch_size * opponent.num_frames:\n",
    "        num_pre_training_steps = opponent.batch_size * opponent.num_frames\n",
    "        \n",
    "    # Loop vars\n",
    "    num_training_steps = n_iter\n",
    "    num_steps = num_pre_training_steps + num_training_steps\n",
    "    step = 0\n",
    "    alive_steps = 0\n",
    "    total_reward = 0\n",
    "    done = True\n",
    "    print(f\"Total number of steps: {num_steps}\")\n",
    "\n",
    "    # Create file system related vars\n",
    "    logpath = os.path.join(log_path, opponent.name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    modelpath = os.path.join(save_path, opponent.name + \".h5\")\n",
    "    opponent.tf_writer = tf.summary.create_file_writer(logpath, name=opponent.name)\n",
    "    opponent._save_hyperparameters(save_path, env, num_steps)\n",
    "    \n",
    "    while step < num_steps:\n",
    "        # Reset environment and agent\n",
    "        if done:\n",
    "            new_obs = env.reset() # This shouldn't raise\n",
    "            opponent.reset(new_obs)\n",
    "            done = False\n",
    "            # use random chronics so that the opponent can learn more scenarios\n",
    "            max_day = (env.chronics_handler.max_timestep() - MAX_TIMESTEP) // 288\n",
    "            start_timestep = np.random.randint(max_day) * 288 - 1\n",
    "            if start_timestep > 0:\n",
    "                env.fast_forward_chronics(start_timestep)\n",
    "            \n",
    "#         if cfg.VERBOSE and step % 1000 == 0:\n",
    "#             print(\"Step [{}] -- Random [{}]\".format(step, opponent.epsilon))\n",
    "\n",
    "        # Save current observation to stacking buffer\n",
    "        opponent._save_current_frame(opponent.state)\n",
    "\n",
    "        # Execute attack if allowed\n",
    "        if step <= num_pre_training_steps:\n",
    "            opponent.remaining_time = 0\n",
    "            attack, a = opponent._do_nothing, 0\n",
    "        else:\n",
    "            attack, a = opponent.attack(new_obs)\n",
    "\n",
    "        if step > num_pre_training_steps:\n",
    "#             print(f'ATTACK step {step}: disconnected {a}')\n",
    "            attack_obs, opp_reward, done, info = env.step(attack)\n",
    "            if info[\"is_illegal\"] or info[\"is_ambiguous\"]:\n",
    "                if cfg.VERBOSE:\n",
    "                    print(attack, info)\n",
    "            new_obs = attack_obs\n",
    "            opponent.tell_attack_continues(None, None, None, None)\n",
    "\n",
    "        while opponent.remaining_time >= 0 and not done:\n",
    "            new_obs.time_before_cooldown_line[opponent.attack_line] = opponent.remaining_time\n",
    "            response = agent.act(new_obs, None, None)\n",
    "            new_obs, reward, done, info = env.step(response)\n",
    "            if info[\"is_illegal\"] or info[\"is_ambiguous\"]:\n",
    "                if cfg.VERBOSE:\n",
    "                    print(attack, info)\n",
    "            opponent.remaining_time -= 1\n",
    "            total_reward += reward\n",
    "            alive_steps += 1\n",
    "        \n",
    "        # Save new observation to stacking buffer\n",
    "        new_state = opponent.convert_obs(new_obs)\n",
    "        opponent._save_next_frame(new_state)\n",
    "#         print('reward')\n",
    "#         print(reward)\n",
    "#         print('--------------------------------')\n",
    "\n",
    "        # Save to experience buffer\n",
    "        if len(opponent.frames2) == opponent.num_frames:\n",
    "            opponent.per_buffer.add(np.array(opponent.frames),\n",
    "                                a, -1 * reward + 1500,\n",
    "                                np.array(opponent.frames2),\n",
    "                                opponent.done)\n",
    "\n",
    "\n",
    "        # Perform training when we have enough experience in buffer\n",
    "        if step >= num_pre_training_steps:\n",
    "            training_step = step - num_pre_training_steps\n",
    "            # Decay chance of random action\n",
    "            opponent.epsilon = opponent._adaptive_epsilon_decay(training_step)\n",
    "\n",
    "            # Perform training at given frequency\n",
    "            if step % cfg.UPDATE_FREQ == 0 and \\\n",
    "               len(opponent.per_buffer) >= opponent.batch_size:\n",
    "                # Perform training\n",
    "                opponent._batch_train(training_step, step)\n",
    "\n",
    "                if cfg.UPDATE_TARGET_SOFT_TAU > 0.0:\n",
    "                    tau = cfg.UPDATE_TARGET_SOFT_TAU\n",
    "                    # Update target network towards primary network\n",
    "                    opponent.policy_net.update_target_soft(opponent.target_net.model, tau)\n",
    "\n",
    "            # Every UPDATE_TARGET_HARD_FREQ trainings, update target completely\n",
    "            if cfg.UPDATE_TARGET_HARD_FREQ > 0 and \\\n",
    "               step % (cfg.UPDATE_FREQ * cfg.UPDATE_TARGET_HARD_FREQ) == 0:\n",
    "                opponent.policy_net.update_target_hard(opponent.target_net.model)\n",
    "        \n",
    "        if done:\n",
    "            opponent.epoch_rewards.append(-1 * total_reward)\n",
    "            opponent.epoch_alive.append(alive_steps)\n",
    "            if cfg.VERBOSE and step > num_pre_training_steps:\n",
    "                print(\"step {}: Agent survived [{}] steps with reward {}\".format(step, alive_steps, total_reward))\n",
    "            alive_steps = 0\n",
    "            total_reward = 0         \n",
    "        else:\n",
    "            alive_steps += 1\n",
    "            \n",
    "        ######## After Each Step #######\n",
    "        if step > 0 and step % 2000 == 0: # save network every 5000 iters\n",
    "            opponent.save(modelpath)\n",
    "        step += 1\n",
    "        # Make new obs the current obs\n",
    "        opponent.obs = new_obs\n",
    "        opponent.state = new_state\n",
    "\n",
    "    # Save model after all steps\n",
    "    opponent.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reasonable-synthesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05-16 19:18:03 MainThread @machine_info.py:91]\u001b[0m Cannot find available GPU devices, using CPU or other devices now.\n",
      "\u001b[32m[05-16 19:18:03 MainThread @machine_info.py:91]\u001b[0m Cannot find available GPU devices, using CPU or other devices now.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "backend = LightSimBackend()\n",
    "env = grid2op.make(\"l2rpn_neurips_2020_track2_small\", backend=backend)\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "agent = Track2PowerNetAgent(env.action_space)\n",
    "\n",
    "# Opponent \n",
    "opponent_name = \"D3QN_PARL\"\n",
    "num_pre_training_steps = 256\n",
    "learning_rate = 5e-5\n",
    "initial_epsilon = 0.99\n",
    "final_epsilon = 0.01\n",
    "decay_epsilon = 2000\n",
    "attack_period = 50\n",
    "attack_duration = 20\n",
    "opponent = D3QN_Opponent(env.action_space, env.observation_space, lines_to_attack=LINES, attack_period=attack_period,\n",
    "            attack_duration=attack_duration, name=opponent_name, is_training=True, learning_rate=learning_rate,\n",
    "            initial_epsilon=initial_epsilon, final_epsilon=final_epsilon, decay_epsilon=decay_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worldwide-coordinate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps: 2256\n",
      "step 273: Agent survived [866] steps with reward 496602.95654296875\n",
      "step 279: Agent survived [68] steps with reward 66895.93841552734\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30ca6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e09a50378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "loss = 256362.62\n",
      "step 288: Agent survived [137] steps with reward 125195.50421142578\n",
      "step 299: Agent survived [178] steps with reward 189948.88745117188\n",
      "step 307: Agent survived [112] steps with reward 96100.87237548828\n",
      "loss = 1604.537\n",
      "step 314: Agent survived [90] steps with reward 81372.06658935547\n",
      "step 324: Agent survived [153] steps with reward 148983.3692626953\n",
      "loss = 646.7311\n",
      "step 342: Agent survived [335] steps with reward 365619.17755126953\n",
      "step 347: Agent survived [46] steps with reward 50465.35925292969\n",
      "step 355: Agent survived [112] steps with reward 85633.00207519531\n",
      "loss = 879.93115\n",
      "step 372: Agent survived [310] steps with reward 263440.87322998047\n",
      "step 378: Agent survived [68] steps with reward 63721.013244628906\n",
      "step 384: Agent survived [68] steps with reward 69057.42358398438\n",
      "step 388: Agent survived [24] steps with reward 23017.680297851562\n",
      "loss = 987.9646\n",
      "step 393: Agent survived [46] steps with reward 34802.80078125\n",
      "step 401: Agent survived [112] steps with reward 101382.8373413086\n",
      "step 405: Agent survived [24] steps with reward 22077.562133789062\n",
      "step 415: Agent survived [156] steps with reward 157560.51385498047\n",
      "loss = 679.12634\n",
      "step 423: Agent survived [109] steps with reward 117449.80352783203\n",
      "step 433: Agent survived [159] steps with reward 111215.57995605469\n",
      "step 447: Agent survived [244] steps with reward 189351.5535888672\n",
      "loss = 387.25024\n",
      "step 452: Agent survived [43] steps with reward 39409.759216308594\n",
      "step 463: Agent survived [181] steps with reward 163366.22595214844\n",
      "step 468: Agent survived [46] steps with reward 44596.154235839844\n",
      "step 473: Agent survived [46] steps with reward 34296.86212158203\n",
      "loss = 763.5092\n",
      "step 477: Agent survived [24] steps with reward 17880.21905517578\n",
      "step 484: Agent survived [90] steps with reward 86825.01208496094\n",
      "step 489: Agent survived [59] steps with reward 64830.1103515625\n",
      "step 493: Agent survived [11] steps with reward 7824.0604248046875\n",
      "step 503: Agent survived [153] steps with reward 136924.43359375\n",
      "loss = 891.35095\n",
      "step 507: Agent survived [27] steps with reward 20898.883361816406\n",
      "step 513: Agent survived [68] steps with reward 60758.36535644531\n",
      "step 517: Agent survived [24] steps with reward 21452.521362304688\n",
      "step 521: Agent survived [24] steps with reward 21620.611877441406\n",
      "step 527: Agent survived [68] steps with reward 49977.986267089844\n",
      "loss = 646.12225\n",
      "step 532: Agent survived [46] steps with reward 40673.98065185547\n",
      "step 536: Agent survived [24] steps with reward 19442.697265625\n",
      "step 540: Agent survived [24] steps with reward 21404.832458496094\n",
      "step 544: Agent survived [24] steps with reward 22173.789306640625\n",
      "step 549: Agent survived [46] steps with reward 34754.30157470703\n",
      "step 557: Agent survived [109] steps with reward 90459.16162109375\n",
      "loss = 425.62427\n",
      "step 562: Agent survived [49] steps with reward 40938.84631347656\n",
      "step 566: Agent survived [24] steps with reward 19875.26483154297\n",
      "step 570: Agent survived [24] steps with reward 20919.05712890625\n",
      "step 578: Agent survived [112] steps with reward 89413.21545410156\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - force disconnection of 1 powerlines ([111])\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('Powerline with ids [111] have been modified illegally (cooldown)',)], 'rewards': {}}\n",
      "loss = 703.0393\n",
      "step 590: Agent survived [200] steps with reward 163200.24395751953\n",
      "step 597: Agent survived [90] steps with reward 97566.98071289062\n",
      "step 605: Agent survived [128] steps with reward 157877.60131835938\n",
      "step 609: Agent survived [8] steps with reward 4528.079833984375\n",
      "loss = 551.207\n",
      "step 618: Agent survived [134] steps with reward 109977.35711669922\n",
      "step 626: Agent survived [109] steps with reward 96794.46252441406\n",
      "step 630: Agent survived [24] steps with reward 21804.22869873047\n",
      "step 634: Agent survived [27] steps with reward 25487.227783203125\n",
      "step 640: Agent survived [68] steps with reward 83649.58630371094\n",
      "loss = 882.2572\n",
      "step 646: Agent survived [68] steps with reward 62398.117126464844\n",
      "step 652: Agent survived [68] steps with reward 73372.44537353516\n",
      "step 658: Agent survived [68] steps with reward 68818.93634033203\n",
      "step 667: Agent survived [131] steps with reward 169554.8829345703\n",
      "loss = 822.90594\n",
      "step 672: Agent survived [49] steps with reward 56440.36071777344\n",
      "step 677: Agent survived [46] steps with reward 39007.472106933594\n",
      "step 685: Agent survived [112] steps with reward 102713.8267211914\n",
      "step 689: Agent survived [24] steps with reward 21277.52001953125\n",
      "step 693: Agent survived [24] steps with reward 21932.709716796875\n",
      "step 699: Agent survived [68] steps with reward 80793.81042480469\n",
      "loss = 721.92804\n",
      "step 703: Agent survived [24] steps with reward 18493.17059326172\n",
      "step 707: Agent survived [24] steps with reward 20029.608459472656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 711: Agent survived [24] steps with reward 20012.62823486328\n",
      "step 715: Agent survived [24] steps with reward 21527.01348876953\n",
      "step 720: Agent survived [46] steps with reward 50893.91125488281\n",
      "step 724: Agent survived [21] steps with reward 14774.512573242188\n",
      "loss = 1253.0491\n",
      "step 729: Agent survived [49] steps with reward 43392.38171386719\n",
      "step 737: Agent survived [112] steps with reward 118638.74725341797\n",
      "step 741: Agent survived [24] steps with reward 22588.123779296875\n",
      "step 746: Agent survived [46] steps with reward 51702.46875\n",
      "step 750: Agent survived [24] steps with reward 19174.23065185547\n",
      "step 755: Agent survived [46] steps with reward 39230.32800292969\n",
      "loss = 1261.2957\n",
      "step 761: Agent survived [68] steps with reward 66745.62072753906\n",
      "step 765: Agent survived [24] steps with reward 22589.52294921875\n",
      "step 770: Agent survived [46] steps with reward 54404.354736328125\n",
      "step 774: Agent survived [24] steps with reward 17852.079345703125\n",
      "step 778: Agent survived [24] steps with reward 18266.938415527344\n",
      "step 782: Agent survived [24] steps with reward 21022.00518798828\n",
      "loss = 991.2805\n",
      "step 787: Agent survived [46] steps with reward 46389.79919433594\n",
      "step 791: Agent survived [24] steps with reward 24135.455200195312\n",
      "step 795: Agent survived [24] steps with reward 18811.88494873047\n",
      "step 802: Agent survived [87] steps with reward 84352.53729248047\n",
      "step 807: Agent survived [49] steps with reward 53242.80383300781\n",
      "loss = 1701.8243\n",
      "step 815: Agent survived [113] steps with reward 151136.57592773438\n",
      "step 819: Agent survived [23] steps with reward 23059.85595703125\n",
      "step 823: Agent survived [24] steps with reward 18530.615783691406\n",
      "step 827: Agent survived [24] steps with reward 20020.500366210938\n",
      "step 831: Agent survived [24] steps with reward 21306.803649902344\n",
      "step 835: Agent survived [24] steps with reward 22752.764770507812\n",
      "loss = 2391.364\n",
      "step 846: Agent survived [178] steps with reward 219211.84619140625\n",
      "step 850: Agent survived [24] steps with reward 18728.305541992188\n",
      "step 854: Agent survived [24] steps with reward 13403.698852539062\n",
      "step 859: Agent survived [46] steps with reward 29664.551818847656\n",
      "step 864: Agent survived [46] steps with reward 32247.656372070312\n",
      "loss = 2324.8115\n",
      "step 869: Agent survived [46] steps with reward 34125.48400878906\n",
      "step 874: Agent survived [46] steps with reward 24321.132934570312\n",
      "step 880: Agent survived [68] steps with reward 42051.14990234375\n",
      "step 884: Agent survived [24] steps with reward 13815.874755859375\n",
      "step 889: Agent survived [46] steps with reward 31641.18292236328\n",
      "step 890: Agent survived [4] steps with reward 2381.9735107421875\n",
      "loss = 880.7682\n",
      "step 896: Agent survived [64] steps with reward 38637.339904785156\n",
      "step 901: Agent survived [46] steps with reward 27493.559814453125\n",
      "step 905: Agent survived [24] steps with reward 14217.605285644531\n",
      "step 909: Agent survived [24] steps with reward 15005.670349121094\n",
      "step 913: Agent survived [24] steps with reward 15478.006225585938\n",
      "step 918: Agent survived [46] steps with reward 24261.380310058594\n",
      "step 922: Agent survived [24] steps with reward 13040.852294921875\n",
      "loss = 659.8054\n",
      "step 927: Agent survived [46] steps with reward 29632.370971679688\n",
      "step 932: Agent survived [61] steps with reward 45560.692321777344\n",
      "step 937: Agent survived [31] steps with reward 20719.421875\n",
      "step 941: Agent survived [24] steps with reward 11664.469909667969\n",
      "step 945: Agent survived [24] steps with reward 12782.507019042969\n",
      "step 949: Agent survived [24] steps with reward 14439.243408203125\n",
      "loss = 1167.4531\n",
      "step 953: Agent survived [21] steps with reward 13605.478149414062\n",
      "step 957: Agent survived [27] steps with reward 19166.68505859375\n",
      "step 962: Agent survived [46] steps with reward 26202.382568359375\n",
      "step 967: Agent survived [46] steps with reward 28088.988159179688\n",
      "step 972: Agent survived [46] steps with reward 30340.695922851562\n",
      "step 978: Agent survived [68] steps with reward 52360.64392089844\n",
      "loss = 1041.9913\n",
      "step 982: Agent survived [24] steps with reward 16380.0146484375\n",
      "step 986: Agent survived [24] steps with reward 11342.255249023438\n",
      "step 990: Agent survived [24] steps with reward 12589.328552246094\n",
      "step 995: Agent survived [46] steps with reward 29010.0205078125\n",
      "step 999: Agent survived [24] steps with reward 15552.037536621094\n",
      "step 1003: Agent survived [21] steps with reward 14893.174255371094\n",
      "step 1007: Agent survived [27] steps with reward 13136.786254882812\n",
      "loss = 775.2629\n",
      "step 1011: Agent survived [24] steps with reward 12616.651550292969\n",
      "step 1017: Agent survived [68] steps with reward 48996.896240234375\n",
      "step 1021: Agent survived [24] steps with reward 15229.159973144531\n",
      "step 1025: Agent survived [24] steps with reward 16209.816833496094\n",
      "step 1032: Agent survived [90] steps with reward 50748.03454589844\n",
      "loss = 653.3502\n",
      "step 1039: Agent survived [90] steps with reward 55316.10076904297\n",
      "step 1045: Agent survived [68] steps with reward 46126.85430908203\n",
      "step 1049: Agent survived [21] steps with reward 13641.45654296875\n",
      "step 1050: Agent survived [5] steps with reward 3135.5352783203125\n",
      "step 1054: Agent survived [22] steps with reward 10343.704956054688\n",
      "step 1058: Agent survived [24] steps with reward 13271.239135742188\n",
      "step 1062: Agent survived [24] steps with reward 14040.071472167969\n",
      "loss = 1079.5181\n",
      "step 1067: Agent survived [43] steps with reward 31906.5458984375\n",
      "step 1071: Agent survived [27] steps with reward 17968.96630859375\n",
      "step 1076: Agent survived [46] steps with reward 24246.779663085938\n",
      "step 1080: Agent survived [24] steps with reward 13783.479125976562\n",
      "step 1084: Agent survived [24] steps with reward 15594.265502929688\n",
      "step 1088: Agent survived [24] steps with reward 16721.573364257812\n",
      "loss = 776.686\n",
      "step 1092: Agent survived [24] steps with reward 17607.309936523438\n",
      "step 1096: Agent survived [24] steps with reward 12217.619201660156\n",
      "step 1100: Agent survived [24] steps with reward 13942.331970214844\n",
      "step 1104: Agent survived [21] steps with reward 13397.275024414062\n",
      "step 1108: Agent survived [27] steps with reward 20031.00634765625\n",
      "step 1112: Agent survived [24] steps with reward 18107.80108642578\n",
      "step 1116: Agent survived [24] steps with reward 12724.825561523438\n",
      "loss = 822.29395\n",
      "step 1120: Agent survived [24] steps with reward 13295.209289550781\n",
      "step 1124: Agent survived [24] steps with reward 15148.431030273438\n",
      "step 1131: Agent survived [90] steps with reward 75853.79418945312\n",
      "step 1136: Agent survived [46] steps with reward 37091.212951660156\n",
      "step 1141: Agent survived [46] steps with reward 26974.99981689453\n",
      "step 1145: Agent survived [24] steps with reward 13707.903747558594\n",
      "loss = 533.9407\n",
      "step 1150: Agent survived [46] steps with reward 30385.733154296875\n",
      "step 1154: Agent survived [24] steps with reward 16184.727722167969\n",
      "step 1159: Agent survived [46] steps with reward 37134.377868652344\n",
      "step 1163: Agent survived [24] steps with reward 11866.897766113281\n",
      "step 1168: Agent survived [46] steps with reward 30222.20233154297\n",
      "step 1172: Agent survived [24] steps with reward 14880.46484375\n",
      "loss = 795.5046\n",
      "step 1176: Agent survived [24] steps with reward 17099.477905273438\n",
      "step 1181: Agent survived [46] steps with reward 37058.76123046875\n",
      "step 1185: Agent survived [24] steps with reward 12464.698364257812\n",
      "step 1190: Agent survived [46] steps with reward 28843.509826660156\n",
      "step 1195: Agent survived [46] steps with reward 32393.34686279297\n",
      "step 1199: Agent survived [24] steps with reward 15739.0986328125\n",
      "step 1203: Agent survived [24] steps with reward 17305.494140625\n",
      "loss = 352.37656\n",
      "step 1207: Agent survived [24] steps with reward 12038.759643554688\n",
      "step 1211: Agent survived [24] steps with reward 13529.026306152344\n",
      "step 1219: Agent survived [112] steps with reward 87478.96667480469\n",
      "step 1223: Agent survived [24] steps with reward 16745.134155273438\n",
      "step 1227: Agent survived [24] steps with reward 17163.101440429688\n",
      "step 1231: Agent survived [24] steps with reward 12172.372131347656\n",
      "loss = 716.9679\n",
      "step 1235: Agent survived [24] steps with reward 13850.5224609375\n",
      "step 1239: Agent survived [24] steps with reward 14163.806701660156\n",
      "step 1243: Agent survived [24] steps with reward 16620.13983154297\n",
      "step 1247: Agent survived [24] steps with reward 17148.72332763672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1251: Agent survived [24] steps with reward 12686.731018066406\n",
      "step 1255: Agent survived [24] steps with reward 13277.059692382812\n",
      "step 1259: Agent survived [24] steps with reward 14984.247436523438\n",
      "loss = 100.82916\n",
      "step 1263: Agent survived [24] steps with reward 15719.5498046875\n",
      "step 1267: Agent survived [24] steps with reward 16857.93133544922\n",
      "step 1271: Agent survived [24] steps with reward 13158.110900878906\n",
      "step 1275: Agent survived [24] steps with reward 13338.95361328125\n",
      "step 1279: Agent survived [24] steps with reward 15623.898498535156\n",
      "step 1283: Agent survived [24] steps with reward 16671.721740722656\n",
      "step 1287: Agent survived [26] steps with reward 19715.890502929688\n",
      "loss = 81.36693\n",
      "step 1291: Agent survived [22] steps with reward 11296.850341796875\n",
      "step 1295: Agent survived [24] steps with reward 12744.759033203125\n",
      "step 1299: Agent survived [24] steps with reward 13724.693542480469\n",
      "step 1303: Agent survived [24] steps with reward 15645.008056640625\n",
      "step 1307: Agent survived [24] steps with reward 17204.511291503906\n",
      "step 1311: Agent survived [24] steps with reward 11732.862548828125\n",
      "step 1315: Agent survived [24] steps with reward 12406.768798828125\n",
      "loss = 103.51324\n",
      "step 1319: Agent survived [24] steps with reward 13922.337768554688\n",
      "step 1323: Agent survived [24] steps with reward 15464.540100097656\n",
      "step 1327: Agent survived [24] steps with reward 16321.5625\n",
      "step 1331: Agent survived [24] steps with reward 11508.876586914062\n",
      "step 1335: Agent survived [24] steps with reward 12600.788452148438\n",
      "step 1339: Agent survived [24] steps with reward 14896.946533203125\n",
      "step 1343: Agent survived [24] steps with reward 14860.135131835938\n",
      "loss = 177.30792\n",
      "step 1347: Agent survived [24] steps with reward 15363.840881347656\n",
      "step 1351: Agent survived [24] steps with reward 12371.083251953125\n",
      "step 1355: Agent survived [24] steps with reward 12858.251831054688\n",
      "step 1359: Agent survived [24] steps with reward 13137.879272460938\n",
      "step 1363: Agent survived [24] steps with reward 14714.212219238281\n",
      "step 1368: Agent survived [46] steps with reward 36710.729736328125\n",
      "loss = 150.79382\n",
      "step 1372: Agent survived [24] steps with reward 11717.047729492188\n",
      "step 1376: Agent survived [24] steps with reward 12395.394897460938\n",
      "step 1380: Agent survived [24] steps with reward 13535.07763671875\n",
      "step 1384: Agent survived [24] steps with reward 14629.521484375\n",
      "step 1388: Agent survived [24] steps with reward 15783.440002441406\n",
      "step 1393: Agent survived [46] steps with reward 25062.91424560547\n",
      "step 1397: Agent survived [24] steps with reward 13573.62158203125\n",
      "loss = 120.92292\n",
      "step 1401: Agent survived [24] steps with reward 14858.17041015625\n",
      "step 1405: Agent survived [24] steps with reward 15272.426452636719\n",
      "step 1409: Agent survived [24] steps with reward 16843.381469726562\n",
      "step 1413: Agent survived [24] steps with reward 11640.299926757812\n",
      "step 1417: Agent survived [24] steps with reward 12922.922058105469\n",
      "step 1421: Agent survived [24] steps with reward 14074.356750488281\n",
      "step 1425: Agent survived [24] steps with reward 16021.895446777344\n",
      "loss = 113.031815\n",
      "step 1429: Agent survived [24] steps with reward 15785.253112792969\n",
      "step 1433: Agent survived [24] steps with reward 12495.136047363281\n",
      "step 1437: Agent survived [24] steps with reward 12691.212585449219\n",
      "step 1442: Agent survived [46] steps with reward 30179.67041015625\n",
      "step 1447: Agent survived [46] steps with reward 32351.229370117188\n",
      "step 1451: Agent survived [24] steps with reward 15734.602905273438\n",
      "step 1455: Agent survived [24] steps with reward 12370.688293457031\n",
      "loss = 90.14673\n",
      "step 1459: Agent survived [24] steps with reward 12413.071533203125\n",
      "step 1463: Agent survived [24] steps with reward 13908.241638183594\n",
      "step 1467: Agent survived [24] steps with reward 15040.263671875\n",
      "step 1471: Agent survived [24] steps with reward 16796.34051513672\n",
      "step 1476: Agent survived [46] steps with reward 24396.14666748047\n",
      "step 1481: Agent survived [46] steps with reward 27013.172973632812\n",
      "loss = 96.677345\n",
      "step 1485: Agent survived [24] steps with reward 14472.008972167969\n",
      "step 1489: Agent survived [24] steps with reward 14834.233825683594\n",
      "step 1493: Agent survived [24] steps with reward 17095.945739746094\n",
      "step 1498: Agent survived [46] steps with reward 26374.686279296875\n",
      "step 1502: Agent survived [24] steps with reward 21408.544982910156\n",
      "step 1507: Agent survived [46] steps with reward 50482.94140625\n",
      "loss = 121.81742\n",
      "step 1512: Agent survived [46] steps with reward 51390.88757324219\n",
      "step 1516: Agent survived [24] steps with reward 26484.268432617188\n",
      "step 1520: Agent survived [24] steps with reward 20077.080932617188\n",
      "step 1524: Agent survived [24] steps with reward 20785.19580078125\n",
      "step 1528: Agent survived [24] steps with reward 24193.085327148438\n",
      "step 1532: Agent survived [24] steps with reward 25627.580078125\n",
      "step 1536: Agent survived [24] steps with reward 27320.718505859375\n",
      "loss = 213.38176\n",
      "step 1540: Agent survived [24] steps with reward 18926.861206054688\n",
      "step 1544: Agent survived [24] steps with reward 23214.827880859375\n",
      "step 1548: Agent survived [24] steps with reward 24151.398071289062\n",
      "step 1552: Agent survived [24] steps with reward 25450.76708984375\n",
      "step 1556: Agent survived [24] steps with reward 26468.903564453125\n",
      "step 1560: Agent survived [24] steps with reward 19112.183959960938\n",
      "step 1565: Agent survived [46] steps with reward 44191.78063964844\n",
      "loss = 327.71088\n",
      "step 1569: Agent survived [24] steps with reward 23059.718994140625\n",
      "step 1573: Agent survived [24] steps with reward 25943.01513671875\n",
      "step 1577: Agent survived [24] steps with reward 27620.720092773438\n",
      "step 1581: Agent survived [24] steps with reward 19453.38134765625\n",
      "step 1585: Agent survived [24] steps with reward 22290.3251953125\n",
      "step 1589: Agent survived [24] steps with reward 25364.083862304688\n",
      "step 1593: Agent survived [24] steps with reward 24952.65380859375\n",
      "loss = 191.81198\n",
      "step 1597: Agent survived [24] steps with reward 28406.872314453125\n",
      "step 1601: Agent survived [24] steps with reward 20047.25311279297\n",
      "step 1605: Agent survived [24] steps with reward 23026.505249023438\n",
      "step 1610: Agent survived [46] steps with reward 51843.48669433594\n",
      "step 1614: Agent survived [24] steps with reward 26103.046264648438\n",
      "step 1619: Agent survived [46] steps with reward 56183.357421875\n",
      "step 1623: Agent survived [24] steps with reward 19363.351623535156\n",
      "loss = 159.56406\n",
      "step 1627: Agent survived [24] steps with reward 22330.587646484375\n",
      "step 1631: Agent survived [24] steps with reward 25530.13037109375\n",
      "step 1635: Agent survived [24] steps with reward 26787.839721679688\n",
      "step 1639: Agent survived [24] steps with reward 27645.490966796875\n",
      "step 1643: Agent survived [24] steps with reward 20742.34130859375\n",
      "step 1647: Agent survived [24] steps with reward 22356.283813476562\n",
      "step 1651: Agent survived [24] steps with reward 24419.64990234375\n",
      "loss = 112.86035\n",
      "step 1655: Agent survived [24] steps with reward 26601.938354492188\n",
      "step 1659: Agent survived [24] steps with reward 27706.24462890625\n",
      "step 1663: Agent survived [24] steps with reward 19360.135375976562\n",
      "step 1667: Agent survived [24] steps with reward 20811.22772216797\n",
      "step 1671: Agent survived [24] steps with reward 22961.077026367188\n",
      "step 1675: Agent survived [24] steps with reward 26536.643188476562\n",
      "step 1679: Agent survived [24] steps with reward 25753.148315429688\n",
      "loss = 383.56958\n",
      "step 1683: Agent survived [24] steps with reward 19668.020141601562\n",
      "step 1687: Agent survived [24] steps with reward 22424.578247070312\n",
      "step 1692: Agent survived [46] steps with reward 49024.46203613281\n",
      "step 1696: Agent survived [24] steps with reward 25937.268310546875\n",
      "step 1700: Agent survived [24] steps with reward 27170.373413085938\n",
      "step 1704: Agent survived [24] steps with reward 20421.795166015625\n",
      "loss = 103.10916\n",
      "step 1708: Agent survived [24] steps with reward 21938.083129882812\n",
      "step 1712: Agent survived [24] steps with reward 24184.927856445312\n",
      "step 1716: Agent survived [24] steps with reward 27461.501953125\n",
      "step 1720: Agent survived [24] steps with reward 28283.997924804688\n",
      "step 1724: Agent survived [24] steps with reward 19047.392639160156\n",
      "step 1728: Agent survived [24] steps with reward 22576.607788085938\n",
      "step 1732: Agent survived [24] steps with reward 25195.668334960938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 170.04945\n",
      "step 1736: Agent survived [24] steps with reward 25703.790161132812\n",
      "step 1740: Agent survived [24] steps with reward 26788.47216796875\n",
      "step 1744: Agent survived [24] steps with reward 20656.955993652344\n",
      "step 1748: Agent survived [24] steps with reward 22505.219604492188\n",
      "step 1752: Agent survived [24] steps with reward 25125.5\n",
      "step 1756: Agent survived [24] steps with reward 25326.613403320312\n",
      "step 1760: Agent survived [24] steps with reward 28522.077514648438\n",
      "loss = 202.56404\n",
      "step 1764: Agent survived [24] steps with reward 19333.202026367188\n",
      "step 1768: Agent survived [24] steps with reward 23228.143920898438\n",
      "step 1772: Agent survived [24] steps with reward 24917.978271484375\n",
      "step 1776: Agent survived [24] steps with reward 26569.139892578125\n",
      "step 1780: Agent survived [24] steps with reward 27542.306884765625\n",
      "step 1784: Agent survived [24] steps with reward 19505.615783691406\n",
      "step 1788: Agent survived [24] steps with reward 22176.5830078125\n",
      "loss = 156.6152\n",
      "step 1792: Agent survived [24] steps with reward 23135.487670898438\n",
      "step 1796: Agent survived [24] steps with reward 26916.967895507812\n",
      "step 1801: Agent survived [46] steps with reward 61718.788330078125\n",
      "step 1806: Agent survived [46] steps with reward 41037.12561035156\n",
      "step 1810: Agent survived [24] steps with reward 22604.069580078125\n",
      "step 1814: Agent survived [24] steps with reward 23911.163208007812\n",
      "step 1818: Agent survived [24] steps with reward 25770.040649414062\n",
      "loss = 151.90025\n",
      "step 1822: Agent survived [24] steps with reward 27373.328002929688\n",
      "step 1826: Agent survived [24] steps with reward 20444.859375\n",
      "step 1830: Agent survived [24] steps with reward 21290.434997558594\n",
      "step 1834: Agent survived [24] steps with reward 23838.272583007812\n",
      "step 1838: Agent survived [24] steps with reward 24999.047973632812\n",
      "step 1842: Agent survived [24] steps with reward 27452.20654296875\n",
      "step 1846: Agent survived [24] steps with reward 19650.210205078125\n",
      "loss = 159.5853\n",
      "step 1850: Agent survived [24] steps with reward 21723.028076171875\n",
      "step 1854: Agent survived [24] steps with reward 23945.310791015625\n",
      "step 1858: Agent survived [24] steps with reward 27408.638916015625\n",
      "step 1862: Agent survived [24] steps with reward 27171.649047851562\n",
      "step 1866: Agent survived [24] steps with reward 19764.155334472656\n",
      "step 1870: Agent survived [24] steps with reward 21022.70098876953\n",
      "step 1874: Agent survived [24] steps with reward 23852.254516601562\n",
      "loss = 125.2953\n",
      "step 1878: Agent survived [24] steps with reward 24886.6533203125\n",
      "step 1882: Agent survived [24] steps with reward 28162.32470703125\n",
      "step 1888: Agent survived [68] steps with reward 64562.37854003906\n",
      "step 1892: Agent survived [24] steps with reward 21565.69610595703\n",
      "step 1896: Agent survived [24] steps with reward 24722.326904296875\n",
      "step 1900: Agent survived [24] steps with reward 25906.239624023438\n",
      "loss = 134.18692\n",
      "step 1904: Agent survived [24] steps with reward 28315.003051757812\n",
      "step 1908: Agent survived [24] steps with reward 20311.396240234375\n",
      "step 1913: Agent survived [46] steps with reward 33565.63153076172\n",
      "step 1918: Agent survived [46] steps with reward 35323.55236816406\n",
      "step 1922: Agent survived [24] steps with reward 17581.44451904297\n",
      "step 1926: Agent survived [24] steps with reward 19212.707092285156\n",
      "step 1930: Agent survived [24] steps with reward 13542.1953125\n",
      "loss = 165.98648\n",
      "step 1934: Agent survived [24] steps with reward 15208.465026855469\n",
      "step 1938: Agent survived [24] steps with reward 16052.651611328125\n",
      "step 1942: Agent survived [24] steps with reward 17317.929931640625\n",
      "step 1946: Agent survived [24] steps with reward 20015.276306152344\n",
      "step 1950: Agent survived [24] steps with reward 13588.88623046875\n",
      "step 1954: Agent survived [24] steps with reward 15726.418579101562\n",
      "step 1959: Agent survived [46] steps with reward 36188.00671386719\n",
      "loss = 107.033775\n",
      "step 1963: Agent survived [24] steps with reward 17140.114196777344\n",
      "step 1967: Agent survived [24] steps with reward 20675.064819335938\n",
      "step 1971: Agent survived [24] steps with reward 13656.656921386719\n",
      "step 1976: Agent survived [46] steps with reward 34833.27117919922\n",
      "step 1980: Agent survived [24] steps with reward 16553.568969726562\n",
      "step 1985: Agent survived [46] steps with reward 39360.83068847656\n",
      "loss = 241.9578\n",
      "step 1989: Agent survived [24] steps with reward 19673.543884277344\n",
      "step 1993: Agent survived [24] steps with reward 14093.591064453125\n",
      "step 1997: Agent survived [24] steps with reward 15278.436401367188\n",
      "Successfully saved model at: PARL_opp_D3QN_shifted_reward_2000_atk_period_50_atk_duration_20/D3QN_PARL.h5\n",
      "step 2001: Agent survived [24] steps with reward 16609.630615234375\n",
      "step 2006: Agent survived [46] steps with reward 38774.460388183594\n",
      "step 2010: Agent survived [24] steps with reward 18569.004150390625\n",
      "step 2014: Agent survived [24] steps with reward 13517.6650390625\n",
      "loss = 127.53909\n",
      "step 2018: Agent survived [24] steps with reward 15723.232727050781\n",
      "step 2022: Agent survived [24] steps with reward 16347.205810546875\n",
      "step 2026: Agent survived [24] steps with reward 18767.838806152344\n",
      "step 2030: Agent survived [24] steps with reward 19626.06982421875\n",
      "step 2034: Agent survived [24] steps with reward 14171.267150878906\n",
      "step 2038: Agent survived [24] steps with reward 15564.105651855469\n",
      "step 2042: Agent survived [24] steps with reward 17195.51348876953\n",
      "loss = 197.1831\n",
      "step 2046: Agent survived [24] steps with reward 17040.07958984375\n",
      "step 2050: Agent survived [24] steps with reward 18739.026123046875\n",
      "step 2054: Agent survived [24] steps with reward 14027.347351074219\n",
      "step 2058: Agent survived [24] steps with reward 14976.588256835938\n",
      "step 2062: Agent survived [24] steps with reward 16279.844909667969\n",
      "step 2066: Agent survived [24] steps with reward 18168.84307861328\n",
      "step 2070: Agent survived [24] steps with reward 19258.304626464844\n",
      "loss = 133.24246\n",
      "step 2074: Agent survived [24] steps with reward 13775.528442382812\n",
      "step 2078: Agent survived [24] steps with reward 14674.763793945312\n",
      "step 2082: Agent survived [24] steps with reward 16968.697509765625\n",
      "step 2086: Agent survived [24] steps with reward 18261.491577148438\n",
      "step 2090: Agent survived [24] steps with reward 19723.817260742188\n",
      "step 2094: Agent survived [24] steps with reward 13522.413269042969\n",
      "step 2098: Agent survived [24] steps with reward 15029.332885742188\n",
      "loss = 124.56283\n",
      "step 2103: Agent survived [46] steps with reward 38064.35858154297\n",
      "step 2107: Agent survived [24] steps with reward 17458.998046875\n",
      "step 2111: Agent survived [24] steps with reward 19426.127502441406\n",
      "step 2115: Agent survived [24] steps with reward 14554.875671386719\n",
      "step 2119: Agent survived [24] steps with reward 20333.336669921875\n",
      "step 2123: Agent survived [24] steps with reward 22276.586669921875\n",
      "step 2127: Agent survived [24] steps with reward 23422.524169921875\n",
      "loss = 113.518555\n",
      "step 2131: Agent survived [24] steps with reward 26695.509399414062\n",
      "step 2135: Agent survived [24] steps with reward 18171.919860839844\n",
      "step 2139: Agent survived [24] steps with reward 20274.474731445312\n",
      "step 2143: Agent survived [24] steps with reward 22355.5771484375\n",
      "step 2147: Agent survived [24] steps with reward 24888.156616210938\n",
      "step 2151: Agent survived [24] steps with reward 23956.648803710938\n",
      "step 2155: Agent survived [24] steps with reward 17767.530395507812\n",
      "loss = 114.81906\n",
      "step 2159: Agent survived [24] steps with reward 20631.994689941406\n",
      "step 2163: Agent survived [24] steps with reward 21383.280151367188\n",
      "step 2167: Agent survived [24] steps with reward 21344.857299804688\n",
      "step 2171: Agent survived [24] steps with reward 25356.01953125\n",
      "step 2175: Agent survived [24] steps with reward 19138.626831054688\n",
      "step 2179: Agent survived [24] steps with reward 20887.951171875\n",
      "step 2183: Agent survived [24] steps with reward 21414.795043945312\n",
      "loss = 125.27907\n",
      "step 2187: Agent survived [24] steps with reward 23281.778930664062\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - force disconnection of 1 powerlines ([111])\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('Powerline with ids [111] have been modified illegally (cooldown)',)], 'rewards': {}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - force disconnection of 1 powerlines ([111])\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - NOT force any particular bus configuration {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('Powerline with ids [111] have been modified illegally (cooldown)',)], 'rewards': {}}\n",
      "step 2195: Agent survived [117] steps with reward 162187.04223632812\n",
      "step 2199: Agent survived [19] steps with reward 14284.166870117188\n",
      "step 2203: Agent survived [24] steps with reward 21227.11407470703\n",
      "step 2207: Agent survived [24] steps with reward 20479.03497314453\n",
      "step 2211: Agent survived [24] steps with reward 24679.95361328125\n",
      "loss = 271.6821\n",
      "step 2216: Agent survived [46] steps with reward 55700.27185058594\n",
      "step 2220: Agent survived [24] steps with reward 19580.181762695312\n",
      "step 2224: Agent survived [24] steps with reward 19043.023193359375\n",
      "step 2228: Agent survived [24] steps with reward 21404.49462890625\n",
      "step 2232: Agent survived [24] steps with reward 22630.185180664062\n",
      "step 2236: Agent survived [24] steps with reward 24890.251586914062\n",
      "loss = 256.06067\n",
      "step 2240: Agent survived [24] steps with reward 19385.203002929688\n",
      "step 2244: Agent survived [24] steps with reward 19262.96807861328\n",
      "step 2248: Agent survived [24] steps with reward 23157.830932617188\n",
      "step 2252: Agent survived [24] steps with reward 23153.433959960938\n",
      "Successfully saved model at: PARL_opp_D3QN_shifted_reward_2000_atk_period_50_atk_duration_20/D3QN_PARL.h5\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "n_iter = 2000\n",
    "save_path = f'PARL_opp_D3QN_shifted_reward_{n_iter}_atk_period_{attack_period}_atk_duration_{attack_duration}'\n",
    "log_path = f'PARL_opp_D3QN_train_logs'\n",
    "\n",
    "train_adversary(env, agent, opponent, num_pre_training_steps, n_iter, save_path, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-ordinary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
